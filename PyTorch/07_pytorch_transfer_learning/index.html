<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=PyTorchの転移学習を使って、事前学習済みEfficientNetモデルをカスタマイズし、ピザ・ステーキ・寿司の画像分類で85%の精度を達成する方法を詳しく解説します。><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/PyTorch/07_pytorch_transfer_learning/ rel=canonical><link href=../06_pytorch_modular/ rel=prev><link href=../08_pytorch_experiment_tracking/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>PyTorch転移学習 - 事前学習済みモデルで画像分類精度を劇的に向上させる - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#pytorch- class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> PyTorch転移学習 - 事前学習済みモデルで画像分類精度を劇的に向上させる </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class=md-tabs__item> <a href=../../LLM/00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 7. PyTorch transfer learning </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 転移学習とは？ </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 転移学習を使う理由 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 事前学習済みモデルの入手先 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 実装内容 </span> </a> </li> <li class=md-nav__item> <a href=#0 class=md-nav__link> <span class=md-ellipsis> 0. 環境設定 </span> </a> </li> <li class=md-nav__item> <a href=#1_1 class=md-nav__link> <span class=md-ellipsis> 1. データ取得 </span> </a> </li> <li class=md-nav__item> <a href=#2-datasetsdataloaders class=md-nav__link> <span class=md-ellipsis> 2. DatasetsとDataLoadersの作成 </span> </a> </li> <li class=md-nav__item> <a href=#3 class=md-nav__link> <span class=md-ellipsis> 3. 事前学習済みモデルの取得 </span> </a> </li> <li class=md-nav__item> <a href=#4 class=md-nav__link> <span class=md-ellipsis> 4. モデル訓練 </span> </a> </li> <li class=md-nav__item> <a href=#5 class=md-nav__link> <span class=md-ellipsis> 5. 損失曲線の可視化によるモデル評価 </span> </a> </li> <li class=md-nav__item> <a href=#6 class=md-nav__link> <span class=md-ellipsis> 6. テストセットの画像での予測 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 重要なポイント </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class=md-nav__item> <a href=../09_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> </li> <li class=md-nav__item> <a href=../10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../LLM/00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/02_using_transformers/ class=md-nav__link> <span class=md-ellipsis> 2. Using transformers </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/03_fine_tuning_a_pretrained_model/ class=md-nav__link> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/04_the_huggingface_tokenizers_library/ class=md-nav__link> <span class=md-ellipsis> 4. Tokenizers library </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/72_masked_language_modeling/ class=md-nav__link> <span class=md-ellipsis> Masked Language Modeling </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 転移学習とは？ </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 転移学習を使う理由 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 事前学習済みモデルの入手先 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 実装内容 </span> </a> </li> <li class=md-nav__item> <a href=#0 class=md-nav__link> <span class=md-ellipsis> 0. 環境設定 </span> </a> </li> <li class=md-nav__item> <a href=#1_1 class=md-nav__link> <span class=md-ellipsis> 1. データ取得 </span> </a> </li> <li class=md-nav__item> <a href=#2-datasetsdataloaders class=md-nav__link> <span class=md-ellipsis> 2. DatasetsとDataLoadersの作成 </span> </a> </li> <li class=md-nav__item> <a href=#3 class=md-nav__link> <span class=md-ellipsis> 3. 事前学習済みモデルの取得 </span> </a> </li> <li class=md-nav__item> <a href=#4 class=md-nav__link> <span class=md-ellipsis> 4. モデル訓練 </span> </a> </li> <li class=md-nav__item> <a href=#5 class=md-nav__link> <span class=md-ellipsis> 5. 損失曲線の可視化によるモデル評価 </span> </a> </li> <li class=md-nav__item> <a href=#6 class=md-nav__link> <span class=md-ellipsis> 6. テストセットの画像での予測 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 重要なポイント </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=pytorch->PyTorch転移学習 - 事前学習済みモデルで画像分類精度を劇的に向上させる<a class=headerlink href=#pytorch- title="Permanent link">&para;</a></h1> <h2 id=_1>概要<a class=headerlink href=#_1 title="Permanent link">&para;</a></h2> <p>これまでいくつかのモデルを手作りで構築してきましたが、性能は期待通りではありませんでした。</p> <p><strong>既存の高性能なモデルを自分の問題に活用できないか？</strong>と考えたことはありませんか？</p> <p>深層学習の世界では、その答えは多くの場合「<strong>Yes</strong>」です。</p> <p>本記事では、<strong>転移学習（Transfer Learning）</strong>という強力な技術を使って、FoodVision Miniプロジェクトの性能を劇的に向上させる方法を学びます。</p> <h2 id=_2>学習目標<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <p>この記事を読むことで、以下のスキルを身につけることができます：</p> <ul> <li>転移学習の概念と利点の理解</li> <li>PyTorchの事前学習済みモデルの活用方法</li> <li>モデルの層の凍結とカスタマイズ技術</li> <li>実際の画像データでの性能検証</li> </ul> <h2 id=_3>前提知識<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <ul> <li>PyTorchの基本的な使い方</li> <li>CNNの基本概念</li> <li>画像分類の基礎知識</li> <li>DataLoaderの使用経験</li> </ul> <h2 id=_4>転移学習とは？<a class=headerlink href=#_4 title="Permanent link">&para;</a></h2> <p><strong>転移学習</strong>は、他の問題で学習したモデルのパターン（重みとも呼ばれる）を、自分の問題に応用する技術です。</p> <p>例えば： - <a href=https://www.image-net.org/ >ImageNet</a>（数百万枚の様々な物体画像）で学習したコンピュータビジョンモデルのパターンを、FoodVision Miniモデルに活用 - 大量のテキストデータで学習した<a href=https://developers.google.com/machine-learning/glossary#masked-language-model>言語モデル</a>のパターンを、テキスト分類問題の基盤として使用</p> <p>基本的な考え方は：<strong>高性能な既存モデルを見つけて、自分の問題に適用する</strong>ことです。</p> <p><em>転移学習をコンピュータビジョンと自然言語処理（NLP）に適用した例。コンピュータビジョンでは、ImageNetの数百万枚の画像で学習したパターンを別の問題に適用し、NLPでは言語モデルがWikipedia全体（またはそれ以上）を読んで言語の構造を学習し、その知識を別の問題に適用します。</em></p> <h2 id=_5>転移学習を使う理由<a class=headerlink href=#_5 title="Permanent link">&para;</a></h2> <p>転移学習には主に2つの利点があります：</p> <h3 id=1>1. 実証済みの既存モデルの活用<a class=headerlink href=#1 title="Permanent link">&para;</a></h3> <p>自分の問題に類似した問題で証明されたニューラルネットワークアーキテクチャを活用できます。</p> <h3 id=2>2. 事前学習済みの知識の利用<a class=headerlink href=#2 title="Permanent link">&para;</a></h3> <p>類似データで<strong>既に学習済み</strong>のモデルを活用できるため、<strong>少ないカスタムデータで優れた結果</strong>を達成できます。</p> <p><em>研究と実践の両方で転移学習の使用が支持されています。スクラッチからの学習と転移学習のどちらが実践的に優れているかを調査した研究では、コストと時間の観点から転移学習が圧倒的に有益であることが判明しました。</em></p> <h2 id=_6>事前学習済みモデルの入手先<a class=headerlink href=#_6 title="Permanent link">&para;</a></h2> <p>深層学習の世界は素晴らしい場所です。多くの人々が自分の研究成果を共有しています。</p> <p>最新の最先端研究のコードと事前学習済みモデルは、論文発表から数日以内にリリースされることが多いです。</p> <table> <thead> <tr> <th><strong>場所</strong></th> <th><strong>提供内容</strong></th> <th><strong>リンク</strong></th> </tr> </thead> <tbody> <tr> <td><strong>PyTorchドメインライブラリ</strong></td> <td>各PyTorchドメインライブラリ（<code>torchvision</code>、<code>torchtext</code>）には何らかの形で事前学習済みモデルが含まれています。これらのモデルはPyTorch内で直接動作します。</td> <td><a href=https://pytorch.org/vision/stable/models.html><code>torchvision.models</code></a>, <a href=https://pytorch.org/text/main/models.html><code>torchtext.models</code></a>, <a href=https://pytorch.org/audio/stable/models.html><code>torchaudio.models</code></a></td> </tr> <tr> <td><strong>HuggingFace Hub</strong></td> <td>世界中の組織による、様々なドメイン（視覚、テキスト、音声など）の事前学習済みモデルのコレクション。多様なデータセットも提供。</td> <td><a href=https://huggingface.co/models>models</a>, <a href=https://huggingface.co/datasets>datasets</a></td> </tr> <tr> <td><strong><code>timm</code>（PyTorch Image Models）ライブラリ</strong></td> <td>最新かつ最高のコンピュータビジョンモデルのほぼすべてをPyTorchコードで提供。その他多くの有用なコンピュータビジョン機能も含む。</td> <td><a href=https://github.com/rwightman/pytorch-image-models>GitHub</a></td> </tr> <tr> <td><strong>Paperswithcode</strong></td> <td>最新の最先端機械学習論文とコード実装のコレクション。異なるタスクでのモデル性能のベンチマークも確認可能。</td> <td><a href=https://paperswithcode.com/ >paperswithcode.com</a></td> </tr> </tbody> </table> <blockquote> <p><strong>重要なポイント：</strong> 上記のような高品質なリソースへのアクセスがあることから、取り組む深層学習問題の開始時には「自分の問題に対する事前学習済みモデルは存在するか？」と問うことが一般的な実践となるべきです。</p> <p><strong>演習：</strong> <a href=https://pytorch.org/vision/stable/models.html><code>torchvision.models</code></a>と<a href=https://huggingface.co/models>HuggingFace Hub Models page</a>を5分間探索してみてください。何を発見しましたか？</p> </blockquote> <h2 id=_7>実装内容<a class=headerlink href=#_7 title="Permanent link">&para;</a></h2> <p><code>torchvision.models</code>から事前学習済みモデルを取得し、FoodVision Mini問題に合わせてカスタマイズします。</p> <table> <thead> <tr> <th><strong>トピック</strong></th> <th><strong>内容</strong></th> </tr> </thead> <tbody> <tr> <td><strong>0. 環境設定</strong></td> <td>過去のセクションで有用なコードを多く作成したので、それらをダウンロードして再利用できるようにします。</td> </tr> <tr> <td><strong>1. データ取得</strong></td> <td>使用してきたピザ、ステーキ、寿司の画像分類データセットを取得し、モデル結果の改善を試みます。</td> </tr> <tr> <td><strong>2. DatasetsとDataLoadersの作成</strong></td> <td>第06章で作成した<code>data_setup.py</code>スクリプトを使用してDataLoadersを設定します。</td> </tr> <tr> <td><strong>3. 事前学習済みモデルの取得とカスタマイズ</strong></td> <td><code>torchvision.models</code>から事前学習済みモデルをダウンロードし、自分の問題に合わせてカスタマイズします。</td> </tr> <tr> <td><strong>4. モデル訓練</strong></td> <td>新しい事前学習済みモデルがピザ、ステーキ、寿司データセットでどのように動作するかを確認します。前章で作成した訓練関数を使用します。</td> </tr> <tr> <td><strong>5. 損失曲線の可視化によるモデル評価</strong></td> <td>最初の転移学習モデルの性能はどうでしたか？過学習や未学習は発生しましたか？</td> </tr> <tr> <td><strong>6. テストセットの画像での予測</strong></td> <td>モデルの評価指標を確認することも重要ですが、テストサンプルでの予測を視覚化することも同様に重要です。<em>可視化、可視化、可視化！</em></td> </tr> </tbody> </table> <h2 id=0>0. 環境設定<a class=headerlink href=#0 title="Permanent link">&para;</a></h2> <p>このセクションに必要なモジュールをインポート/ダウンロードして開始しましょう。</p> <p>コードの記述を節約するため、前のセクションで作成したPythonスクリプト（<code>data_setup.py</code>や<code>engine.py</code>など）を活用します。</p> <p>また、後でモデルの視覚的表現を提供するのに役立つ<a href=https://github.com/TylerYep/torchinfo><code>torchinfo</code></a>パッケージも取得します。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># 通常のインポートを続行</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=kn>from</span><span class=w> </span><span class=nn>torch</span><span class=w> </span><span class=kn>import</span> <span class=n>nn</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>transforms</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=kn>from</span><span class=w> </span><span class=nn>torchinfo</span><span class=w> </span><span class=kn>import</span> <span class=n>summary</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=c1># pip install -q torchinfo</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular</span><span class=w> </span><span class=kn>import</span> <span class=n>data_setup</span><span class=p>,</span> <span class=n>engine</span>
</span></code></pre></div> <p>デバイス非依存コードを設定しましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># デバイス非依存コードの設定</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>device</span> <span class=o>=</span> <span class=s2>&quot;mps&quot;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>mps</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&quot;cpu&quot;</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>device</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>&#39;mps&#39;
</span></code></pre></div> <h2 id=1_1>1. データ取得<a class=headerlink href=#1_1 title="Permanent link">&para;</a></h2> <p><strong>転移学習</strong>を使用する前に、データセットが必要です。</p> <p>転移学習と以前のモデル構築の試みを比較するため、FoodVision Miniで使用してきた同じデータセットをダウンロードします。</p> <p>GitHubから<a href=https://github.com/vinsmoke-three/deeplearning-with-pytorch/raw/main/data/pizza_steak_sushi.zip><code>pizza_steak_sushi.zip</code></a>データセットをダウンロードして解凍するコードを書きましょう。</p> <p>データが既に存在する場合は再ダウンロードしないようにすることもできます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=kn>import</span><span class=w> </span><span class=nn>zipfile</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=kn>from</span><span class=w> </span><span class=nn>pathlib</span><span class=w> </span><span class=kn>import</span> <span class=n>Path</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=kn>import</span><span class=w> </span><span class=nn>requests</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=c1># データフォルダへのパスを設定</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=n>data_path</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&quot;data/&quot;</span><span class=p>)</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=n>image_path</span> <span class=o>=</span> <span class=n>data_path</span> <span class=o>/</span> <span class=s2>&quot;pizza_steak_sushi&quot;</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=c1># 画像フォルダが存在しない場合、ダウンロードして準備...</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=k>if</span> <span class=n>image_path</span><span class=o>.</span><span class=n>is_dir</span><span class=p>():</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>image_path</span><span class=si>}</span><span class=s2> ディレクトリが存在します。&quot;</span><span class=p>)</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=k>else</span><span class=p>:</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>image_path</span><span class=si>}</span><span class=s2> ディレクトリが見つかりません。作成します...&quot;</span><span class=p>)</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a>    <span class=n>image_path</span><span class=o>.</span><span class=n>mkdir</span><span class=p>(</span><span class=n>parents</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a>    <span class=c1># ピザ、ステーキ、寿司データをダウンロード</span>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>data_path</span> <span class=o>/</span> <span class=s2>&quot;pizza_steak_sushi.zip&quot;</span><span class=p>,</span> <span class=s2>&quot;wb&quot;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>        <span class=n>request</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;https://github.com/vinsmoke-three/deeplearning-with-pytorch/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class=p>)</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;ピザ、ステーキ、寿司データをダウンロード中...&quot;</span><span class=p>)</span>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a>        <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a>
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25 href=#__codelineno-3-25></a>    <span class=c1># ピザ、ステーキ、寿司データを解凍</span>
</span><span id=__span-3-26><a id=__codelineno-3-26 name=__codelineno-3-26 href=#__codelineno-3-26></a>    <span class=k>with</span> <span class=n>zipfile</span><span class=o>.</span><span class=n>ZipFile</span><span class=p>(</span><span class=n>data_path</span> <span class=o>/</span> <span class=s2>&quot;pizza_steak_sushi.zip&quot;</span><span class=p>,</span> <span class=s2>&quot;r&quot;</span><span class=p>)</span> <span class=k>as</span> <span class=n>zip_ref</span><span class=p>:</span>
</span><span id=__span-3-27><a id=__codelineno-3-27 name=__codelineno-3-27 href=#__codelineno-3-27></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;ピザ、ステーキ、寿司データを解凍中...&quot;</span><span class=p>)</span> 
</span><span id=__span-3-28><a id=__codelineno-3-28 name=__codelineno-3-28 href=#__codelineno-3-28></a>        <span class=n>zip_ref</span><span class=o>.</span><span class=n>extractall</span><span class=p>(</span><span class=n>image_path</span><span class=p>)</span>
</span><span id=__span-3-29><a id=__codelineno-3-29 name=__codelineno-3-29 href=#__codelineno-3-29></a>
</span><span id=__span-3-30><a id=__codelineno-3-30 name=__codelineno-3-30 href=#__codelineno-3-30></a>    <span class=c1># .zipファイルを削除</span>
</span><span id=__span-3-31><a id=__codelineno-3-31 name=__codelineno-3-31 href=#__codelineno-3-31></a>    <span class=n>os</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>data_path</span> <span class=o>/</span> <span class=s2>&quot;pizza_steak_sushi.zip&quot;</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>data/pizza_steak_sushi ディレクトリが存在します。
</span></code></pre></div> <p>訓練とテストディレクトリへのパスを作成しましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># ディレクトリの設定</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=n>train_dir</span> <span class=o>=</span> <span class=n>image_path</span> <span class=o>/</span> <span class=s2>&quot;train&quot;</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=n>test_dir</span> <span class=o>=</span> <span class=n>image_path</span> <span class=o>/</span> <span class=s2>&quot;test&quot;</span>
</span></code></pre></div> <h2 id=2-datasetsdataloaders>2. DatasetsとDataLoadersの作成<a class=headerlink href=#2-datasetsdataloaders title="Permanent link">&para;</a></h2> <h3 id=21-torchvisionmodels>2.1 <code>torchvision.models</code>用の変換の作成（手動作成）<a class=headerlink href=#21-torchvisionmodels title="Permanent link">&para;</a></h3> <p>事前学習済みモデルを使用する際は、<strong>モデルに入力するカスタムデータが、モデルの元の訓練データと同じ方法で準備されている</strong>ことが重要です。</p> <p><code>torchvision</code> v0.13+以前では、<code>torchvision.models</code>の事前学習済みモデル用の変換を作成するため、ドキュメントに以下のように記載されていました：</p> <blockquote> <p>すべての事前学習済みモデルは、同じ方法で正規化された入力画像を期待します。つまり、形状 (3 x H x W) の3チャンネルRGB画像のミニバッチで、HとWは少なくとも224である必要があります。</p> <p>画像は <code>[0, 1]</code> の範囲にロードし、<code>mean = [0.485, 0.456, 0.406]</code> と <code>std = [0.229, 0.224, 0.225]</code> を使用して正規化する必要があります。</p> <p>正規化には以下の変換を使用できます：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>                                 std=[0.229, 0.224, 0.225])
</span></code></pre></div> </blockquote> <p>良いニュースは、上記の変換を以下の組み合わせで実現できることです：</p> <table> <thead> <tr> <th><strong>変換番号</strong></th> <th><strong>必要な変換</strong></th> <th><strong>変換を実行するコード</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>サイズ <code>[batch_size, 3, height, width]</code> のミニバッチ（heightとwidthは少なくとも224x224）</td> <td><code>torchvision.transforms.Resize()</code> で画像を <code>[3, 224, 224]</code> にリサイズし、<code>torch.utils.data.DataLoader()</code> で画像のバッチを作成</td> </tr> <tr> <td>2</td> <td>0と1の間の値</td> <td><code>torchvision.transforms.ToTensor()</code></td> </tr> <tr> <td>3</td> <td><code>[0.485, 0.456, 0.406]</code> の平均（各カラーチャンネル全体の値）</td> <td><code>torchvision.transforms.Normalize(mean=...)</code> で画像の平均を調整</td> </tr> <tr> <td>4</td> <td><code>[0.229, 0.224, 0.225]</code> の標準偏差（各カラーチャンネル全体の値）</td> <td><code>torchvision.transforms.Normalize(std=...)</code> で画像の標準偏差を調整</td> </tr> </tbody> </table> <blockquote> <p><strong>注意：</strong> <code>torchvision.models</code>の一部の事前学習済みモデルは、<code>[3, 224, 224]</code>とは異なるサイズを取ります。例えば、<code>[3, 240, 240]</code>を取るものもあります。特定の入力画像サイズについては、ドキュメントを参照してください。</p> <p><strong>質問：</strong> <em>平均と標準偏差の値はどこから来たのですか？なぜこれを行う必要があるのですか？</em></p> <p>これらの値はデータから計算されました。具体的には、画像のサブセット全体で平均と標準偏差を取ることによって、ImageNetデータセットから計算されました。</p> <p>また、これを行う<em>必要</em>はありません。ニューラルネットワークは通常、適切なデータ分布を自分で把握する能力があります（平均と標準偏差がどこにある必要があるかを自分で計算します）が、開始時に設定することで、ネットワークがより良い性能をより早く達成するのに役立ちます。</p> </blockquote> <p>上記の手順を実行するために、一連の<code>torchvision.transforms</code>を構成しましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># 変換パイプラインを手動で作成（torchvision &lt; 0.13で必要）</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>manual_transforms</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>((</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>)),</span> <span class=c1># 1. すべての画像を224x224にリシェイプ（一部のモデルでは異なるサイズが必要な場合があります）</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span> <span class=c1># 2. 画像の値を0と1の間に変換</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span> <span class=c1># 3. [0.485, 0.456, 0.406]の平均（各カラーチャンネル全体）</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>                         <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>])</span> <span class=c1># 4. [0.229, 0.224, 0.225]の標準偏差（各カラーチャンネル全体）</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class=p>])</span>
</span></code></pre></div> <p>素晴らしい！<strong>手動で作成した一連の変換</strong>が画像を準備する準備ができました。訓練とテスト用のDataLoadersを作成しましょう。</p> <p><code>data_setup.py</code>スクリプトの<code>create_dataloaders</code>関数を使用してこれらを作成できます。</p> <p>モデルが一度に32サンプルのミニバッチを見るように<code>batch_size=32</code>を設定します。</p> <p>そして、<code>transform=manual_transforms</code>を設定することで、上記で作成した変換パイプラインを使用して画像を変換できます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=c1># 訓練とテスト用のDataLoadersを作成し、クラス名のリストも取得</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span> <span class=o>=</span> <span class=n>data_setup</span><span class=o>.</span><span class=n>create_dataloaders</span><span class=p>(</span><span class=n>train_dir</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>train_dir</span><span class=p>),</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>                                                                               <span class=n>test_dir</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>test_dir</span><span class=p>),</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>                                                                               <span class=n>transform</span><span class=o>=</span><span class=n>manual_transforms</span><span class=p>,</span> <span class=c1># 画像をリサイズ、0〜1に変換、正規化</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>                                                                               <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span> <span class=c1># ミニバッチサイズを32に設定</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>(&lt;torch.utils.data.dataloader.DataLoader at 0x3537e8170&gt;,
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a> &lt;torch.utils.data.dataloader.DataLoader at 0x35296dd00&gt;,
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a> [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</span></code></pre></div> <h3 id=22-torchvisionmodels>2.2 <code>torchvision.models</code>用の変換の作成（自動作成）<a class=headerlink href=#22-torchvisionmodels title="Permanent link">&para;</a></h3> <p>前述のように、事前学習済みモデルを使用する際は、<strong>モデルに入力するカスタムデータが、モデルの元の訓練データと同じ方法で準備されている</strong>ことが重要です。</p> <p>上記では、事前学習済みモデル用の変換を手動で作成する方法を見ました。</p> <p>しかし、<code>torchvision</code> v0.13+以降、自動変換作成機能が追加されました。</p> <p><code>torchvision.models</code>からモデルを設定し、使用したい事前学習済みモデルの重みを選択する場合、例えば以下を使用したいとします：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=n>weights</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>EfficientNet_B0_Weights</span><span class=o>.</span><span class=n>DEFAULT</span>
</span></code></pre></div> <p>ここで： * <code>EfficientNet_B0_Weights</code>は使用したいモデルアーキテクチャの重みです（<code>torchvision.models</code>には多くの異なるモデルアーキテクチャオプションがあります） * <code>DEFAULT</code>は<em>利用可能な最高</em>の重み（ImageNetでの最高性能）を意味します * <strong>注意：</strong> 選択するモデルアーキテクチャによって、<code>IMAGENET_V1</code>や<code>IMAGENET_V2</code>などの他のオプションも表示される場合があります。一般的に、バージョン番号が高いほど良好です。ただし、利用可能な最高のものが必要な場合は、<code>DEFAULT</code>が最も簡単なオプションです。詳細については、<a href=https://pytorch.org/vision/main/models.html><code>torchvision.models</code>ドキュメント</a>を参照してください。</p> <p>試してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=c1># 事前学習済みモデルの重みのセットを取得</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>weights</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>EfficientNet_B0_Weights</span><span class=o>.</span><span class=n>DEFAULT</span> <span class=c1># .DEFAULT = ImageNetでの事前学習から利用可能な最高の重み</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=n>weights</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>EfficientNet_B0_Weights.IMAGENET1K_V1
</span></code></pre></div> <p>そして、<code>weights</code>に関連する変換にアクセスするために、<code>transforms()</code>メソッドを使用できます。</p> <p>これは本質的に「ImageNet上で<code>EfficientNet_B0_Weights</code>を訓練するために使用されたデータ変換を取得する」と言っています。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># 事前学習済み重みを作成するために使用された変換を取得</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>auto_transforms</span> <span class=o>=</span> <span class=n>weights</span><span class=o>.</span><span class=n>transforms</span><span class=p>()</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=n>auto_transforms</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>ImageClassification(
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>    crop_size=[224]
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>    resize_size=[256]
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>    mean=[0.485, 0.456, 0.406]
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>    std=[0.229, 0.224, 0.225]
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>    interpolation=InterpolationMode.BICUBIC
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a>)
</span></code></pre></div> <p><code>auto_transforms</code>が<code>manual_transforms</code>と非常に似ていることに注目してください。唯一の違いは、<code>auto_transforms</code>が選択したモデルアーキテクチャに付属していたのに対し、<code>manual_transforms</code>は手動で作成する必要があったことです。</p> <p><code>weights.transforms()</code>を通じて自動的に変換を作成する利点は、事前学習済みモデルが訓練時に使用したのと同じデータ変換を確実に使用できることです。</p> <p>ただし、自動作成された変換を使用するトレードオフは、カスタマイズの欠如です。</p> <p>以前と同様に<code>auto_transforms</code>を使用して<code>create_dataloaders()</code>でDataLoadersを作成できます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=c1># 訓練とテスト用のDataLoadersを作成し、クラス名のリストも取得</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span> <span class=o>=</span> <span class=n>data_setup</span><span class=o>.</span><span class=n>create_dataloaders</span><span class=p>(</span><span class=n>train_dir</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>train_dir</span><span class=p>),</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>                                                                               <span class=n>test_dir</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>test_dir</span><span class=p>),</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>                                                                               <span class=n>transform</span><span class=o>=</span><span class=n>auto_transforms</span><span class=p>,</span> <span class=c1># 事前学習済みモデルと同じデータ変換を自分のデータに実行</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>                                                                               <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span> <span class=c1># ミニバッチサイズを32に設定</span>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a>(&lt;torch.utils.data.dataloader.DataLoader at 0x35466bfb0&gt;,
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a> &lt;torch.utils.data.dataloader.DataLoader at 0x3537bbda0&gt;,
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a> [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</span></code></pre></div> <h2 id=3>3. 事前学習済みモデルの取得<a class=headerlink href=#3 title="Permanent link">&para;</a></h2> <p>さて、ここからが楽しい部分です！</p> <p>過去数回のノートブックでは、PyTorchニューラルネットワークをゼロから構築してきました。</p> <p>それは良いスキルですが、モデルの性能は期待通りではありませんでした。</p> <p>そこで<strong>転移学習</strong>の出番です。</p> <p>転移学習の全体的なアイデアは、<strong>自分の問題空間と類似した問題で既に良好な性能を示しているモデルを取得し、自分の用途にカスタマイズする</strong>ことです。</p> <p>コンピュータビジョン問題（FoodVision Miniでの画像分類）に取り組んでいるので、<a href=https://pytorch.org/vision/stable/models.html#classification><code>torchvision.models</code></a>で事前学習済み分類モデルを見つけることができます。</p> <p>ドキュメントを探索すると、以下のような一般的なコンピュータビジョンアーキテクチャバックボーンが多数見つかります：</p> <table> <thead> <tr> <th><strong>アーキテクチャバックボーン</strong></th> <th><strong>コード</strong></th> </tr> </thead> <tbody> <tr> <td><a href=https://arxiv.org/abs/1512.03385>ResNet</a></td> <td><code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>...</td> </tr> <tr> <td><a href=https://arxiv.org/abs/1409.1556>VGG</a>（TinyVGGで使用したものと類似）</td> <td><code>torchvision.models.vgg16()</code></td> </tr> <tr> <td><a href=https://arxiv.org/abs/1905.11946>EfficientNet</a></td> <td><code>torchvision.models.efficientnet_b0()</code>, <code>torchvision.models.efficientnet_b1()</code>...</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2010.11929>VisionTransformer</a>（ViT）</td> <td><code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>...</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2201.03545>ConvNeXt</a></td> <td><code>torchvision.models.convnext_tiny()</code>, <code>torchvision.models.convnext_small()</code>...</td> </tr> <tr> <td><code>torchvision.models</code>で利用可能なその他</td> <td><code>torchvision.models...</code></td> </tr> </tbody> </table> <h3 id=31>3.1 どの事前学習済みモデルを使用すべきか？<a class=headerlink href=#31 title="Permanent link">&para;</a></h3> <p>それは問題や作業しているデバイスによって異なります。</p> <p>一般的に、モデル名の数字が高いほど（例：<code>efficientnet_b0()</code> → <code>efficientnet_b1()</code> → <code>efficientnet_b7()</code>）、<em>性能は向上</em>しますが、<em>モデルは大きく</em>なります。</p> <p>性能の向上は<em>常に良い</em>と思うかもしれませんよね？</p> <p>それは正しいですが、<strong>一部の高性能モデルは一部のデバイスには大きすぎます</strong>。</p> <p>例えば、モバイルデバイスでモデルを実行したい場合、デバイスの限られた計算リソースを考慮する必要があり、より小さなモデルを探すことになります。</p> <p>しかし、無制限の計算力がある場合、<a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html><em>The Bitter Lesson</em></a>が述べるように、可能な限り大きく、計算量の多いモデルを選ぶでしょう。</p> <p>この<strong>性能 vs. 速度 vs. サイズ</strong>のトレードオフを理解することは、時間と実践を通じて身につきます。</p> <p>私にとっては、<code>efficientnet_bX</code>モデルで良いバランスを見つけました。</p> <h3 id=32>3.2 事前学習済みモデルの設定<a class=headerlink href=#32 title="Permanent link">&para;</a></h3> <p>使用する事前学習済みモデルは<a href=https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html><code>torchvision.models.efficientnet_b0()</code></a>です。</p> <p>このアーキテクチャは論文<em><a href=https://arxiv.org/abs/1905.11946>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></em>からのものです。</p> <p><em>作成予定の内容の例：<code>torchvision.models</code>の事前学習済み<a href=https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html><code>EfficientNet_B0</code>モデル</a>の出力層をピザ、ステーキ、寿司画像の分類用途に調整します。</em></p> <p>変換を作成するために使用したのと同じコードを使用して、<code>EfficientNet_B0</code>事前学習済みImageNet重みを設定できます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=n>weights</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>EfficientNet_B0_Weights</span><span class=o>.</span><span class=n>DEFAULT</span> <span class=c1># .DEFAULT = ImageNetで利用可能な最高の重み</span>
</span></code></pre></div> <p>これは、モデルが既に数百万枚の画像で訓練されており、画像データの良好な基本表現を持っていることを意味します。</p> <p>このPyTorch版の事前学習済みモデルは、ImageNetの1000クラス全体で約77.7%の精度を達成することができます。</p> <p>また、ターゲットデバイスに送信します。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=c1># 事前学習済み重みでモデルを設定し、ターゲットデバイスに送信</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=n>weights</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>EfficientNet_B0_Weights</span><span class=o>.</span><span class=n>DEFAULT</span> <span class=c1># .DEFAULT = 利用可能な最高の重み </span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>efficientnet_b0</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=n>weights</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a><span class=n>model</span><span class=o>.</span><span class=n>classifier</span>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a><span class=c1>#model # コメントアウトを解除して出力（非常に長いです）</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a>Sequential(
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>  (0): Dropout(p=0.2, inplace=True)
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>  (1): Linear(in_features=1280, out_features=1000, bias=True)
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a>)
</span></code></pre></div> <p>モデルを印刷すると、以下のようなものが得られます：</p> <p><img alt=EfficientNet_B0モデルの出力 src=../07_pytorch_transfer_learning_files/07-v2-effnetb0-model-print-out.png></p> <p>多くの層があります。</p> <p>これは転移学習の利点の1つです。世界で最高のエンジニアの一部によって作られた既存のモデルを取得し、自分の問題に適用することです。</p> <p><code>efficientnet_b0</code>は主に3つの部分で構成されています：</p> <ol> <li><strong><code>features</code></strong> - 視覚データの基本表現を学習するための畳み込み層とその他の様々な活性化層のコレクション（この基本表現/層のコレクションは<strong>特徴量</strong>または<strong>特徴抽出器</strong>と呼ばれることが多く、「モデルの基本層は画像の異なる<strong>特徴量</strong>を学習する」）</li> <li><strong><code>avgpool</code></strong> - <code>features</code>層の出力の平均を取り、<strong>特徴ベクトル</strong>に変換</li> <li><strong><code>classifier</code></strong> - <strong>特徴ベクトル</strong>を必要な出力クラス数と同じ次元のベクトルに変換（<code>efficientnet_b0</code>はImageNetで事前学習されており、ImageNetには1000クラスがあるため、<code>out_features=1000</code>がデフォルト）</li> </ol> <h3 id=33-torchinfosummary>3.3 <code>torchinfo.summary()</code>でモデルの要約を取得<a class=headerlink href=#33-torchinfosummary title="Permanent link">&para;</a></h3> <p>モデルについてさらに学ぶために、<code>torchinfo</code>の<a href=https://github.com/TylerYep/torchinfo#documentation><code>summary()</code>メソッド</a>を使用しましょう。</p> <p>以下を渡します： * <code>model</code> - 要約を取得したいモデル * <code>input_size</code> - モデルに渡したいデータの形状。<code>efficientnet_b0</code>の場合、入力サイズは<code>(batch_size, 3, 224, 224)</code>ですが、<a href=https://github.com/pytorch/vision/blob/d2bfd639e46e1c5dc3c177f889dc7750c8d137c7/references/classification/train.py#L92-L93><code>efficientnet_bX</code>の他のバリアントは異なる入力サイズを持ちます</a> * <strong>注意：</strong> 多くの現代的なモデルは<a href=https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html><code>torch.nn.AdaptiveAvgPool2d()</code></a>のおかげで、様々なサイズの入力画像を処理できます。この層は、必要に応じて与えられた入力の<code>output_size</code>を適応的に調整します。異なるサイズの入力画像を<code>summary()</code>やモデルに渡すことで試すことができます * <code>col_names</code> - モデルについて表示したい様々な情報列 * <code>col_width</code> - 要約の列の幅 * <code>row_settings</code> - 行に表示する機能</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># torchinfoを使用して要約を印刷（実際の出力についてはコメントアウトを解除）</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=n>summary</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> 
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>        <span class=n>input_size</span><span class=o>=</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>),</span> <span class=c1># これが &quot;input_size&quot; であることを確認、&quot;input_shape&quot; ではありません</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a>        <span class=c1># col_names=[&quot;input_size&quot;], # より小さな出力についてはコメントアウトを解除</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a>        <span class=n>col_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;input_size&quot;</span><span class=p>,</span> <span class=s2>&quot;output_size&quot;</span><span class=p>,</span> <span class=s2>&quot;num_params&quot;</span><span class=p>,</span> <span class=s2>&quot;trainable&quot;</span><span class=p>],</span>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a>        <span class=n>col_width</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a>        <span class=n>row_settings</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;var_names&quot;</span><span class=p>]</span>
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=p>)</span> 
</span></code></pre></div> <p><img alt=torchinfo.summaryの出力 src=../07_pytorch_transfer_learning_files/07-torchinfo-summary-unfrozen-layers.png></p> <p>うわあ！これは大きなモデルです！</p> <p>要約の出力から、画像データがモデルを通過する際の様々な入力と出力の形状変化をすべて見ることができます。</p> <p>そして、データの異なるパターンを認識するための訓練済みパラメータ（事前学習済み重み）が大量にあります。</p> <p>参考として、前のセクションのモデル<strong>TinyVGGは8,083パラメータでしたが、<code>efficientnet_b0</code>は5,288,548パラメータで、約654倍の増加</strong>です！</p> <p>どう思いますか、これはより良い性能を意味するでしょうか？</p> <h3 id=34>3.4 ベースモデルの凍結と出力層の調整<a class=headerlink href=#34 title="Permanent link">&para;</a></h3> <p>転移学習のプロセスは通常以下のように進みます：事前学習済みモデルの一部のベース層（通常は<code>features</code>セクション）を凍結し、出力層（ヘッド/分類器層とも呼ばれる）を自分のニーズに合わせて調整します。</p> <p><img alt=分類器ヘッドの変更 src=../07_pytorch_transfer_learning_files/07-v2-effnet-changing-the-classifier-head.png></p> <p><em>事前学習済みモデルの出力層を自分の問題に合うように変更することで、出力をカスタマイズできます。元の<code>torchvision.models.efficientnet_b0()</code>は、訓練されたデータセットであるImageNetに1000クラスがあるため、<code>out_features=1000</code>で提供されます。しかし、ピザ、ステーキ、寿司の画像を分類する私たちの問題では、<code>out_features=3</code>のみが必要です。</em></p> <p><code>efficientnet_b0</code>モデルの<code>features</code>セクションのすべての層/パラメータを凍結しましょう。</p> <blockquote> <p><strong>注意：</strong> 層を<em>凍結</em>するということは、訓練中にそれらをそのまま保持することを意味します。例えば、モデルに事前学習済み層がある場合、それらを<em>凍結</em>するということは、「訓練中にこれらの層のパターンを変更せず、そのまま保持する」ということです。本質的に、ImageNetから学習したモデルの事前学習済み重み/パターンをバックボーンとして保持し、出力層のみを変更したいのです。</p> </blockquote> <p><code>requires_grad=False</code>属性を設定することで、<code>features</code>セクションのすべての層/パラメータを凍結できます。</p> <p><code>requires_grad=False</code>のパラメータについて、PyTorchは勾配更新を追跡せず、結果として、これらのパラメータは訓練中にオプティマイザーによって変更されません。</p> <p>本質的に、<code>requires_grad=False</code>のパラメータは「訓練不可能」または「凍結」されています。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=c1># モデルの &quot;features&quot; セクション（特徴抽出器）のすべてのベース層を requires_grad=False に設定して凍結</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>features</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></code></pre></div> <p>特徴抽出器層が凍結されました！</p> <p>事前学習済みモデルの<code>classifier</code>部分を私たちのニーズに合わせて調整しましょう。</p> <p>現在、事前学習済みモデルには、ImageNetに1000クラスがあるため<code>out_features=1000</code>があります。</p> <p>しかし、私たちには1000クラスはなく、ピザ、ステーキ、寿司の3つしかありません。</p> <p>新しい一連の層を作成することで、モデルの<code>classifier</code>部分を変更できます。</p> <p>現在の<code>classifier</code>は以下で構成されています：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a>(classifier): Sequential(
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a>    (0): Dropout(p=0.2, inplace=True)
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a>    (1): Linear(in_features=1280, out_features=1000, bias=True)
</span></code></pre></div> <p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html><code>torch.nn.Dropout(p=0.2, inplace=True)</code></a>を使用して<code>Dropout</code>層を同じまま保ちます。</p> <blockquote> <p><strong>注意：</strong> <a href=https://developers.google.com/machine-learning/glossary#dropout_regularization>Dropout層</a>は、確率<code>p</code>で2つのニューラルネットワーク層間の接続をランダムに削除します。例えば、<code>p=0.2</code>の場合、ニューラルネットワーク層間の接続の20%が各パスでランダムに削除されます。この実践は、残りの接続が他の接続の削除を補償するために特徴量を学習することを確実にすることで、モデルを正則化（過学習を防ぐ）することを意図しています（これらの残りの特徴量は<em>より一般的</em>であることを願って）。</p> </blockquote> <p><code>Linear</code>出力層については<code>in_features=1280</code>を保持しますが、<code>out_features</code>値を<code>class_names</code>の長さ（<code>len(['pizza', 'steak', 'sushi']) = 3</code>）に変更します。</p> <p>新しい<code>classifier</code>層は<code>model</code>と同じデバイス上にある必要があります。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a><span class=c1># 手動シードを設定</span>
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a><span class=c1># class_namesの長さを取得（各クラスに1つの出力ユニット）</span>
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a><span class=n>output_shape</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>class_names</span><span class=p>)</span>
</span><span id=__span-23-7><a id=__codelineno-23-7 name=__codelineno-23-7 href=#__codelineno-23-7></a>
</span><span id=__span-23-8><a id=__codelineno-23-8 name=__codelineno-23-8 href=#__codelineno-23-8></a><span class=c1># 分類器層を再作成し、ターゲットデバイスにシードする</span>
</span><span id=__span-23-9><a id=__codelineno-23-9 name=__codelineno-23-9 href=#__codelineno-23-9></a><span class=n>model</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-23-10><a id=__codelineno-23-10 name=__codelineno-23-10 href=#__codelineno-23-10></a>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span> 
</span><span id=__span-23-11><a id=__codelineno-23-11 name=__codelineno-23-11 href=#__codelineno-23-11></a>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>1280</span><span class=p>,</span> 
</span><span id=__span-23-12><a id=__codelineno-23-12 name=__codelineno-23-12 href=#__codelineno-23-12></a>                    <span class=n>out_features</span><span class=o>=</span><span class=n>output_shape</span><span class=p>,</span> <span class=c1># クラス数と同じ出力ユニット数</span>
</span><span id=__span-23-13><a id=__codelineno-23-13 name=__codelineno-23-13 href=#__codelineno-23-13></a>                    <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></code></pre></div> <p>素晴らしい！出力層が更新されました。モデルの別の要約を取得して、何が変わったかを見てみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=c1># 特徴量を凍結し、出力分類器層を変更した*後*の要約を実行（実際の出力についてはコメントアウトを解除）</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a><span class=n>summary</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> 
</span><span id=__span-24-3><a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a>        <span class=n>input_size</span><span class=o>=</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>),</span> <span class=c1># これが &quot;input_size&quot; であることを確認、&quot;input_shape&quot; ではありません (batch_size, color_channels, height, width)</span>
</span><span id=__span-24-4><a id=__codelineno-24-4 name=__codelineno-24-4 href=#__codelineno-24-4></a>        <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span><span id=__span-24-5><a id=__codelineno-24-5 name=__codelineno-24-5 href=#__codelineno-24-5></a>        <span class=n>col_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;input_size&quot;</span><span class=p>,</span> <span class=s2>&quot;output_size&quot;</span><span class=p>,</span> <span class=s2>&quot;num_params&quot;</span><span class=p>,</span> <span class=s2>&quot;trainable&quot;</span><span class=p>],</span>
</span><span id=__span-24-6><a id=__codelineno-24-6 name=__codelineno-24-6 href=#__codelineno-24-6></a>        <span class=n>col_width</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
</span><span id=__span-24-7><a id=__codelineno-24-7 name=__codelineno-24-7 href=#__codelineno-24-7></a>        <span class=n>row_settings</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;var_names&quot;</span><span class=p>]</span>
</span><span id=__span-24-8><a id=__codelineno-24-8 name=__codelineno-24-8 href=#__codelineno-24-8></a><span class=p>)</span>
</span></code></pre></div> <p><img alt=凍結層後のtorchinfo.summaryの出力 src=../07_pytorch_transfer_learning_files/07-torchinfo-summary-frozen-layers.png></p> <p>おお、おお！ここにはかなりの変化があります！</p> <p>それらを見ていきましょう：</p> <ul> <li><strong>Trainable列</strong> - 多くのベース層（<code>features</code>部分の層）のTrainable値が<code>False</code>になっていることがわかります。これは、属性<code>requires_grad=False</code>を設定したためです。これを変更しない限り、これらの層は将来の訓練で更新されません。</li> <li><strong><code>classifier</code>の出力形状</strong> - モデルの<code>classifier</code>部分のOutput Shape値が<code>[32, 1000]</code>ではなく<code>[32, 3]</code>になりました。Trainable値も<code>True</code>です。これは、そのパラメータが訓練中に更新されることを意味します。本質的に、<code>features</code>部分を使用して<code>classifier</code>部分に画像の基本表現を供給し、<code>classifier</code>層がその基本表現が私たちの問題とどのように整合するかを学習します。</li> <li><strong>訓練可能パラメータの減少</strong> - 以前は5,288,548の訓練可能パラメータがありました。しかし、モデルの多くの層を凍結し、<code>classifier</code>のみを訓練可能のままにしたため、現在は3,843の訓練可能パラメータのみです（TinyVGGモデルよりもさらに少ない）。ただし、4,007,548の非訓練可能パラメータもあり、これらは<code>classifier</code>層に供給するための入力画像の基本表現を作成します。</li> </ul> <blockquote> <p><strong>注意：</strong> モデルの訓練可能パラメータが多いほど、より多くの計算力/より長い訓練時間が必要です。モデルのベース層を凍結し、より少ない訓練可能パラメータを残すことで、モデルはかなり高速に訓練されるはずです。これは転移学習の大きな利点の1つで、自分の問題に類似した問題で訓練されたモデルの既に学習済みパラメータを取得し、自分の問題に合うように出力をわずかに調整するだけです。</p> </blockquote> <h2 id=4>4. モデル訓練<a class=headerlink href=#4 title="Permanent link">&para;</a></h2> <p>半凍結され、カスタマイズされた<code>classifier</code>を持つ事前学習済みモデルができたので、転移学習の実際の効果を見てみましょう。</p> <p>訓練を開始するために、損失関数とオプティマイザーを作成しましょう。</p> <p>まだマルチクラス分類に取り組んでいるので、損失関数には<code>nn.CrossEntropyLoss()</code>を使用します。</p> <p>オプティマイザーは<code>lr=0.001</code>で<code>torch.optim.Adam()</code>を継続して使用します。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=c1># 損失とオプティマイザーを定義</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=c1># ランダムシードを設定</span>
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a><span class=n>torch</span><span class=o>.</span><span class=n>mps</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a>
</span><span id=__span-26-5><a id=__codelineno-26-5 name=__codelineno-26-5 href=#__codelineno-26-5></a><span class=c1># タイマーを開始</span>
</span><span id=__span-26-6><a id=__codelineno-26-6 name=__codelineno-26-6 href=#__codelineno-26-6></a><span class=kn>from</span><span class=w> </span><span class=nn>timeit</span><span class=w> </span><span class=kn>import</span> <span class=n>default_timer</span> <span class=k>as</span> <span class=n>timer</span> 
</span><span id=__span-26-7><a id=__codelineno-26-7 name=__codelineno-26-7 href=#__codelineno-26-7></a><span class=n>start_time</span> <span class=o>=</span> <span class=n>timer</span><span class=p>()</span>
</span><span id=__span-26-8><a id=__codelineno-26-8 name=__codelineno-26-8 href=#__codelineno-26-8></a>
</span><span id=__span-26-9><a id=__codelineno-26-9 name=__codelineno-26-9 href=#__codelineno-26-9></a><span class=c1># 訓練を設定し、結果を保存</span>
</span><span id=__span-26-10><a id=__codelineno-26-10 name=__codelineno-26-10 href=#__codelineno-26-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>),</span>
</span><span id=__span-26-11><a id=__codelineno-26-11 name=__codelineno-26-11 href=#__codelineno-26-11></a>                       <span class=n>train_dataloader</span><span class=o>=</span><span class=n>train_dataloader</span><span class=p>,</span>
</span><span id=__span-26-12><a id=__codelineno-26-12 name=__codelineno-26-12 href=#__codelineno-26-12></a>                       <span class=n>test_dataloader</span><span class=o>=</span><span class=n>test_dataloader</span><span class=p>,</span>
</span><span id=__span-26-13><a id=__codelineno-26-13 name=__codelineno-26-13 href=#__codelineno-26-13></a>                       <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-26-14><a id=__codelineno-26-14 name=__codelineno-26-14 href=#__codelineno-26-14></a>                       <span class=n>loss_fn</span><span class=o>=</span><span class=n>loss_fn</span><span class=p>,</span>
</span><span id=__span-26-15><a id=__codelineno-26-15 name=__codelineno-26-15 href=#__codelineno-26-15></a>                       <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span><span id=__span-26-16><a id=__codelineno-26-16 name=__codelineno-26-16 href=#__codelineno-26-16></a>                       <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span>
</span><span id=__span-26-17><a id=__codelineno-26-17 name=__codelineno-26-17 href=#__codelineno-26-17></a>                       <span class=n>writer</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span><span id=__span-26-18><a id=__codelineno-26-18 name=__codelineno-26-18 href=#__codelineno-26-18></a>
</span><span id=__span-26-19><a id=__codelineno-26-19 name=__codelineno-26-19 href=#__codelineno-26-19></a><span class=c1># タイマーを終了し、かかった時間を印刷</span>
</span><span id=__span-26-20><a id=__codelineno-26-20 name=__codelineno-26-20 href=#__codelineno-26-20></a><span class=n>end_time</span> <span class=o>=</span> <span class=n>timer</span><span class=p>()</span>
</span><span id=__span-26-21><a id=__codelineno-26-21 name=__codelineno-26-21 href=#__codelineno-26-21></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;[INFO] 総訓練時間: </span><span class=si>{</span><span class=n>end_time</span><span class=o>-</span><span class=n>start_time</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> 秒&quot;</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a>  0%|          | 0/5 [00:00&lt;?, ?it/s]/Users/vinsmoke/miniconda3/envs/deep-learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: &#39;pin_memory&#39; argument is set as true but not supported on MPS now, then device pinned memory won&#39;t be used.
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a>  warnings.warn(warn_msg)
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a> 20%|██        | 1/5 [00:02&lt;00:09,  2.35s/it]
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a>Epoch: 1 | train_loss: 1.0823 | train_acc: 0.4062 | test_loss: 0.8991 | test_acc: 0.5909
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a>
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a> 40%|████      | 2/5 [00:03&lt;00:05,  1.71s/it]
</span><span id=__span-27-8><a id=__codelineno-27-8 name=__codelineno-27-8 href=#__codelineno-27-8></a>
</span><span id=__span-27-9><a id=__codelineno-27-9 name=__codelineno-27-9 href=#__codelineno-27-9></a>Epoch: 2 | train_loss: 0.8564 | train_acc: 0.7695 | test_loss: 0.7927 | test_acc: 0.8456
</span><span id=__span-27-10><a id=__codelineno-27-10 name=__codelineno-27-10 href=#__codelineno-27-10></a>
</span><span id=__span-27-11><a id=__codelineno-27-11 name=__codelineno-27-11 href=#__codelineno-27-11></a> 60%|██████    | 3/5 [00:04&lt;00:03,  1.51s/it]
</span><span id=__span-27-12><a id=__codelineno-27-12 name=__codelineno-27-12 href=#__codelineno-27-12></a>
</span><span id=__span-27-13><a id=__codelineno-27-13 name=__codelineno-27-13 href=#__codelineno-27-13></a>Epoch: 3 | train_loss: 0.7914 | train_acc: 0.7891 | test_loss: 0.7373 | test_acc: 0.8561
</span><span id=__span-27-14><a id=__codelineno-27-14 name=__codelineno-27-14 href=#__codelineno-27-14></a>
</span><span id=__span-27-15><a id=__codelineno-27-15 name=__codelineno-27-15 href=#__codelineno-27-15></a> 80%|████████  | 4/5 [00:06&lt;00:01,  1.41s/it]
</span><span id=__span-27-16><a id=__codelineno-27-16 name=__codelineno-27-16 href=#__codelineno-27-16></a>
</span><span id=__span-27-17><a id=__codelineno-27-17 name=__codelineno-27-17 href=#__codelineno-27-17></a>Epoch: 4 | train_loss: 0.7206 | train_acc: 0.7500 | test_loss: 0.6338 | test_acc: 0.8759
</span><span id=__span-27-18><a id=__codelineno-27-18 name=__codelineno-27-18 href=#__codelineno-27-18></a>
</span><span id=__span-27-19><a id=__codelineno-27-19 name=__codelineno-27-19 href=#__codelineno-27-19></a>100%|██████████| 5/5 [00:07&lt;00:00,  1.48s/it]
</span><span id=__span-27-20><a id=__codelineno-27-20 name=__codelineno-27-20 href=#__codelineno-27-20></a>
</span><span id=__span-27-21><a id=__codelineno-27-21 name=__codelineno-27-21 href=#__codelineno-27-21></a>Epoch: 5 | train_loss: 0.6368 | train_acc: 0.7812 | test_loss: 0.6190 | test_acc: 0.8665
</span><span id=__span-27-22><a id=__codelineno-27-22 name=__codelineno-27-22 href=#__codelineno-27-22></a>[INFO] 総訓練時間: 7.524 秒
</span></code></pre></div> <p>うわあ！モデルはかなり高速に訓練されました。</p> <p>そして、以前のモデル結果を大幅に上回ったようです！</p> <p><code>efficientnet_b0</code>バックボーンを使用することで、モデルはテストデータセットでほぼ85%+の精度を達成し、TinyVGGで達成できたものの<em>約2倍</em>です。</p> <p>数行のコードでダウンロードしたモデルとしては悪くありません。</p> <h2 id=5>5. 損失曲線の可視化によるモデル評価<a class=headerlink href=#5 title="Permanent link">&para;</a></h2> <p>モデルはかなり良好に実行されているようです。</p> <p>時間の経過とともに訓練がどのようになっているかを確認するために、損失曲線をプロットしてみましょう。</p> <p><code>plot_loss_curves()</code>関数を使用して損失曲線をプロットできます。</p> <p>この関数は<a href=https://github.com/vinsmoke-three/deeplearning-with-pytorch/blob/main/helper_functions.py><code>helper_functions.py</code></a>スクリプトに保存されているので、インポートを試み、持っていない場合はスクリプトをダウンロードします。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=c1># helper_functions.pyからplot_loss_curves()関数を取得</span>
</span><span id=__span-28-2><a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a><span class=kn>from</span><span class=w> </span><span class=nn>helper_functions</span><span class=w> </span><span class=kn>import</span> <span class=n>plot_loss_curves</span>
</span><span id=__span-28-3><a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a>
</span><span id=__span-28-4><a id=__codelineno-28-4 name=__codelineno-28-4 href=#__codelineno-28-4></a><span class=c1># モデルの損失曲線をプロット</span>
</span><span id=__span-28-5><a id=__codelineno-28-5 name=__codelineno-28-5 href=#__codelineno-28-5></a><span class=n>plot_loss_curves</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</span></code></pre></div> <p><img alt=損失曲線 src=../07_pytorch_transfer_learning_files/07_pytorch_transfer_learning_44_0.png></p> <p>これらは素晴らしい損失曲線です！</p> <p>両方のデータセット（訓練とテスト）の損失が正しい方向に向かっているようです。</p> <p>精度値についても同様で、上昇傾向にあります。</p> <p>これは<strong>転移学習</strong>の力を示しています。事前学習済みモデルを使用することで、少ないデータで短時間でかなり良い結果を得ることができます。</p> <p>モデルをより長く訓練したり、より多くのデータを追加したりすると何が起こるか気になりませんか？</p> <h2 id=6>6. テストセットの画像での予測<a class=headerlink href=#6 title="Permanent link">&para;</a></h2> <p>モデルは定量的には良好に実行されているようですが、定性的にはどうでしょうか？</p> <p>テストセット（訓練中に見られない）の画像でモデルを使って予測を行い、それらをプロットして確認してみましょう。</p> <p><em>可視化、可視化、可視化！</em></p> <p>モデルが画像で予測を行うために覚えておくべきことの1つは、画像がモデルが訓練された画像と<em>同じ</em>形式である必要があることです。</p> <p>これは、画像が以下を持つ必要があることを意味します：</p> <ul> <li><strong>同じ形状</strong> - 画像がモデルが訓練されたものと異なる形状の場合、形状エラーが発生します</li> <li><strong>同じデータ型</strong> - 画像が異なるデータ型（例：<code>torch.int8</code> vs <code>torch.float32</code>）の場合、データ型エラーが発生します</li> <li><strong>同じデバイス</strong> - 画像がモデルと異なるデバイス上にある場合、デバイスエラーが発生します</li> <li><strong>同じ変換</strong> - モデルが特定の方法で変換された画像（例：特定の平均と標準偏差で正規化）で訓練され、異なる方法で変換された画像で予測を試みる場合、これらの予測は外れる可能性があります</li> </ul> <blockquote> <p><strong>注意：</strong> これらの要件は、訓練済みモデルで予測を行う場合のあらゆる種類のデータに適用されます。予測したいデータは、モデルが訓練されたのと同じ形式である必要があります。</p> </blockquote> <p>これらすべてを実行するために、以下を行う<code>pred_and_plot_image()</code>関数を作成します：</p> <ol> <li>訓練済みモデル、クラス名のリスト、ターゲット画像へのファイルパス、画像サイズ、変換、ターゲットデバイスを受け取る</li> <li><a href=https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open><code>PIL.Image.open()</code></a>で画像を開く</li> <li>画像の変換を作成（これは上記で作成した<code>manual_transforms</code>にデフォルトするか、<code>weights.transforms()</code>から生成された変換を使用できます）</li> <li>モデルがターゲットデバイス上にあることを確認</li> <li><code>model.eval()</code>でモデル評価モードをオンにし（これは推論では使用されない<code>nn.Dropout()</code>などの層をオフにします）、推論モードコンテキストマネージャーを使用</li> <li>ステップ3で作成した変換でターゲット画像を変換し、<code>torch.unsqueeze(dim=0)</code>で追加のバッチ次元を追加して、入力画像が<code>[batch_size, color_channels, height, width]</code>の形状になるようにする</li> <li>画像をモデルに渡してターゲットデバイス上にあることを確認し、画像で予測を行う</li> <li><code>torch.softmax()</code>でモデルの出力ロジットを予測確率に変換</li> <li><code>torch.argmax()</code>でモデルの予測確率を予測ラベルに変換</li> <li><code>matplotlib</code>で画像をプロットし、タイトルをステップ9の予測ラベルとステップ8の予測確率に設定</li> </ol> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a><span class=kn>from</span><span class=w> </span><span class=nn>PIL</span><span class=w> </span><span class=kn>import</span> <span class=n>Image</span>
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a>
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a><span class=c1># 1. 訓練済みモデル、クラス名、画像パス、画像サイズ、変換、ターゲットデバイスを受け取る</span>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a><span class=k>def</span><span class=w> </span><span class=nf>pred_and_plot_image</span><span class=p>(</span><span class=n>model</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a>                        <span class=n>image_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> 
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a>                        <span class=n>class_names</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span>
</span><span id=__span-29-9><a id=__codelineno-29-9 name=__codelineno-29-9 href=#__codelineno-29-9></a>                        <span class=n>image_size</span><span class=p>:</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>),</span>
</span><span id=__span-29-10><a id=__codelineno-29-10 name=__codelineno-29-10 href=#__codelineno-29-10></a>                        <span class=n>transform</span><span class=p>:</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-29-11><a id=__codelineno-29-11 name=__codelineno-29-11 href=#__codelineno-29-11></a>                        <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>):</span>
</span><span id=__span-29-12><a id=__codelineno-29-12 name=__codelineno-29-12 href=#__codelineno-29-12></a>
</span><span id=__span-29-13><a id=__codelineno-29-13 name=__codelineno-29-13 href=#__codelineno-29-13></a>    <span class=c1># 2. 画像を開く</span>
</span><span id=__span-29-14><a id=__codelineno-29-14 name=__codelineno-29-14 href=#__codelineno-29-14></a>    <span class=n>img</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=n>image_path</span><span class=p>)</span>
</span><span id=__span-29-15><a id=__codelineno-29-15 name=__codelineno-29-15 href=#__codelineno-29-15></a>
</span><span id=__span-29-16><a id=__codelineno-29-16 name=__codelineno-29-16 href=#__codelineno-29-16></a>    <span class=c1># 3. 画像の変換を作成（存在しない場合）</span>
</span><span id=__span-29-17><a id=__codelineno-29-17 name=__codelineno-29-17 href=#__codelineno-29-17></a>    <span class=k>if</span> <span class=n>transform</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-29-18><a id=__codelineno-29-18 name=__codelineno-29-18 href=#__codelineno-29-18></a>        <span class=n>image_transform</span> <span class=o>=</span> <span class=n>transform</span>
</span><span id=__span-29-19><a id=__codelineno-29-19 name=__codelineno-29-19 href=#__codelineno-29-19></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-29-20><a id=__codelineno-29-20 name=__codelineno-29-20 href=#__codelineno-29-20></a>        <span class=n>image_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-29-21><a id=__codelineno-29-21 name=__codelineno-29-21 href=#__codelineno-29-21></a>            <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>(</span><span class=n>image_size</span><span class=p>),</span>
</span><span id=__span-29-22><a id=__codelineno-29-22 name=__codelineno-29-22 href=#__codelineno-29-22></a>            <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span><span id=__span-29-23><a id=__codelineno-29-23 name=__codelineno-29-23 href=#__codelineno-29-23></a>            <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span>
</span><span id=__span-29-24><a id=__codelineno-29-24 name=__codelineno-29-24 href=#__codelineno-29-24></a>                                 <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>]),</span>
</span><span id=__span-29-25><a id=__codelineno-29-25 name=__codelineno-29-25 href=#__codelineno-29-25></a>        <span class=p>])</span>
</span><span id=__span-29-26><a id=__codelineno-29-26 name=__codelineno-29-26 href=#__codelineno-29-26></a>
</span><span id=__span-29-27><a id=__codelineno-29-27 name=__codelineno-29-27 href=#__codelineno-29-27></a>    <span class=c1>### 画像で予測 ### </span>
</span><span id=__span-29-28><a id=__codelineno-29-28 name=__codelineno-29-28 href=#__codelineno-29-28></a>
</span><span id=__span-29-29><a id=__codelineno-29-29 name=__codelineno-29-29 href=#__codelineno-29-29></a>    <span class=c1># 4. モデルがターゲットデバイス上にあることを確認</span>
</span><span id=__span-29-30><a id=__codelineno-29-30 name=__codelineno-29-30 href=#__codelineno-29-30></a>    <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-29-31><a id=__codelineno-29-31 name=__codelineno-29-31 href=#__codelineno-29-31></a>
</span><span id=__span-29-32><a id=__codelineno-29-32 name=__codelineno-29-32 href=#__codelineno-29-32></a>    <span class=c1># 5. モデル評価モードと推論モードをオンにする</span>
</span><span id=__span-29-33><a id=__codelineno-29-33 name=__codelineno-29-33 href=#__codelineno-29-33></a>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span><span id=__span-29-34><a id=__codelineno-29-34 name=__codelineno-29-34 href=#__codelineno-29-34></a>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>inference_mode</span><span class=p>():</span>
</span><span id=__span-29-35><a id=__codelineno-29-35 name=__codelineno-29-35 href=#__codelineno-29-35></a>      <span class=c1># 6. 画像を変換し、追加の次元を追加（モデルは[batch_size, color_channels, height, width]のサンプルを必要とします）</span>
</span><span id=__span-29-36><a id=__codelineno-29-36 name=__codelineno-29-36 href=#__codelineno-29-36></a>      <span class=n>transformed_image</span> <span class=o>=</span> <span class=n>image_transform</span><span class=p>(</span><span class=n>img</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-29-37><a id=__codelineno-29-37 name=__codelineno-29-37 href=#__codelineno-29-37></a>
</span><span id=__span-29-38><a id=__codelineno-29-38 name=__codelineno-29-38 href=#__codelineno-29-38></a>      <span class=c1># 7. 追加の次元を持つ画像で予測を行い、ターゲットデバイスに送信</span>
</span><span id=__span-29-39><a id=__codelineno-29-39 name=__codelineno-29-39 href=#__codelineno-29-39></a>      <span class=n>target_image_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>transformed_image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>))</span>
</span><span id=__span-29-40><a id=__codelineno-29-40 name=__codelineno-29-40 href=#__codelineno-29-40></a>
</span><span id=__span-29-41><a id=__codelineno-29-41 name=__codelineno-29-41 href=#__codelineno-29-41></a>    <span class=c1># 8. ロジット -&gt; 予測確率に変換（マルチクラス分類にはtorch.softmax()を使用）</span>
</span><span id=__span-29-42><a id=__codelineno-29-42 name=__codelineno-29-42 href=#__codelineno-29-42></a>    <span class=n>target_image_pred_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>target_image_pred</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-29-43><a id=__codelineno-29-43 name=__codelineno-29-43 href=#__codelineno-29-43></a>
</span><span id=__span-29-44><a id=__codelineno-29-44 name=__codelineno-29-44 href=#__codelineno-29-44></a>    <span class=c1># 9. 予測確率 -&gt; 予測ラベルに変換</span>
</span><span id=__span-29-45><a id=__codelineno-29-45 name=__codelineno-29-45 href=#__codelineno-29-45></a>    <span class=n>target_image_pred_label</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>target_image_pred_probs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-29-46><a id=__codelineno-29-46 name=__codelineno-29-46 href=#__codelineno-29-46></a>
</span><span id=__span-29-47><a id=__codelineno-29-47 name=__codelineno-29-47 href=#__codelineno-29-47></a>    <span class=c1># 10. 予測ラベルと確率で画像をプロット</span>
</span><span id=__span-29-48><a id=__codelineno-29-48 name=__codelineno-29-48 href=#__codelineno-29-48></a>    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
</span><span id=__span-29-49><a id=__codelineno-29-49 name=__codelineno-29-49 href=#__codelineno-29-49></a>    <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
</span><span id=__span-29-50><a id=__codelineno-29-50 name=__codelineno-29-50 href=#__codelineno-29-50></a>    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;予測: </span><span class=si>{</span><span class=n>class_names</span><span class=p>[</span><span class=n>target_image_pred_label</span><span class=p>]</span><span class=si>}</span><span class=s2> | 確率: </span><span class=si>{</span><span class=n>target_image_pred_probs</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-29-51><a id=__codelineno-29-51 name=__codelineno-29-51 href=#__codelineno-29-51></a>    <span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></code></pre></div> <p>テストセットからいくつかのランダムな画像で予測を行ってテストしてみましょう。</p> <p><code>list(Path(test_dir).glob("*/*.jpg"))</code>を使用してすべてのテスト画像パスのリストを取得できます。<code>glob()</code>メソッドの星は「このパターンに一致するファイル」を意味し、つまり<code>.jpg</code>で終わるファイル（すべての画像）を指します。</p> <p>そして、Pythonの<a href=https://docs.python.org/3/library/random.html#random.sample><code>random.sample(population, k)</code></a>を使用してこれらの一部をランダムにサンプリングできます。ここで<code>population</code>はサンプリングするシーケンス、<code>k</code>は取得するサンプル数です。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=c1># テストセットからランダムな画像パスのリストを取得</span>
</span><span id=__span-30-2><a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a><span class=kn>import</span><span class=w> </span><span class=nn>random</span>
</span><span id=__span-30-3><a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a><span class=n>num_images_to_plot</span> <span class=o>=</span> <span class=mi>3</span>
</span><span id=__span-30-4><a id=__codelineno-30-4 name=__codelineno-30-4 href=#__codelineno-30-4></a><span class=n>test_image_path_list</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>Path</span><span class=p>(</span><span class=n>test_dir</span><span class=p>)</span><span class=o>.</span><span class=n>glob</span><span class=p>(</span><span class=s2>&quot;*/*.jpg&quot;</span><span class=p>))</span> <span class=c1># テストデータからすべての画像パスのリストを取得</span>
</span><span id=__span-30-5><a id=__codelineno-30-5 name=__codelineno-30-5 href=#__codelineno-30-5></a><span class=n>test_image_path_sample</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>population</span><span class=o>=</span><span class=n>test_image_path_list</span><span class=p>,</span> <span class=c1># すべてのテスト画像パスを通る</span>
</span><span id=__span-30-6><a id=__codelineno-30-6 name=__codelineno-30-6 href=#__codelineno-30-6></a>                                       <span class=n>k</span><span class=o>=</span><span class=n>num_images_to_plot</span><span class=p>)</span> <span class=c1># 予測とプロットのために &#39;k&#39; 個の画像パスをランダムに選択</span>
</span><span id=__span-30-7><a id=__codelineno-30-7 name=__codelineno-30-7 href=#__codelineno-30-7></a>
</span><span id=__span-30-8><a id=__codelineno-30-8 name=__codelineno-30-8 href=#__codelineno-30-8></a><span class=c1># 画像で予測を行いプロット</span>
</span><span id=__span-30-9><a id=__codelineno-30-9 name=__codelineno-30-9 href=#__codelineno-30-9></a><span class=k>for</span> <span class=n>image_path</span> <span class=ow>in</span> <span class=n>test_image_path_sample</span><span class=p>:</span>
</span><span id=__span-30-10><a id=__codelineno-30-10 name=__codelineno-30-10 href=#__codelineno-30-10></a>    <span class=n>pred_and_plot_image</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> 
</span><span id=__span-30-11><a id=__codelineno-30-11 name=__codelineno-30-11 href=#__codelineno-30-11></a>                        <span class=n>image_path</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>image_path</span><span class=p>),</span>
</span><span id=__span-30-12><a id=__codelineno-30-12 name=__codelineno-30-12 href=#__codelineno-30-12></a>                        <span class=n>class_names</span><span class=o>=</span><span class=n>class_names</span><span class=p>,</span>
</span><span id=__span-30-13><a id=__codelineno-30-13 name=__codelineno-30-13 href=#__codelineno-30-13></a>                        <span class=c1># transform=weights.transforms(), # オプションで事前学習済みモデル重みから指定された変換を渡す</span>
</span><span id=__span-30-14><a id=__codelineno-30-14 name=__codelineno-30-14 href=#__codelineno-30-14></a>                        <span class=n>image_size</span><span class=o>=</span><span class=p>(</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>))</span>
</span></code></pre></div> <p><img alt=予測結果1 src=../07_pytorch_transfer_learning_files/07_pytorch_transfer_learning_49_0.png> <img alt=予測結果2 src=../07_pytorch_transfer_learning_files/07_pytorch_transfer_learning_49_1.png> <img alt=予測結果3 src=../07_pytorch_transfer_learning_files/07_pytorch_transfer_learning_49_2.png></p> <p>わーい！これらの予測は、TinyVGGモデルが以前に行っていたものよりもはるかに良く見えます。</p> <h3 id=61>6.1 カスタム画像での予測<a class=headerlink href=#61 title="Permanent link">&para;</a></h3> <p>モデルはテストセットのデータで定性的に良好に実行されているようです。しかし、自分のカスタム画像ではどうでしょうか？</p> <p>ここに機械学習の本当の楽しさがあります！訓練やテストセット以外の自分のカスタムデータで予測を行うことです。</p> <p>カスタム画像でモデルをテストするために、信頼できる<code>sushi.jpg</code>画像（寿司の画像）をインポートしましょう。</p> <p>そして、上記で作成した<code>pred_and_plot_image()</code>関数に渡して何が起こるかを見てみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=c1># カスタム画像をダウンロード</span>
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a><span class=kn>import</span><span class=w> </span><span class=nn>requests</span>
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>
</span><span id=__span-31-4><a id=__codelineno-31-4 name=__codelineno-31-4 href=#__codelineno-31-4></a><span class=c1># カスタム画像パスを設定</span>
</span><span id=__span-31-5><a id=__codelineno-31-5 name=__codelineno-31-5 href=#__codelineno-31-5></a><span class=n>custom_image_path</span> <span class=o>=</span> <span class=n>data_path</span> <span class=o>/</span> <span class=s2>&quot;sushi.jpg&quot;</span>
</span><span id=__span-31-6><a id=__codelineno-31-6 name=__codelineno-31-6 href=#__codelineno-31-6></a>
</span><span id=__span-31-7><a id=__codelineno-31-7 name=__codelineno-31-7 href=#__codelineno-31-7></a><span class=c1># カスタム画像で予測</span>
</span><span id=__span-31-8><a id=__codelineno-31-8 name=__codelineno-31-8 href=#__codelineno-31-8></a><span class=n>pred_and_plot_image</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-31-9><a id=__codelineno-31-9 name=__codelineno-31-9 href=#__codelineno-31-9></a>                    <span class=n>image_path</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>custom_image_path</span><span class=p>),</span>
</span><span id=__span-31-10><a id=__codelineno-31-10 name=__codelineno-31-10 href=#__codelineno-31-10></a>                    <span class=n>class_names</span><span class=o>=</span><span class=n>class_names</span><span class=p>)</span>
</span></code></pre></div> <p><img alt=カスタム画像での予測結果 src=../07_pytorch_transfer_learning_files/07_pytorch_transfer_learning_52_0.png></p> <p>素晴らしい！モデルは寿司の画像を高い確信度で正しく分類しました。</p> <h2 id=_8>まとめ<a class=headerlink href=#_8 title="Permanent link">&para;</a></h2> <p>この記事では、PyTorchの転移学習を使用して画像分類モデルの性能を劇的に向上させる方法を学びました。</p> <h3 id=_9>主要な学習ポイント<a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <ul> <li><strong>転移学習の威力</strong> - 比較的少ないカスタムデータで優秀な結果を得ることができる</li> <li><strong>事前学習済みモデルの活用</strong> - ImageNetで訓練されたEfficientNet_B0を使用して85%以上の精度を達成</li> <li><strong>層の凍結とカスタマイズ</strong> - ベース層を凍結し、分類器層のみを調整することで効率的な訓練を実現</li> <li><strong>適切なデータ前処理</strong> - 事前学習済みモデルと同じ変換を適用することの重要性</li> </ul> <h3 id=_10>技術的なハイライト<a class=headerlink href=#_10 title="Permanent link">&para;</a></h3> <ul> <li><strong>パラメータ数の比較</strong>: TinyVGG（8,083パラメータ）vs EfficientNet_B0（5,288,548パラメータ）</li> <li><strong>訓練時間</strong>: わずか7.5秒で5エポック完了</li> <li><strong>精度向上</strong>: 前回の約50%から85%以上へと大幅改善</li> <li><strong>訓練可能パラメータ</strong>: 層の凍結により3,843パラメータのみが更新対象</li> </ul> <h3 id=_11>実装で使用した主要技術<a class=headerlink href=#_11 title="Permanent link">&para;</a></h3> <ol> <li><strong><code>torchvision.models</code>からの事前学習済みモデル取得</strong></li> <li><strong>特徴量抽出器の凍結</strong> (<code>requires_grad=False</code>)</li> <li><strong>分類器ヘッドのカスタマイズ</strong> (出力次元を3に変更)</li> <li><strong>適切なデータ変換</strong> (ImageNetと同じ正規化)</li> <li><strong>効率的な訓練戦略</strong> (少ない訓練可能パラメータ)</li> </ol> <h2 id=_12>重要なポイント<a class=headerlink href=#_12 title="Permanent link">&para;</a></h2> <ul> <li><strong>転移学習</strong>は、比較的少ないカスタムデータで優れた結果を得ることを可能にします</li> <li>転移学習の力を知ることで、すべての問題の開始時に「自分の問題に対する既存の高性能モデルは存在するか？」と問うことが良いアイデアです</li> <li>事前学習済みモデルを使用する場合、カスタムデータが元のモデルが訓練されたのと同じ方法でフォーマット/前処理されていることが重要です。そうでないと性能が低下する可能性があります</li> <li>カスタムデータで予測する場合も同様で、カスタムデータがモデルが訓練されたデータと同じ形式であることを確認してください</li> <li>PyTorchドメインライブラリ、HuggingFace Hub、<code>timm</code>（PyTorch Image Models）などのライブラリから事前学習済みモデルを見つけることができる場所がいくつかあります</li> </ul> <p>転移学習は現代の深層学習における最も強力な技術の一つです。この知識を活用して、あなた自身の画像分類プロジェクトに取り組んでみてください！</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/meta.js></script> <script src=../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>