<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PyTorchを使ってVision Transformer（ViT）論文を再現し、食べ物画像分類問題に適用する実践的なチュートリアル"><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/PyTorch/09_pytorch_paper_replicating/ rel=canonical><link href=../08_pytorch_experiment_tracking/ rel=prev><link href=../10_pytorch_model_deployment/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>Vision Transformerを一から実装してFoodVision Miniに適用する - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#vision-transformerfoodvision-mini class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Vision Transformerを一から実装してFoodVision Miniに適用する </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class=md-tabs__item> <a href=../../LLM/00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class=md-nav__item> <a href=../07_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 9. PyTorch paper replicating </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 論文再現実装とは？ </span> </a> </li> <li class=md-nav__item> <a href=#vision-transformer class=md-nav__link> <span class=md-ellipsis> Vision Transformerについて </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 実装内容 </span> </a> </li> <li class=md-nav__item> <a href=#3-vit class=md-nav__link> <span class=md-ellipsis> 3. ViT論文の再現実装：概要 </span> </a> </li> <li class=md-nav__item> <a href=#4-1 class=md-nav__link> <span class=md-ellipsis> 4. 数式1の実装：パッチ分割とクラス・位置・パッチ埋め込みの作成 </span> </a> </li> <li class=md-nav__item> <a href=#5-2msa class=md-nav__link> <span class=md-ellipsis> 5. 数式2の実装：マルチヘッドアテンション（MSA） </span> </a> </li> <li class=md-nav__item> <a href=#6-3mlp class=md-nav__link> <span class=md-ellipsis> 6. 数式3の実装：多層パーセプトロン（MLP） </span> </a> </li> <li class=md-nav__item> <a href=#7-transformer-encoder class=md-nav__link> <span class=md-ellipsis> 7. Transformer Encoderの作成 </span> </a> </li> <li class=md-nav__item> <a href=#8-vit class=md-nav__link> <span class=md-ellipsis> 8. すべてをまとめてViTを作成 </span> </a> </li> <li class=md-nav__item> <a href=#9-vit class=md-nav__link> <span class=md-ellipsis> 9. ViTモデルの訓練コードの設定 </span> </a> </li> <li class=md-nav__item> <a href=#10-torchvisionmodelsvit class=md-nav__link> <span class=md-ellipsis> 10. torchvision.modelsの事前訓練済みViTを同じデータセットで使用 </span> </a> </li> <li class=md-nav__item> <a href=#11 class=md-nav__link> <span class=md-ellipsis> 11. カスタム画像で予測を行う </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 主な要点 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 追加学習リソース </span> </a> </li> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 注意事項 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 参考文献 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../LLM/00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/02_using_transformers/ class=md-nav__link> <span class=md-ellipsis> 2. Using transformers </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/03_fine_tuning_a_pretrained_model/ class=md-nav__link> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/04_the_huggingface_tokenizers_library/ class=md-nav__link> <span class=md-ellipsis> 4. Tokenizers library </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/72_masked_language_modeling/ class=md-nav__link> <span class=md-ellipsis> Masked Language Modeling </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../../LLM/ClassicalNLP/75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 論文再現実装とは？ </span> </a> </li> <li class=md-nav__item> <a href=#vision-transformer class=md-nav__link> <span class=md-ellipsis> Vision Transformerについて </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 実装内容 </span> </a> </li> <li class=md-nav__item> <a href=#3-vit class=md-nav__link> <span class=md-ellipsis> 3. ViT論文の再現実装：概要 </span> </a> </li> <li class=md-nav__item> <a href=#4-1 class=md-nav__link> <span class=md-ellipsis> 4. 数式1の実装：パッチ分割とクラス・位置・パッチ埋め込みの作成 </span> </a> </li> <li class=md-nav__item> <a href=#5-2msa class=md-nav__link> <span class=md-ellipsis> 5. 数式2の実装：マルチヘッドアテンション（MSA） </span> </a> </li> <li class=md-nav__item> <a href=#6-3mlp class=md-nav__link> <span class=md-ellipsis> 6. 数式3の実装：多層パーセプトロン（MLP） </span> </a> </li> <li class=md-nav__item> <a href=#7-transformer-encoder class=md-nav__link> <span class=md-ellipsis> 7. Transformer Encoderの作成 </span> </a> </li> <li class=md-nav__item> <a href=#8-vit class=md-nav__link> <span class=md-ellipsis> 8. すべてをまとめてViTを作成 </span> </a> </li> <li class=md-nav__item> <a href=#9-vit class=md-nav__link> <span class=md-ellipsis> 9. ViTモデルの訓練コードの設定 </span> </a> </li> <li class=md-nav__item> <a href=#10-torchvisionmodelsvit class=md-nav__link> <span class=md-ellipsis> 10. torchvision.modelsの事前訓練済みViTを同じデータセットで使用 </span> </a> </li> <li class=md-nav__item> <a href=#11 class=md-nav__link> <span class=md-ellipsis> 11. カスタム画像で予測を行う </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 主な要点 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 追加学習リソース </span> </a> </li> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 注意事項 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 参考文献 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=vision-transformerfoodvision-mini>Vision Transformerを一から実装してFoodVision Miniに適用する<a class=headerlink href=#vision-transformerfoodvision-mini title="Permanent link">&para;</a></h1> <h2 id=_1>概要<a class=headerlink href=#_1 title="Permanent link">&para;</a></h2> <p>本記事では、機械学習論文の再現実装について実践的に学びます。具体的には、<strong>Vision Transformer（ViT）</strong>を一から実装し、私たちのFoodVision Mini問題（ピザ、ステーキ、寿司の画像分類）に適用します。</p> <p>論文の再現実装は、最新の機械学習技術を理解し、自分の問題に応用するための重要なスキルです。この記事を通じて、研究論文を読み解き、数式とテキストをPyTorchコードに変換する方法を習得できます。</p> <p><img alt="Vision Transformer適用例" src=../09_pytorch_paper_replicating_files/09-vit-paper-applying-vit-to-food-vision-mini.png></p> <p><em>マイルストーンプロジェクト2では、Vision Transformer（ViT）アーキテクチャを再現し、FoodVision Mini問題に適用してピザ、ステーキ、寿司の画像を分類します。</em></p> <h2 id=_2>前提知識<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <ul> <li>PyTorchの基本的な使い方</li> <li>畳み込みニューラルネットワーク（CNN）の理解</li> <li>深層学習の基礎概念</li> <li>Python プログラミングの経験</li> </ul> <h2 id=_3>学習目標<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <p>この記事を読み終える頃には、以下のことができるようになります：</p> <ol> <li>機械学習論文を読み解く方法を理解する</li> <li>Vision Transformerアーキテクチャの仕組みを把握する</li> <li>論文の数式をPyTorchコードに変換する</li> <li>一から実装したViTモデルでFoodVision Mini問題を解く</li> </ol> <h2 id=_4>論文再現実装とは？<a class=headerlink href=#_4 title="Permanent link">&para;</a></h2> <p>機械学習の分野は急速に進歩しており、多くの革新的な技術が研究論文として発表されています。</p> <p><strong>論文再現実装</strong>の目標は、これらの進歩をコードで再現し、自分の問題に応用できる技術として活用することです。</p> <p><img alt=論文実装のプロセス src=../09_pytorch_paper_replicating_files/09-vit-paper-what-is-paper-replicating-images-math-text-to-code.png></p> <p><em>機械学習論文の再現実装では、画像/図表、数式、テキストで構成された論文を実用的なコードに変換します。図、数式、テキストは<a href=https://arxiv.org/abs/2010.11929>ViT論文</a>から引用。</em></p> <p>例えば、新しいモデルアーキテクチャが既存の手法よりも優れた性能を示した場合、そのアーキテクチャを自分の問題に試してみたいと思うでしょう。</p> <h2 id=vision-transformer>Vision Transformerについて<a class=headerlink href=#vision-transformer title="Permanent link">&para;</a></h2> <p><strong>Transformer</strong>アーキテクチャは元々、論文<a href=https://arxiv.org/abs/1706.03762><em>Attention is all you need</em></a>で一次元（1D）のテキストシーケンスを処理するために設計されました。</p> <p><strong>Vision Transformer（ViT）</strong>は、このTransformerアーキテクチャを<strong>視覚問題に適応</strong>させたものです（最初は分類問題、その後多くの応用が続きました）。</p> <p>本記事では、元論文<a href=https://arxiv.org/abs/2010.11929><em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a>に基づいて、「バニラ Vision Transformer」を構築します。元のアーキテクチャを再現できれば、他のバリエーションにも対応できるからです。</p> <h2 id=_5>実装内容<a class=headerlink href=#_5 title="Permanent link">&para;</a></h2> <h3 id=0>0. 環境設定<a class=headerlink href=#0 title="Permanent link">&para;</a></h3> <p>まず、必要なライブラリとモジュールを準備します。PyTorch 1.12以上とtorchvision 0.13以上が必要です。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># 必要なバージョンの確認とインストール</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=k>try</span><span class=p>:</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>    <span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>    <span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>    <span class=k>assert</span> <span class=nb>int</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&quot;.&quot;</span><span class=p>)[</span><span class=mi>1</span><span class=p>])</span> <span class=o>&gt;=</span> <span class=mi>12</span> <span class=ow>or</span> <span class=nb>int</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&quot;.&quot;</span><span class=p>)[</span><span class=mi>0</span><span class=p>])</span> <span class=o>==</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&quot;torch version should be 1.12+&quot;</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>    <span class=k>assert</span> <span class=nb>int</span><span class=p>(</span><span class=n>torchvision</span><span class=o>.</span><span class=n>__version__</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&quot;.&quot;</span><span class=p>)[</span><span class=mi>1</span><span class=p>])</span> <span class=o>&gt;=</span> <span class=mi>13</span><span class=p>,</span> <span class=s2>&quot;torchvision version should be 0.13+&quot;</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;torch version: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;torchvision version: </span><span class=si>{</span><span class=n>torchvision</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=k>except</span><span class=p>:</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;[INFO] torch/torchvision versions not as required, installing nightly versions.&quot;</span><span class=p>)</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>    <span class=err>!</span><span class=n>pip3</span> <span class=n>install</span> <span class=o>-</span><span class=n>U</span> <span class=n>torch</span> <span class=n>torchvision</span> <span class=n>torchaudio</span> <span class=o>--</span><span class=n>index</span><span class=o>-</span><span class=n>url</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>download</span><span class=o>.</span><span class=n>pytorch</span><span class=o>.</span><span class=n>org</span><span class=o>/</span><span class=n>whl</span><span class=o>/</span><span class=n>cu118</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>    <span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>    <span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;torch version: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;torchvision version: </span><span class=si>{</span><span class=n>torchvision</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>torch version: 2.7.1
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>torchvision version: 0.22.1
</span></code></pre></div></p> <p>続いて、必要なライブラリをインポートし、デバイス設定を行います：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># 必要なライブラリのインポート</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=kn>from</span><span class=w> </span><span class=nn>torch</span><span class=w> </span><span class=kn>import</span> <span class=n>nn</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>transforms</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=kn>from</span><span class=w> </span><span class=nn>torchinfo</span><span class=w> </span><span class=kn>import</span> <span class=n>summary</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular</span><span class=w> </span><span class=kn>import</span> <span class=n>data_setup</span><span class=p>,</span> <span class=n>engine</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=kn>from</span><span class=w> </span><span class=nn>helper_functions</span><span class=w> </span><span class=kn>import</span> <span class=n>download_data</span><span class=p>,</span> <span class=n>set_seeds</span><span class=p>,</span> <span class=n>plot_loss_curves</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># デバイス設定（GPU利用可能性の確認）</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=n>device</span> <span class=o>=</span> <span class=s2>&quot;mps&quot;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>mps</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&quot;cpu&quot;</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a><span class=n>device</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>&#39;mps&#39;
</span></code></pre></div></p> <h3 id=1>1. データの準備<a class=headerlink href=#1 title="Permanent link">&para;</a></h3> <p>FoodVision Miniで使用するピザ、ステーキ、寿司の画像データセットをダウンロードします：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># GitHubからピザ、ステーキ、寿司画像をダウンロード</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>image_path</span> <span class=o>=</span> <span class=n>download_data</span><span class=p>(</span><span class=n>source</span><span class=o>=</span><span class=s2>&quot;https://github.com/vinsmoke-three/deeplearning-with-pytorch/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class=p>,</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>                           <span class=n>destination</span><span class=o>=</span><span class=s2>&quot;pizza_steak_sushi&quot;</span><span class=p>)</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>image_path</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>[INFO] data/pizza_steak_sushi directory exists, skipping download.
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>PosixPath(&#39;data/pizza_steak_sushi&#39;)
</span></code></pre></div></p> <p>データディレクトリの設定：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># 訓練用とテスト用ディレクトリのパスを設定</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=n>train_dir</span> <span class=o>=</span> <span class=n>image_path</span> <span class=o>/</span> <span class=s2>&quot;train&quot;</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=n>test_dir</span> <span class=o>=</span> <span class=n>image_path</span> <span class=o>/</span> <span class=s2>&quot;test&quot;</span>
</span></code></pre></div> <h3 id=2-dataloader>2. データセットとDataLoaderの作成<a class=headerlink href=#2-dataloader title="Permanent link">&para;</a></h3> <h4 id=21>2.1 画像変換の準備<a class=headerlink href=#21 title="Permanent link">&para;</a></h4> <p>ViT論文のTable 3によると、訓練時の解像度は224×224ピクセルです。</p> <p><img alt="ViT論文Table 3" src=../09_pytorch_paper_replicating_files/09-vit-paper-image-size-and-batch-size.png></p> <p><em>ViT論文のTable 3では、画像サイズやバッチサイズなどのハイパーパラメータ設定が示されています。</em></p> <p>この情報に基づいて画像変換を設定します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># 画像サイズの設定（ViT論文のTable 3から）</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>IMG_SIZE</span> <span class=o>=</span> <span class=mi>224</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=c1># 手動で変換パイプラインを作成</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=n>manual_transforms</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>((</span><span class=n>IMG_SIZE</span><span class=p>,</span> <span class=n>IMG_SIZE</span><span class=p>)),</span>  <span class=c1># 224x224にリサイズ</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>                    <span class=c1># テンソルに変換</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=p>])</span>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;手動作成された変換: </span><span class=si>{</span><span class=n>manual_transforms</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>手動作成された変換: Compose(
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>    ToTensor()
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>)
</span></code></pre></div></p> <h4 id=22-dataloader>2.2 DataLoaderの作成<a class=headerlink href=#22-dataloader title="Permanent link">&para;</a></h4> <p>ViT論文ではバッチサイズ4096を使用していますが、一般的なハードウェアでは処理が困難なため、バッチサイズ32を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=c1># バッチサイズの設定（ViT論文より小さく設定）</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=n>BATCH_SIZE</span> <span class=o>=</span> <span class=mi>32</span>  <span class=c1># ViT論文より小さいが、ハードウェア制約のため</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a><span class=c1># データローダーの作成</span>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span> <span class=o>=</span> <span class=n>data_setup</span><span class=o>.</span><span class=n>create_dataloaders</span><span class=p>(</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>    <span class=n>train_dir</span><span class=o>=</span><span class=n>train_dir</span><span class=p>,</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>    <span class=n>test_dir</span><span class=o>=</span><span class=n>test_dir</span><span class=p>,</span>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a>    <span class=n>transform</span><span class=o>=</span><span class=n>manual_transforms</span><span class=p>,</span>  <span class=c1># 手動作成した変換を使用</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a>    <span class=n>batch_size</span><span class=o>=</span><span class=n>BATCH_SIZE</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a><span class=p>)</span>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>class_names</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>/Users/vinsmoke/miniconda3/envs/deep-learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: &#39;pin_memory&#39; argument is set as true but not supported on MPS now, then device pinned memory won&#39;t be used.
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>  warnings.warn(warn_msg)
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>(&lt;torch.utils.data.dataloader.DataLoader at 0x3586a46b0&gt;,
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a> &lt;torch.utils.data.dataloader.DataLoader at 0x358b17620&gt;,
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a> [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</span></code></pre></div></p> <h4 id=23>2.3 単一画像の可視化<a class=headerlink href=#23 title="Permanent link">&para;</a></h4> <p>データが正しく読み込まれているか確認しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=c1># 画像バッチを取得</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>image_batch</span><span class=p>,</span> <span class=n>label_batch</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=nb>iter</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>))</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=c1># バッチから単一の画像を取得</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a><span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image_batch</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>label_batch</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=c1># バッチの形状を確認</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>label</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>(torch.Size([3, 224, 224]), tensor(2))
</span></code></pre></div></p> <p>画像を可視化：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># matplotlibで画像をプロット</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>  <span class=c1># 画像次元を調整 [color_channels, height, width] -&gt; [height, width, color_channels]</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>class_names</span><span class=p>[</span><span class=n>label</span><span class=p>])</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=kc>False</span><span class=p>);</span>
</span></code></pre></div> <p><img alt=画像サンプル src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_25_0.png></p> <p>素晴らしい！画像が正しくインポートされています。</p> <h2 id=3-vit>3. ViT論文の再現実装：概要<a class=headerlink href=#3-vit title="Permanent link">&para;</a></h2> <h3 id=31>3.1 入力と出力、層とブロック<a class=headerlink href=#31 title="Permanent link">&para;</a></h3> <p>ニューラルネットワークアーキテクチャは一般的に<strong>層（layer）</strong>で構成されます。</p> <p>層の集合は<strong>ブロック（block）</strong>と呼ばれます。</p> <p>多くのブロックを積み重ねることで、全体のアーキテクチャが構成されます。</p> <ul> <li><strong>層</strong> - 入力を受け取り、何らかの関数を適用し、出力を返す</li> <li><strong>ブロック</strong> - 層の集合、入力を受け取り、一連の関数を適用し、出力を返す</li> <li><strong>アーキテクチャ（モデル）</strong> - ブロックの集合、入力を受け取り、一連の関数を適用し、出力を返す</li> </ul> <p><img alt=層とブロックの概念 src=../09_pytorch_paper_replicating_files/09-vit-paper-intputs-outputs-layers-and-blocks.png></p> <p><em>現代の深層学習アーキテクチャは通常、層とブロックの集合です。層は入力（数値表現としてのデータ）を受け取り、何らかの関数を使って操作し、出力します。</em></p> <h3 id=32-vit>3.2 具体的なViTの構成要素<a class=headerlink href=#32-vit title="Permanent link">&para;</a></h3> <h4 id=321-figure-1>3.2.1 Figure 1の探索<a class=headerlink href=#321-figure-1 title="Permanent link">&para;</a></h4> <p>ViT論文のFigure 1から、アーキテクチャの主要なコンポーネントを理解しましょう：</p> <p><img alt="ViT Figure 1" src=../09_pytorch_paper_replicating_files/09-vit-paper-figure-1-inputs-and-outputs.png></p> <p><em>ViT論文のFigure 1は、アーキテクチャを構成する様々な入力、出力、層、ブロックを示しています。</em></p> <p>ViTアーキテクチャは以下の段階で構成されます：</p> <ul> <li><strong>Patch + Position Embedding（入力）</strong> - 入力画像を画像パッチのシーケンスに変換し、パッチの順序を指定する位置番号を追加</li> <li><strong>Linear projection of flattened patches（埋め込みパッチ）</strong> - 画像パッチを<strong>埋め込み表現</strong>に変換</li> <li><strong>Norm</strong> - Layer Normalization（正規化層）</li> <li><strong>Multi-Head Attention</strong> - マルチヘッド自己注意層（MSA）</li> <li><strong>MLP</strong> - 多層パーセプトロン</li> <li><strong>Transformer Encoder</strong> - 上記の層を組み合わせたブロック</li> <li><strong>MLP Head</strong> - 分類のための出力層</li> </ul> <h4 id=322-4>3.2.2 4つの数式の探索<a class=headerlink href=#322-4 title="Permanent link">&para;</a></h4> <p>ViT論文の3.1節にある4つの数式が、アーキテクチャの数学的基盤を提供します：</p> <p><img alt=ViT論文の4つの数式 src=../09_pytorch_paper_replicating_files/09-vit-paper-four-equations.png></p> <p><em>これらの4つの数式は、ViTアーキテクチャの4つの主要部分の数学を表しています。</em></p> <p><strong>数式1：パッチ埋め込み</strong></p> <div class=arithmatex>\[ \mathbf{z}_{0} =\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }} \]</div> <p>これは、クラストークン、パッチ埋め込み、位置埋め込みを扱います。</p> <p><strong>数式2：MSAブロック</strong></p> <div class=arithmatex>\[ \mathbf{z}_{\ell}^{\prime} =\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1} \]</div> <p>LayerNormでラップされたMulti-Head Attention層と、残差接続を示します。</p> <p><strong>数式3：MLPブロック</strong></p> <div class=arithmatex>\[ \mathbf{z}_{\ell} =\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime} \]</div> <p>LayerNormでラップされたMultilayer Perceptron層と、残差接続を示します。</p> <p><strong>数式4：出力</strong></p> <div class=arithmatex>\[ \mathbf{y} =\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) \]</div> <p>最終層での出力を定義します。</p> <h4 id=323-table-1>3.2.3 Table 1の探索<a class=headerlink href=#323-table-1 title="Permanent link">&para;</a></h4> <p>Table 1は、様々なViTモデルのハイパーパラメータ設定を示しています：</p> <table> <thead> <tr> <th>Model</th> <th>Layers</th> <th>Hidden size D</th> <th>MLP size</th> <th>Heads</th> <th>Params</th> </tr> </thead> <tbody> <tr> <td>ViT-Base</td> <td>12</td> <td>768</td> <td>3072</td> <td>12</td> <td>86M</td> </tr> <tr> <td>ViT-Large</td> <td>24</td> <td>1024</td> <td>4096</td> <td>16</td> <td>307M</td> </tr> <tr> <td>ViT-Huge</td> <td>32</td> <td>1280</td> <td>5120</td> <td>16</td> <td>632M</td> </tr> </tbody> </table> <p>私たちは最小のViT-Baseから始めて、うまくいけばより大きなバリアントにスケールアップできます。</p> <ul> <li><strong>Layers</strong> - Transformer Encoderブロックの数</li> <li><strong>Hidden size D</strong> - 埋め込み次元</li> <li><strong>MLP size</strong> - MLP層の隠れユニット数</li> <li><strong>Heads</strong> - Multi-Head Attention層のヘッド数</li> <li><strong>Params</strong> - モデルの総パラメータ数</li> </ul> <h2 id=4-1>4. 数式1の実装：パッチ分割とクラス・位置・パッチ埋め込みの作成<a class=headerlink href=#4-1 title="Permanent link">&para;</a></h2> <p>私の機械学習エンジニアの友人がよく「すべては埋め込み次第だ」と言っていました。</p> <p>つまり、データを良い学習可能な方法（<strong>埋め込みは学習可能な表現</strong>）で表現できれば、学習アルゴリズムは良い性能を発揮する可能性が高いということです。</p> <p>それを踏まえて、ViTアーキテクチャのクラス、位置、パッチ埋め込みを作成することから始めましょう。</p> <p>まず<strong>パッチ埋め込み</strong>から始めます。これは、入力画像をパッチのシーケンスに変換し、それらのパッチを埋め込むことを意味します。</p> <p><strong>埋め込み</strong>は何らかの形式の学習可能な表現であり、多くの場合ベクトルであることを思い出してください。</p> <p>学習可能という用語が重要です。これは、入力画像の数値表現（モデルが見るもの）が時間とともに改善されることを意味するからです。</p> <p>ViT論文の3.1節の冒頭段落に従って始めましょう：</p> <blockquote> <p>標準的なTransformerは、1Dトークン埋め込みのシーケンスを入力として受け取ります。2D画像を扱うために、画像 <span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^{H \times W \times C}\)</span> を平坦化された2Dパッチのシーケンス <span class=arithmatex>\(\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}\)</span> に変形します。ここで<span class=arithmatex>\((H, W)\)</span>は元画像の解像度、<span class=arithmatex>\(C\)</span>はチャンネル数、<span class=arithmatex>\((P, P)\)</span>は各画像パッチの解像度、<span class=arithmatex>\(N=H W / P^{2}\)</span>は結果として得られるパッチ数で、これはTransformerの有効な入力シーケンス長でもあります。</p> </blockquote> <p>ViT論文のTable 3から「Training resolution is <strong>224</strong>」という情報も念頭に置いておきましょう。</p> <p><img alt=ViT論文の数式1図解 src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-1-annotated.png></p> <p><em>ViTアーキテクチャのFigure 1からEquation 1への位置・パッチ埋め込み部分のマッピング。3.1節の冒頭段落は、パッチ埋め込み層の異なる入力・出力形状を説明しています。</em></p> <h3 id=41>4.1 パッチ埋め込みの入力・出力形状を手計算で求める<a class=headerlink href=#41 title="Permanent link">&para;</a></h3> <p>まず、これらの入力・出力形状の値を手計算で求めてみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=c1># サンプル値を作成</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=n>height</span> <span class=o>=</span> <span class=mi>224</span> <span class=c1># H (&quot;Training resolution is 224.&quot;)</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=n>width</span> <span class=o>=</span> <span class=mi>224</span> <span class=c1># W</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=n>color_channels</span> <span class=o>=</span> <span class=mi>3</span> <span class=c1># C</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=n>patch_size</span> <span class=o>=</span> <span class=mi>16</span> <span class=c1># P</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=c1># N（パッチ数）を計算</span>
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a><span class=n>number_of_patches</span> <span class=o>=</span> <span class=nb>int</span><span class=p>((</span><span class=n>height</span> <span class=o>*</span> <span class=n>width</span><span class=p>)</span> <span class=o>/</span> <span class=n>patch_size</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチサイズ(P=</span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2>)における画像高さ(H=</span><span class=si>{</span><span class=n>height</span><span class=si>}</span><span class=s2>)、幅(W=</span><span class=si>{</span><span class=n>width</span><span class=si>}</span><span class=s2>)でのパッチ数(N): </span><span class=si>{</span><span class=n>number_of_patches</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>パッチサイズ(P=16)における画像高さ(H=224)、幅(W=224)でのパッチ数(N): 196
</span></code></pre></div></p> <p>パッチ埋め込み層の入力・出力形状を確認しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=c1># 入力形状（単一画像のサイズ）</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=n>embedding_layer_input_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>color_channels</span><span class=p>)</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a><span class=c1># 出力形状</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a><span class=n>embedding_layer_output_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>number_of_patches</span><span class=p>,</span> <span class=n>patch_size</span><span class=o>**</span><span class=mi>2</span> <span class=o>*</span> <span class=n>color_channels</span><span class=p>)</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;入力形状（単一2D画像）: </span><span class=si>{</span><span class=n>embedding_layer_input_shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;出力形状（パッチに平坦化された単一2D画像）: </span><span class=si>{</span><span class=n>embedding_layer_output_shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a>入力形状（単一2D画像）: (224, 224, 3)
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>出力形状（パッチに平坦化された単一2D画像）: (196, 768)
</span></code></pre></div></p> <h3 id=42>4.2 単一画像をパッチに変換<a class=headerlink href=#42 title="Permanent link">&para;</a></h3> <p>画像をパッチに変換する様子を可視化してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=c1># 単一画像を表示</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span> <span class=c1># matplotlibに合わせて調整</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>class_names</span><span class=p>[</span><span class=n>label</span><span class=p>])</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a><span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></code></pre></div> <p><img alt=単一画像表示 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_45_1.png></p> <p>まず、画像の上部行のパッチ化されたピクセルを可視化してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=c1># matplotlib用に画像形状を変更 [color_channels, height, width] -&gt; [height, width, color_channels]</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=n>image_permuted</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a><span class=c1># 上部行のパッチ化されたピクセルをプロットするためのインデックス</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a><span class=n>patch_size</span> <span class=o>=</span> <span class=mi>16</span>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>))</span>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image_permuted</span><span class=p>[:</span><span class=n>patch_size</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:])</span>
</span></code></pre></div> <p><img alt=上部行パッチ src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_47_1.png></p> <p>次に、上部行を個別のパッチに分割しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># ハイパーパラメータの設定とimg_sizeとpatch_sizeの互換性確認</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=n>img_size</span> <span class=o>=</span> <span class=mi>224</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a><span class=n>patch_size</span> <span class=o>=</span> <span class=mi>16</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a><span class=n>num_patches</span> <span class=o>=</span> <span class=n>img_size</span><span class=o>/</span><span class=n>patch_size</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a><span class=k>assert</span> <span class=n>img_size</span> <span class=o>%</span> <span class=n>patch_size</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&quot;画像サイズはパッチサイズで割り切れる必要があります&quot;</span>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;行ごとのパッチ数: </span><span class=si>{</span><span class=n>num_patches</span><span class=si>}</span><span class=se>\n</span><span class=s2>パッチサイズ: </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2> pixels x </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2> pixels&quot;</span><span class=p>)</span>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a>
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=c1># 一連のサブプロットを作成</span>
</span><span id=__span-20-9><a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=n>fig</span><span class=p>,</span> <span class=n>axs</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>nrows</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-20-10><a id=__codelineno-20-10 name=__codelineno-20-10 href=#__codelineno-20-10></a>                        <span class=n>ncols</span><span class=o>=</span><span class=n>img_size</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>,</span> <span class=c1># 各パッチに1列</span>
</span><span id=__span-20-11><a id=__codelineno-20-11 name=__codelineno-20-11 href=#__codelineno-20-11></a>                        <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=n>num_patches</span><span class=p>,</span> <span class=n>num_patches</span><span class=p>),</span>
</span><span id=__span-20-12><a id=__codelineno-20-12 name=__codelineno-20-12 href=#__codelineno-20-12></a>                        <span class=n>sharex</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-20-13><a id=__codelineno-20-13 name=__codelineno-20-13 href=#__codelineno-20-13></a>                        <span class=n>sharey</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-20-14><a id=__codelineno-20-14 name=__codelineno-20-14 href=#__codelineno-20-14></a>
</span><span id=__span-20-15><a id=__codelineno-20-15 name=__codelineno-20-15 href=#__codelineno-20-15></a><span class=c1># 上部行のパッチ数を反復処理</span>
</span><span id=__span-20-16><a id=__codelineno-20-16 name=__codelineno-20-16 href=#__codelineno-20-16></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>patch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>img_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>)):</span>
</span><span id=__span-20-17><a id=__codelineno-20-17 name=__codelineno-20-17 href=#__codelineno-20-17></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image_permuted</span><span class=p>[:</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>patch</span><span class=p>:</span><span class=n>patch</span><span class=o>+</span><span class=n>patch_size</span><span class=p>,</span> <span class=p>:])</span> <span class=c1># 高さインデックスを一定に保ち、幅インデックスを変更</span>
</span><span id=__span-20-18><a id=__codelineno-20-18 name=__codelineno-20-18 href=#__codelineno-20-18></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span> <span class=c1># ラベルを設定</span>
</span><span id=__span-20-19><a id=__codelineno-20-19 name=__codelineno-20-19 href=#__codelineno-20-19></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>([])</span>
</span><span id=__span-20-20><a id=__codelineno-20-20 name=__codelineno-20-20 href=#__codelineno-20-20></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>([])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a>行ごとのパッチ数: 14.0
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>パッチサイズ: 16 pixels x 16 pixels
</span></code></pre></div></p> <p><img alt=上部行パッチ分割 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_49_1.png></p> <p>今度は画像全体をパッチ化してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=c1># ハイパーパラメータの設定</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a><span class=n>img_size</span> <span class=o>=</span> <span class=mi>224</span>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a><span class=n>patch_size</span> <span class=o>=</span> <span class=mi>16</span>
</span><span id=__span-22-4><a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a><span class=n>num_patches</span> <span class=o>=</span> <span class=n>img_size</span><span class=o>/</span><span class=n>patch_size</span>
</span><span id=__span-22-5><a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;行ごとのパッチ数: </span><span class=si>{</span><span class=n>num_patches</span><span class=si>}</span><span class=se>\</span>
</span><span id=__span-22-6><a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a><span class=s2>        </span><span class=se>\n</span><span class=s2>列ごとのパッチ数: </span><span class=si>{</span><span class=n>num_patches</span><span class=si>}</span><span class=se>\</span>
</span><span id=__span-22-7><a id=__codelineno-22-7 name=__codelineno-22-7 href=#__codelineno-22-7></a><span class=s2>        </span><span class=se>\n</span><span class=s2>総パッチ数: </span><span class=si>{</span><span class=n>num_patches</span><span class=o>*</span><span class=n>num_patches</span><span class=si>}</span><span class=se>\</span>
</span><span id=__span-22-8><a id=__codelineno-22-8 name=__codelineno-22-8 href=#__codelineno-22-8></a><span class=s2>        </span><span class=se>\n</span><span class=s2>パッチサイズ: </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2> pixels x </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2> pixels&quot;</span><span class=p>)</span>
</span><span id=__span-22-9><a id=__codelineno-22-9 name=__codelineno-22-9 href=#__codelineno-22-9></a>
</span><span id=__span-22-10><a id=__codelineno-22-10 name=__codelineno-22-10 href=#__codelineno-22-10></a><span class=c1># サブプロットシリーズを作成</span>
</span><span id=__span-22-11><a id=__codelineno-22-11 name=__codelineno-22-11 href=#__codelineno-22-11></a><span class=n>fig</span><span class=p>,</span> <span class=n>axs</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>nrows</span><span class=o>=</span><span class=n>img_size</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-22-12><a id=__codelineno-22-12 name=__codelineno-22-12 href=#__codelineno-22-12></a>                        <span class=n>ncols</span><span class=o>=</span><span class=n>img_size</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-22-13><a id=__codelineno-22-13 name=__codelineno-22-13 href=#__codelineno-22-13></a>                        <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=n>num_patches</span><span class=p>,</span> <span class=n>num_patches</span><span class=p>),</span>
</span><span id=__span-22-14><a id=__codelineno-22-14 name=__codelineno-22-14 href=#__codelineno-22-14></a>                        <span class=n>sharex</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-22-15><a id=__codelineno-22-15 name=__codelineno-22-15 href=#__codelineno-22-15></a>                        <span class=n>sharey</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-22-16><a id=__codelineno-22-16 name=__codelineno-22-16 href=#__codelineno-22-16></a>
</span><span id=__span-22-17><a id=__codelineno-22-17 name=__codelineno-22-17 href=#__codelineno-22-17></a><span class=c1># 画像の高さと幅をループ</span>
</span><span id=__span-22-18><a id=__codelineno-22-18 name=__codelineno-22-18 href=#__codelineno-22-18></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>patch_height</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>img_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>)):</span> <span class=c1># 高さを反復</span>
</span><span id=__span-22-19><a id=__codelineno-22-19 name=__codelineno-22-19 href=#__codelineno-22-19></a>    <span class=k>for</span> <span class=n>j</span><span class=p>,</span> <span class=n>patch_width</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>img_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>)):</span> <span class=c1># 幅を反復</span>
</span><span id=__span-22-20><a id=__codelineno-22-20 name=__codelineno-22-20 href=#__codelineno-22-20></a>        <span class=c1># 置換された画像パッチをプロット</span>
</span><span id=__span-22-21><a id=__codelineno-22-21 name=__codelineno-22-21 href=#__codelineno-22-21></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image_permuted</span><span class=p>[</span><span class=n>patch_height</span><span class=p>:</span><span class=n>patch_height</span><span class=o>+</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-22-22><a id=__codelineno-22-22 name=__codelineno-22-22 href=#__codelineno-22-22></a>                                        <span class=n>patch_width</span><span class=p>:</span><span class=n>patch_width</span><span class=o>+</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-22-23><a id=__codelineno-22-23 name=__codelineno-22-23 href=#__codelineno-22-23></a>                                        <span class=p>:])</span>
</span><span id=__span-22-24><a id=__codelineno-22-24 name=__codelineno-22-24 href=#__codelineno-22-24></a>        <span class=c1># ラベル情報を設定</span>
</span><span id=__span-22-25><a id=__codelineno-22-25 name=__codelineno-22-25 href=#__codelineno-22-25></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-22-26><a id=__codelineno-22-26 name=__codelineno-22-26 href=#__codelineno-22-26></a>                             <span class=n>rotation</span><span class=o>=</span><span class=s2>&quot;horizontal&quot;</span><span class=p>,</span>
</span><span id=__span-22-27><a id=__codelineno-22-27 name=__codelineno-22-27 href=#__codelineno-22-27></a>                             <span class=n>horizontalalignment</span><span class=o>=</span><span class=s2>&quot;right&quot;</span><span class=p>,</span>
</span><span id=__span-22-28><a id=__codelineno-22-28 name=__codelineno-22-28 href=#__codelineno-22-28></a>                             <span class=n>verticalalignment</span><span class=o>=</span><span class=s2>&quot;center&quot;</span><span class=p>)</span>
</span><span id=__span-22-29><a id=__codelineno-22-29 name=__codelineno-22-29 href=#__codelineno-22-29></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=n>j</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-22-30><a id=__codelineno-22-30 name=__codelineno-22-30 href=#__codelineno-22-30></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>([])</span>
</span><span id=__span-22-31><a id=__codelineno-22-31 name=__codelineno-22-31 href=#__codelineno-22-31></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>([])</span>
</span><span id=__span-22-32><a id=__codelineno-22-32 name=__codelineno-22-32 href=#__codelineno-22-32></a>        <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>label_outer</span><span class=p>()</span>
</span><span id=__span-22-33><a id=__codelineno-22-33 name=__codelineno-22-33 href=#__codelineno-22-33></a>
</span><span id=__span-22-34><a id=__codelineno-22-34 name=__codelineno-22-34 href=#__codelineno-22-34></a><span class=c1># スーパータイトルを設定</span>
</span><span id=__span-22-35><a id=__codelineno-22-35 name=__codelineno-22-35 href=#__codelineno-22-35></a><span class=n>fig</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>class_names</span><span class=p>[</span><span class=n>label</span><span class=p>]</span><span class=si>}</span><span class=s2> -&gt; パッチ化済み&quot;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
</span><span id=__span-22-36><a id=__codelineno-22-36 name=__codelineno-22-36 href=#__codelineno-22-36></a><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>行ごとのパッチ数: 14.0        
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>列ごとのパッチ数: 14.0        
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>総パッチ数: 196.0        
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>パッチサイズ: 16 pixels x 16 pixels
</span></code></pre></div></p> <p><img alt=全画像パッチ化 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_51_1.png></p> <p>画像のパッチ化完了！素晴らしい見た目ですね。</p> <h3 id=43-torchnnconv2d>4.3 <code>torch.nn.Conv2d()</code>による画像パッチの作成<a class=headerlink href=#43-torchnnconv2d title="Permanent link">&para;</a></h3> <p>PyTorchでパッチ埋め込み層を複製する作業に移りましょう。</p> <p>この操作は、畳み込み操作と非常に似ています。実際、ViT論文の著者は3.1節で、パッチ埋め込みは畳み込みニューラルネットワーク（CNN）で実現できると述べています：</p> <blockquote> <p><strong>ハイブリッドアーキテクチャ。</strong> 生の画像パッチの代替として、入力シーケンスは<strong>CNNの特徴マップ</strong>から形成できます。</p> </blockquote> <p><img alt=パッチ埋め込みアニメーション src=../09_pytorch_paper_replicating_files/09-vit-paper-patch-embedding-animation.gif></p> <p><em><code>torch.nn.Conv2d()</code>層の<code>kernel_size</code>と<code>stride</code>パラメータを<code>patch_size</code>と等しく設定することで、画像をパッチに分割し、各パッチの学習可能な埋め込みを作成する層を効果的に取得できます。</em></p> <p><code>torch.nn.Conv2d()</code>層でパッチ作成を複製するには、<code>kernel_size</code>と<code>stride</code>を<code>patch_size</code>と等しく設定します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch</span><span class=w> </span><span class=kn>import</span> <span class=n>nn</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a>
</span><span id=__span-24-3><a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a><span class=c1># パッチサイズを設定</span>
</span><span id=__span-24-4><a id=__codelineno-24-4 name=__codelineno-24-4 href=#__codelineno-24-4></a><span class=n>patch_size</span><span class=o>=</span><span class=mi>16</span>
</span><span id=__span-24-5><a id=__codelineno-24-5 name=__codelineno-24-5 href=#__codelineno-24-5></a>
</span><span id=__span-24-6><a id=__codelineno-24-6 name=__codelineno-24-6 href=#__codelineno-24-6></a><span class=c1># ViT論文のハイパーパラメータでConv2d層を作成</span>
</span><span id=__span-24-7><a id=__codelineno-24-7 name=__codelineno-24-7 href=#__codelineno-24-7></a><span class=n>conv2d</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=c1># カラーチャンネル数</span>
</span><span id=__span-24-8><a id=__codelineno-24-8 name=__codelineno-24-8 href=#__codelineno-24-8></a>                   <span class=n>out_channels</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1より: Hidden size D、これが埋め込みサイズ</span>
</span><span id=__span-24-9><a id=__codelineno-24-9 name=__codelineno-24-9 href=#__codelineno-24-9></a>                   <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-24-10><a id=__codelineno-24-10 name=__codelineno-24-10 href=#__codelineno-24-10></a>                   <span class=n>stride</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-24-11><a id=__codelineno-24-11 name=__codelineno-24-11 href=#__codelineno-24-11></a>                   <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></code></pre></div> <p>画像を畳み込み層に通してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=c1># 画像を畳み込み層に通す</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a><span class=n>image_out_of_conv</span> <span class=o>=</span> <span class=n>conv2d</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span> <span class=c1># 単一バッチ次元を追加</span>
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a><span class=nb>print</span><span class=p>(</span><span class=n>image_out_of_conv</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a>torch.Size([1, 768, 14, 14])
</span></code></pre></div></p> <p>ランダムな5つの畳み込み特徴マップを可視化してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=c1># ランダムな5つの畳み込み特徴マップをプロット</span>
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a><span class=kn>import</span><span class=w> </span><span class=nn>random</span>
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a><span class=n>random_indexes</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>758</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;インデックスからのランダムな畳み込み特徴マップを表示: </span><span class=si>{</span><span class=n>random_indexes</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a>
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a><span class=c1># プロット作成</span>
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a><span class=n>fig</span><span class=p>,</span> <span class=n>axs</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>nrows</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>ncols</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>12</span><span class=p>))</span>
</span><span id=__span-27-8><a id=__codelineno-27-8 name=__codelineno-27-8 href=#__codelineno-27-8></a>
</span><span id=__span-27-9><a id=__codelineno-27-9 name=__codelineno-27-9 href=#__codelineno-27-9></a><span class=c1># ランダムな画像特徴マップをプロット</span>
</span><span id=__span-27-10><a id=__codelineno-27-10 name=__codelineno-27-10 href=#__codelineno-27-10></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>random_indexes</span><span class=p>):</span>
</span><span id=__span-27-11><a id=__codelineno-27-11 name=__codelineno-27-11 href=#__codelineno-27-11></a>    <span class=n>image_conv_feature_map</span> <span class=o>=</span> <span class=n>image_out_of_conv</span><span class=p>[:,</span> <span class=n>idx</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span>
</span><span id=__span-27-12><a id=__codelineno-27-12 name=__codelineno-27-12 href=#__codelineno-27-12></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image_conv_feature_map</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span>
</span><span id=__span-27-13><a id=__codelineno-27-13 name=__codelineno-27-13 href=#__codelineno-27-13></a>    <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>xticklabels</span><span class=o>=</span><span class=p>[],</span> <span class=n>yticklabels</span><span class=o>=</span><span class=p>[],</span> <span class=n>xticks</span><span class=o>=</span><span class=p>[],</span> <span class=n>yticks</span><span class=o>=</span><span class=p>[]);</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a>インデックスからのランダムな畳み込み特徴マップを表示: [678, 116, 559, 48, 577]
</span></code></pre></div></p> <p><img alt=ランダム特徴マップ src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_59_1.png></p> <h3 id=44-torchnnflatten>4.4 <code>torch.nn.Flatten()</code>によるパッチ埋め込みの平坦化<a class=headerlink href=#44-torchnnflatten title="Permanent link">&para;</a></h3> <p>画像をパッチ埋め込みに変換しましたが、まだ2D形式です。ViTモデルの理想的な出力形状にするにはどうすればよいでしょうか？</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=c1># 現在のテンソル形状</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;現在のテンソル形状: </span><span class=si>{</span><span class=n>image_out_of_conv</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a>現在のテンソル形状: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]
</span></code></pre></div></p> <p><code>torch.nn.Flatten()</code>を使用して特徴マップの空間次元を平坦化しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=c1># 平坦化層を作成</span>
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a><span class=n>flatten</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>start_dim</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=c1># feature_map_height（次元2）を平坦化</span>
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>                     <span class=n>end_dim</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span> <span class=c1># feature_map_width（次元3）を平坦化</span>
</span></code></pre></div> <p>すべてをまとめてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=c1># 1. 単一画像を表示</span>
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span> <span class=c1># matplotlib用に調整</span>
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>class_names</span><span class=p>[</span><span class=n>label</span><span class=p>])</span>
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a><span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-32-5><a id=__codelineno-32-5 name=__codelineno-32-5 href=#__codelineno-32-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;元画像の形状: </span><span class=si>{</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-32-6><a id=__codelineno-32-6 name=__codelineno-32-6 href=#__codelineno-32-6></a>
</span><span id=__span-32-7><a id=__codelineno-32-7 name=__codelineno-32-7 href=#__codelineno-32-7></a><span class=c1># 2. 画像を特徴マップに変換</span>
</span><span id=__span-32-8><a id=__codelineno-32-8 name=__codelineno-32-8 href=#__codelineno-32-8></a><span class=n>image_out_of_conv</span> <span class=o>=</span> <span class=n>conv2d</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span> <span class=c1># 形状エラーを避けるためバッチ次元を追加</span>
</span><span id=__span-32-9><a id=__codelineno-32-9 name=__codelineno-32-9 href=#__codelineno-32-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;画像特徴マップの形状: </span><span class=si>{</span><span class=n>image_out_of_conv</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-32-10><a id=__codelineno-32-10 name=__codelineno-32-10 href=#__codelineno-32-10></a>
</span><span id=__span-32-11><a id=__codelineno-32-11 name=__codelineno-32-11 href=#__codelineno-32-11></a><span class=c1># 3. 特徴マップを平坦化</span>
</span><span id=__span-32-12><a id=__codelineno-32-12 name=__codelineno-32-12 href=#__codelineno-32-12></a><span class=n>image_out_of_conv_flattened</span> <span class=o>=</span> <span class=n>flatten</span><span class=p>(</span><span class=n>image_out_of_conv</span><span class=p>)</span>
</span><span id=__span-32-13><a id=__codelineno-32-13 name=__codelineno-32-13 href=#__codelineno-32-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;平坦化された画像特徴マップの形状: </span><span class=si>{</span><span class=n>image_out_of_conv_flattened</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a>元画像の形状: torch.Size([3, 224, 224])
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a>画像特徴マップの形状: torch.Size([1, 768, 14, 14])
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a>平坦化された画像特徴マップの形状: torch.Size([1, 768, 196])
</span></code></pre></div></p> <p><img alt=元画像 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_68_1.png></p> <p>次元を再配置して最終的な形状にしましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=c1># 平坦化された画像パッチ埋め込みを正しい形状に取得</span>
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2 href=#__codelineno-34-2></a><span class=n>image_out_of_conv_flattened_reshaped</span> <span class=o>=</span> <span class=n>image_out_of_conv_flattened</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=c1># [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span>
</span><span id=__span-34-3><a id=__codelineno-34-3 name=__codelineno-34-3 href=#__codelineno-34-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチ埋め込みシーケンス形状: </span><span class=si>{</span><span class=n>image_out_of_conv_flattened_reshaped</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> -&gt; [batch_size, num_patches, embedding_size]&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a>パッチ埋め込みシーケンス形状: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]
</span></code></pre></div></p> <h3 id=45-vitpytorch>4.5 ViTパッチ埋め込み層をPyTorchモジュールに変換<a class=headerlink href=#45-vitpytorch title="Permanent link">&para;</a></h3> <p>パッチ埋め込みのすべての作業を単一のPyTorchレイヤーにまとめましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=c1># 1. nn.Moduleをサブクラス化するクラスを作成</span>
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a><span class=k>class</span><span class=w> </span><span class=nc>PatchEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;2D入力画像を1D学習可能埋め込みベクトルシーケンスに変換します。</span>
</span><span id=__span-36-4><a id=__codelineno-36-4 name=__codelineno-36-4 href=#__codelineno-36-4></a>
</span><span id=__span-36-5><a id=__codelineno-36-5 name=__codelineno-36-5 href=#__codelineno-36-5></a><span class=sd>    Args:</span>
</span><span id=__span-36-6><a id=__codelineno-36-6 name=__codelineno-36-6 href=#__codelineno-36-6></a><span class=sd>        in_channels (int): 入力画像のカラーチャンネル数。デフォルトは3。</span>
</span><span id=__span-36-7><a id=__codelineno-36-7 name=__codelineno-36-7 href=#__codelineno-36-7></a><span class=sd>        patch_size (int): 入力画像を変換するパッチのサイズ。デフォルトは16。</span>
</span><span id=__span-36-8><a id=__codelineno-36-8 name=__codelineno-36-8 href=#__codelineno-36-8></a><span class=sd>        embedding_dim (int): 画像を変換する埋め込みのサイズ。デフォルトは768。</span>
</span><span id=__span-36-9><a id=__codelineno-36-9 name=__codelineno-36-9 href=#__codelineno-36-9></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-36-10><a id=__codelineno-36-10 name=__codelineno-36-10 href=#__codelineno-36-10></a>    <span class=c1># 2. 適切な変数でクラスを初期化</span>
</span><span id=__span-36-11><a id=__codelineno-36-11 name=__codelineno-36-11 href=#__codelineno-36-11></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span><span id=__span-36-12><a id=__codelineno-36-12 name=__codelineno-36-12 href=#__codelineno-36-12></a>                 <span class=n>in_channels</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-36-13><a id=__codelineno-36-13 name=__codelineno-36-13 href=#__codelineno-36-13></a>                 <span class=n>patch_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-36-14><a id=__codelineno-36-14 name=__codelineno-36-14 href=#__codelineno-36-14></a>                 <span class=n>embedding_dim</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>768</span><span class=p>):</span>
</span><span id=__span-36-15><a id=__codelineno-36-15 name=__codelineno-36-15 href=#__codelineno-36-15></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-36-16><a id=__codelineno-36-16 name=__codelineno-36-16 href=#__codelineno-36-16></a>
</span><span id=__span-36-17><a id=__codelineno-36-17 name=__codelineno-36-17 href=#__codelineno-36-17></a>        <span class=c1># 3. 画像をパッチに変換する層を作成</span>
</span><span id=__span-36-18><a id=__codelineno-36-18 name=__codelineno-36-18 href=#__codelineno-36-18></a>        <span class=bp>self</span><span class=o>.</span><span class=n>patcher</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=n>in_channels</span><span class=p>,</span>
</span><span id=__span-36-19><a id=__codelineno-36-19 name=__codelineno-36-19 href=#__codelineno-36-19></a>                                 <span class=n>out_channels</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-36-20><a id=__codelineno-36-20 name=__codelineno-36-20 href=#__codelineno-36-20></a>                                 <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-36-21><a id=__codelineno-36-21 name=__codelineno-36-21 href=#__codelineno-36-21></a>                                 <span class=n>stride</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-36-22><a id=__codelineno-36-22 name=__codelineno-36-22 href=#__codelineno-36-22></a>                                 <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-36-23><a id=__codelineno-36-23 name=__codelineno-36-23 href=#__codelineno-36-23></a>
</span><span id=__span-36-24><a id=__codelineno-36-24 name=__codelineno-36-24 href=#__codelineno-36-24></a>        <span class=c1># 4. パッチ特徴マップを単一次元に平坦化する層を作成</span>
</span><span id=__span-36-25><a id=__codelineno-36-25 name=__codelineno-36-25 href=#__codelineno-36-25></a>        <span class=bp>self</span><span class=o>.</span><span class=n>flatten</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>start_dim</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=c1># 特徴マップ次元のみを単一ベクトルに平坦化</span>
</span><span id=__span-36-26><a id=__codelineno-36-26 name=__codelineno-36-26 href=#__codelineno-36-26></a>                                  <span class=n>end_dim</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span><span id=__span-36-27><a id=__codelineno-36-27 name=__codelineno-36-27 href=#__codelineno-36-27></a>
</span><span id=__span-36-28><a id=__codelineno-36-28 name=__codelineno-36-28 href=#__codelineno-36-28></a>    <span class=c1># 5. forwardメソッドを定義</span>
</span><span id=__span-36-29><a id=__codelineno-36-29 name=__codelineno-36-29 href=#__codelineno-36-29></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-36-30><a id=__codelineno-36-30 name=__codelineno-36-30 href=#__codelineno-36-30></a>        <span class=c1># 入力が正しい形状かチェックするアサーションを作成</span>
</span><span id=__span-36-31><a id=__codelineno-36-31 name=__codelineno-36-31 href=#__codelineno-36-31></a>        <span class=n>image_resolution</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-36-32><a id=__codelineno-36-32 name=__codelineno-36-32 href=#__codelineno-36-32></a>        <span class=k>assert</span> <span class=n>image_resolution</span> <span class=o>%</span> <span class=n>patch_size</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&quot;入力画像サイズはパッチサイズで割り切れる必要があります。画像形状: </span><span class=si>{</span><span class=n>image_resolution</span><span class=si>}</span><span class=s2>、パッチサイズ: </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-36-33><a id=__codelineno-36-33 name=__codelineno-36-33 href=#__codelineno-36-33></a>
</span><span id=__span-36-34><a id=__codelineno-36-34 name=__codelineno-36-34 href=#__codelineno-36-34></a>        <span class=c1># フォワードパスを実行</span>
</span><span id=__span-36-35><a id=__codelineno-36-35 name=__codelineno-36-35 href=#__codelineno-36-35></a>        <span class=n>x_patched</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patcher</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-36-36><a id=__codelineno-36-36 name=__codelineno-36-36 href=#__codelineno-36-36></a>        <span class=n>x_flattened</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>x_patched</span><span class=p>)</span>
</span><span id=__span-36-37><a id=__codelineno-36-37 name=__codelineno-36-37 href=#__codelineno-36-37></a>        <span class=c1># 6. 出力形状が正しい順序になるようにする</span>
</span><span id=__span-36-38><a id=__codelineno-36-38 name=__codelineno-36-38 href=#__codelineno-36-38></a>        <span class=k>return</span> <span class=n>x_flattened</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=c1># 埋め込みが最後の次元になるように調整</span>
</span></code></pre></div> <p><code>PatchEmbedding</code>層を作成できました！単一画像で試してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a>
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a><span class=c1># パッチ埋め込み層のインスタンスを作成</span>
</span><span id=__span-37-4><a id=__codelineno-37-4 name=__codelineno-37-4 href=#__codelineno-37-4></a><span class=n>patchify</span> <span class=o>=</span> <span class=n>PatchEmbedding</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-37-5><a id=__codelineno-37-5 name=__codelineno-37-5 href=#__codelineno-37-5></a>                          <span class=n>patch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-37-6><a id=__codelineno-37-6 name=__codelineno-37-6 href=#__codelineno-37-6></a>                          <span class=n>embedding_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>)</span>
</span><span id=__span-37-7><a id=__codelineno-37-7 name=__codelineno-37-7 href=#__codelineno-37-7></a>
</span><span id=__span-37-8><a id=__codelineno-37-8 name=__codelineno-37-8 href=#__codelineno-37-8></a><span class=c1># 単一画像を通す</span>
</span><span id=__span-37-9><a id=__codelineno-37-9 name=__codelineno-37-9 href=#__codelineno-37-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;入力画像形状: </span><span class=si>{</span><span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-37-10><a id=__codelineno-37-10 name=__codelineno-37-10 href=#__codelineno-37-10></a><span class=n>patch_embedded_image</span> <span class=o>=</span> <span class=n>patchify</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span> <span class=c1># エラーを避けるため0番目のインデックスに追加のバッチ次元を追加</span>
</span><span id=__span-37-11><a id=__codelineno-37-11 name=__codelineno-37-11 href=#__codelineno-37-11></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;出力パッチ埋め込み形状: </span><span class=si>{</span><span class=n>patch_embedded_image</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a>入力画像形状: torch.Size([1, 3, 224, 224])
</span><span id=__span-38-2><a id=__codelineno-38-2 name=__codelineno-38-2 href=#__codelineno-38-2></a>出力パッチ埋め込み形状: torch.Size([1, 196, 768])
</span></code></pre></div></p> <h3 id=46>4.6 クラストークン埋め込みの作成<a class=headerlink href=#46 title="Permanent link">&para;</a></h3> <p>パッチ埋め込みを作成したので、次はクラストークン埋め込みに取り組みましょう。</p> <p>ViT論文の3.1節の第2段落を読むと：</p> <blockquote> <p>BERTの<code>[ class ]</code>トークンと同様に、埋め込まれたパッチのシーケンスに学習可能な埋め込みを前に付け加えます。</p> </blockquote> <p>パッチ埋め込みシーケンスを確認しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a><span class=c1># パッチ埋め込みとパッチ埋め込み形状を表示</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a><span class=nb>print</span><span class=p>(</span><span class=n>patch_embedded_image</span><span class=p>)</span>
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチ埋め込み形状: </span><span class=si>{</span><span class=n>patch_embedded_image</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a>tensor([[[-0.9698,  0.2378, -0.2770,  ...,  0.7685, -0.4388,  0.3720],
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a>         [-0.9932,  0.2355, -0.2763,  ...,  0.7275, -0.4185,  0.3424],
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a>         ...
</span><span id=__span-40-4><a id=__codelineno-40-4 name=__codelineno-40-4 href=#__codelineno-40-4></a>         [-0.6786,  0.2779, -0.0972,  ...,  0.5548, -0.1956, -0.0277]]],
</span><span id=__span-40-5><a id=__codelineno-40-5 name=__codelineno-40-5 href=#__codelineno-40-5></a>       grad_fn=&lt;PermuteBackward0&gt;)
</span><span id=__span-40-6><a id=__codelineno-40-6 name=__codelineno-40-6 href=#__codelineno-40-6></a>パッチ埋め込み形状: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</span></code></pre></div></p> <p>クラストークンの学習可能な埋め込みを作成しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a><span class=c1># バッチサイズと埋め込み次元を取得</span>
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a><span class=n>batch_size</span> <span class=o>=</span> <span class=n>patch_embedded_image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a><span class=n>embedding_dimension</span> <span class=o>=</span> <span class=n>patch_embedded_image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a>
</span><span id=__span-41-5><a id=__codelineno-41-5 name=__codelineno-41-5 href=#__codelineno-41-5></a><span class=c1># 埋め込み次元と同じサイズを共有する学習可能パラメータとしてクラストークン埋め込みを作成</span>
</span><span id=__span-41-6><a id=__codelineno-41-6 name=__codelineno-41-6 href=#__codelineno-41-6></a><span class=n>class_token</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>embedding_dimension</span><span class=p>),</span> <span class=c1># [batch_size, number_of_tokens, embedding_dimension]</span>
</span><span id=__span-41-7><a id=__codelineno-41-7 name=__codelineno-41-7 href=#__codelineno-41-7></a>                           <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 埋め込みが学習可能であることを確認</span>
</span><span id=__span-41-8><a id=__codelineno-41-8 name=__codelineno-41-8 href=#__codelineno-41-8></a>
</span><span id=__span-41-9><a id=__codelineno-41-9 name=__codelineno-41-9 href=#__codelineno-41-9></a><span class=c1># class_tokenの最初の10例を表示</span>
</span><span id=__span-41-10><a id=__codelineno-41-10 name=__codelineno-41-10 href=#__codelineno-41-10></a><span class=nb>print</span><span class=p>(</span><span class=n>class_token</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=mi>10</span><span class=p>])</span>
</span><span id=__span-41-11><a id=__codelineno-41-11 name=__codelineno-41-11 href=#__codelineno-41-11></a>
</span><span id=__span-41-12><a id=__codelineno-41-12 name=__codelineno-41-12 href=#__codelineno-41-12></a><span class=c1># class_token形状を出力</span>
</span><span id=__span-41-13><a id=__codelineno-41-13 name=__codelineno-41-13 href=#__codelineno-41-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;クラストークン形状: </span><span class=si>{</span><span class=n>class_token</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> -&gt; [batch_size, number_of_tokens, embedding_dimension]&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)
</span><span id=__span-42-2><a id=__codelineno-42-2 name=__codelineno-42-2 href=#__codelineno-42-2></a>クラストークン形状: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]
</span></code></pre></div></p> <p>クラストークン埋め込みをパッチ埋め込みシーケンスの前に追加しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a><span class=c1># クラストークン埋め込みをパッチ埋め込みの前面に追加</span>
</span><span id=__span-43-2><a id=__codelineno-43-2 name=__codelineno-43-2 href=#__codelineno-43-2></a><span class=n>patch_embedded_image_with_class_embedding</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>class_token</span><span class=p>,</span> <span class=n>patch_embedded_image</span><span class=p>),</span>
</span><span id=__span-43-3><a id=__codelineno-43-3 name=__codelineno-43-3 href=#__codelineno-43-3></a>                                                      <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=c1># 最初の次元で連結</span>
</span><span id=__span-43-4><a id=__codelineno-43-4 name=__codelineno-43-4 href=#__codelineno-43-4></a>
</span><span id=__span-43-5><a id=__codelineno-43-5 name=__codelineno-43-5 href=#__codelineno-43-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;クラストークンが前に付けられたパッチ埋め込みのシーケンス形状: </span><span class=si>{</span><span class=n>patch_embedded_image_with_class_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a>クラストークンが前に付けられたパッチ埋め込みのシーケンス形状: torch.Size([1, 197, 768])
</span></code></pre></div></p> <h3 id=47>4.7 位置埋め込みの作成<a class=headerlink href=#47 title="Permanent link">&para;</a></h3> <p>クラストークン埋め込みとパッチ埋め込みができたので、位置埋め込みを作成しましょう。</p> <p>ViT論文の3.1節より：</p> <blockquote> <p>位置埋め込みは、位置情報を保持するためにパッチ埋め込みに追加されます。標準的な学習可能な1D位置埋め込みを使用します。</p> </blockquote> <p>学習可能な1D位置埋め込みを作成しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a><span class=c1># N（パッチ数）を計算</span>
</span><span id=__span-45-2><a id=__codelineno-45-2 name=__codelineno-45-2 href=#__codelineno-45-2></a><span class=n>number_of_patches</span> <span class=o>=</span> <span class=nb>int</span><span class=p>((</span><span class=n>height</span> <span class=o>*</span> <span class=n>width</span><span class=p>)</span> <span class=o>/</span> <span class=n>patch_size</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-45-3><a id=__codelineno-45-3 name=__codelineno-45-3 href=#__codelineno-45-3></a>
</span><span id=__span-45-4><a id=__codelineno-45-4 name=__codelineno-45-4 href=#__codelineno-45-4></a><span class=c1># 埋め込み次元を取得</span>
</span><span id=__span-45-5><a id=__codelineno-45-5 name=__codelineno-45-5 href=#__codelineno-45-5></a><span class=n>embedding_dimension</span> <span class=o>=</span> <span class=n>patch_embedded_image_with_class_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span><span id=__span-45-6><a id=__codelineno-45-6 name=__codelineno-45-6 href=#__codelineno-45-6></a>
</span><span id=__span-45-7><a id=__codelineno-45-7 name=__codelineno-45-7 href=#__codelineno-45-7></a><span class=c1># 学習可能な1D位置埋め込みを作成</span>
</span><span id=__span-45-8><a id=__codelineno-45-8 name=__codelineno-45-8 href=#__codelineno-45-8></a><span class=n>position_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-45-9><a id=__codelineno-45-9 name=__codelineno-45-9 href=#__codelineno-45-9></a>                                             <span class=n>number_of_patches</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-45-10><a id=__codelineno-45-10 name=__codelineno-45-10 href=#__codelineno-45-10></a>                                             <span class=n>embedding_dimension</span><span class=p>),</span>
</span><span id=__span-45-11><a id=__codelineno-45-11 name=__codelineno-45-11 href=#__codelineno-45-11></a>                                  <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 学習可能であることを確認</span>
</span><span id=__span-45-12><a id=__codelineno-45-12 name=__codelineno-45-12 href=#__codelineno-45-12></a>
</span><span id=__span-45-13><a id=__codelineno-45-13 name=__codelineno-45-13 href=#__codelineno-45-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;位置埋め込み形状: </span><span class=si>{</span><span class=n>position_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a>位置埋め込み形状: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</span></code></pre></div></p> <p>位置埋め込みをクラストークンが前に付けられたパッチ埋め込みシーケンスに追加しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a><span class=c1># 位置埋め込みをパッチおよびクラストークン埋め込みに追加</span>
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a><span class=n>patch_and_position_embedding</span> <span class=o>=</span> <span class=n>patch_embedded_image_with_class_embedding</span> <span class=o>+</span> <span class=n>position_embedding</span>
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチ埋め込み、クラストークン前付け、位置埋め込み追加の形状: </span><span class=si>{</span><span class=n>patch_and_position_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a>パッチ埋め込み、クラストークン前付け、位置埋め込み追加の形状: torch.Size([1, 197, 768])
</span></code></pre></div></p> <h3 id=48>4.8 すべてをまとめる：画像から埋め込みへ<a class=headerlink href=#48 title="Permanent link">&para;</a></h3> <p>ViT論文の数式1を複製するため、すべてを単一のコードセルにまとめましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a>
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a><span class=c1># 1. パッチサイズを設定</span>
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a><span class=n>patch_size</span> <span class=o>=</span> <span class=mi>16</span>
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a>
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6 href=#__codelineno-49-6></a><span class=c1># 2. 元画像テンソル形状を出力し、画像次元を取得</span>
</span><span id=__span-49-7><a id=__codelineno-49-7 name=__codelineno-49-7 href=#__codelineno-49-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;画像テンソル形状: </span><span class=si>{</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-8><a id=__codelineno-49-8 name=__codelineno-49-8 href=#__codelineno-49-8></a><span class=n>height</span><span class=p>,</span> <span class=n>width</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span><span id=__span-49-9><a id=__codelineno-49-9 name=__codelineno-49-9 href=#__codelineno-49-9></a>
</span><span id=__span-49-10><a id=__codelineno-49-10 name=__codelineno-49-10 href=#__codelineno-49-10></a><span class=c1># 3. 画像テンソルを取得してバッチ次元を追加</span>
</span><span id=__span-49-11><a id=__codelineno-49-11 name=__codelineno-49-11 href=#__codelineno-49-11></a><span class=n>x</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-49-12><a id=__codelineno-49-12 name=__codelineno-49-12 href=#__codelineno-49-12></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;バッチ次元付き入力画像形状: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-13><a id=__codelineno-49-13 name=__codelineno-49-13 href=#__codelineno-49-13></a>
</span><span id=__span-49-14><a id=__codelineno-49-14 name=__codelineno-49-14 href=#__codelineno-49-14></a><span class=c1># 4. パッチ埋め込み層を作成</span>
</span><span id=__span-49-15><a id=__codelineno-49-15 name=__codelineno-49-15 href=#__codelineno-49-15></a><span class=n>patch_embedding_layer</span> <span class=o>=</span> <span class=n>PatchEmbedding</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-49-16><a id=__codelineno-49-16 name=__codelineno-49-16 href=#__codelineno-49-16></a>                                       <span class=n>patch_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-49-17><a id=__codelineno-49-17 name=__codelineno-49-17 href=#__codelineno-49-17></a>                                       <span class=n>embedding_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>)</span>
</span><span id=__span-49-18><a id=__codelineno-49-18 name=__codelineno-49-18 href=#__codelineno-49-18></a>
</span><span id=__span-49-19><a id=__codelineno-49-19 name=__codelineno-49-19 href=#__codelineno-49-19></a><span class=c1># 5. パッチ埋め込み層を通して画像を渡す</span>
</span><span id=__span-49-20><a id=__codelineno-49-20 name=__codelineno-49-20 href=#__codelineno-49-20></a><span class=n>patch_embedding</span> <span class=o>=</span> <span class=n>patch_embedding_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-49-21><a id=__codelineno-49-21 name=__codelineno-49-21 href=#__codelineno-49-21></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチ化埋め込み形状: </span><span class=si>{</span><span class=n>patch_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-22><a id=__codelineno-49-22 name=__codelineno-49-22 href=#__codelineno-49-22></a>
</span><span id=__span-49-23><a id=__codelineno-49-23 name=__codelineno-49-23 href=#__codelineno-49-23></a><span class=c1># 6. クラストークン埋め込みを作成</span>
</span><span id=__span-49-24><a id=__codelineno-49-24 name=__codelineno-49-24 href=#__codelineno-49-24></a><span class=n>batch_size</span> <span class=o>=</span> <span class=n>patch_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span><span id=__span-49-25><a id=__codelineno-49-25 name=__codelineno-49-25 href=#__codelineno-49-25></a><span class=n>embedding_dimension</span> <span class=o>=</span> <span class=n>patch_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-49-26><a id=__codelineno-49-26 name=__codelineno-49-26 href=#__codelineno-49-26></a><span class=n>class_token</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>embedding_dimension</span><span class=p>),</span>
</span><span id=__span-49-27><a id=__codelineno-49-27 name=__codelineno-49-27 href=#__codelineno-49-27></a>                           <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 学習可能であることを確認</span>
</span><span id=__span-49-28><a id=__codelineno-49-28 name=__codelineno-49-28 href=#__codelineno-49-28></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;クラストークン埋め込み形状: </span><span class=si>{</span><span class=n>class_token</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-29><a id=__codelineno-49-29 name=__codelineno-49-29 href=#__codelineno-49-29></a>
</span><span id=__span-49-30><a id=__codelineno-49-30 name=__codelineno-49-30 href=#__codelineno-49-30></a><span class=c1># 7. クラストークン埋め込みをパッチ埋め込みの前に追加</span>
</span><span id=__span-49-31><a id=__codelineno-49-31 name=__codelineno-49-31 href=#__codelineno-49-31></a><span class=n>patch_embedding_class_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>class_token</span><span class=p>,</span> <span class=n>patch_embedding</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-49-32><a id=__codelineno-49-32 name=__codelineno-49-32 href=#__codelineno-49-32></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;クラストークン付きパッチ埋め込み形状: </span><span class=si>{</span><span class=n>patch_embedding_class_token</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-33><a id=__codelineno-49-33 name=__codelineno-49-33 href=#__codelineno-49-33></a>
</span><span id=__span-49-34><a id=__codelineno-49-34 name=__codelineno-49-34 href=#__codelineno-49-34></a><span class=c1># 8. 位置埋め込みを作成</span>
</span><span id=__span-49-35><a id=__codelineno-49-35 name=__codelineno-49-35 href=#__codelineno-49-35></a><span class=n>number_of_patches</span> <span class=o>=</span> <span class=nb>int</span><span class=p>((</span><span class=n>height</span> <span class=o>*</span> <span class=n>width</span><span class=p>)</span> <span class=o>/</span> <span class=n>patch_size</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-49-36><a id=__codelineno-49-36 name=__codelineno-49-36 href=#__codelineno-49-36></a><span class=n>position_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>number_of_patches</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>embedding_dimension</span><span class=p>),</span>
</span><span id=__span-49-37><a id=__codelineno-49-37 name=__codelineno-49-37 href=#__codelineno-49-37></a>                                  <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 学習可能であることを確認</span>
</span><span id=__span-49-38><a id=__codelineno-49-38 name=__codelineno-49-38 href=#__codelineno-49-38></a>
</span><span id=__span-49-39><a id=__codelineno-49-39 name=__codelineno-49-39 href=#__codelineno-49-39></a><span class=c1># 9. 位置埋め込みをクラストークン付きパッチ埋め込みに追加</span>
</span><span id=__span-49-40><a id=__codelineno-49-40 name=__codelineno-49-40 href=#__codelineno-49-40></a><span class=n>patch_and_position_embedding</span> <span class=o>=</span> <span class=n>patch_embedding_class_token</span> <span class=o>+</span> <span class=n>position_embedding</span>
</span><span id=__span-49-41><a id=__codelineno-49-41 name=__codelineno-49-41 href=#__codelineno-49-41></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;パッチと位置埋め込み形状: </span><span class=si>{</span><span class=n>patch_and_position_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a>画像テンソル形状: torch.Size([3, 224, 224])
</span><span id=__span-50-2><a id=__codelineno-50-2 name=__codelineno-50-2 href=#__codelineno-50-2></a>バッチ次元付き入力画像形状: torch.Size([1, 3, 224, 224])
</span><span id=__span-50-3><a id=__codelineno-50-3 name=__codelineno-50-3 href=#__codelineno-50-3></a>パッチ化埋め込み形状: torch.Size([1, 196, 768])
</span><span id=__span-50-4><a id=__codelineno-50-4 name=__codelineno-50-4 href=#__codelineno-50-4></a>クラストークン埋め込み形状: torch.Size([1, 1, 768])
</span><span id=__span-50-5><a id=__codelineno-50-5 name=__codelineno-50-5 href=#__codelineno-50-5></a>クラストークン付きパッチ埋め込み形状: torch.Size([1, 197, 768])
</span><span id=__span-50-6><a id=__codelineno-50-6 name=__codelineno-50-6 href=#__codelineno-50-6></a>パッチと位置埋め込み形状: torch.Size([1, 197, 768])
</span></code></pre></div></p> <p>素晴らしい！単一の画像から単一のコードセルでパッチと位置埋め込みまで変換できました。</p> <p><img alt=ViT数式1の実装 src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-1-putting-it-all-together.png></p> <p><em>ViT論文の数式1をPyTorchコードにマッピング。これが論文再現実装の本質で、研究論文を実用的なコードに変換することです。</em></p> <p>これで画像をエンコードし、ViT論文のFigure 1のTransformer Encoderに渡す方法ができました。</p> <h2 id=5-2msa>5. 数式2の実装：マルチヘッドアテンション（MSA）<a class=headerlink href=#5-2msa title="Permanent link">&para;</a></h2> <p>入力データのパッチ化と埋め込みができたので、ViTアーキテクチャの次の部分に移りましょう。</p> <p>まず、Transformer Encoderセクションを2つの部分に分解します（小さく始めて必要に応じて増やします）。</p> <p>最初が数式2、次が数式3です。</p> <p>数式2を思い出してください：</p> <div class=arithmatex>\[ \mathbf{z}_{\ell}^{\prime} =\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, \quad \ell=1 \ldots L \]</div> <p>これは、LayerNorm（LN）層でラップされたMulti-Head Attention（MSA）層と残差接続（層への入力が層の出力に追加される）を示しています。</p> <p>数式2を「MSAブロック」と呼びます。</p> <p><img alt=ViT数式2のマッピング src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-2-msa-block-mapped-to-equation.png></p> <p><em>左：ViT論文のFigure 1で、Transformer Encoderブロック内のMulti-Head AttentionとNorm層、および残差接続（+）がハイライトされています。右：Multi-Head Self Attention（MSA）層、Norm層、残差接続をViT論文の数式2の各部分にマッピング。</em></p> <h3 id=51-layernormln>5.1 LayerNorm（LN）層<a class=headerlink href=#51-layernormln title="Permanent link">&para;</a></h3> <p><a href=https://paperswithcode.com/method/layer-normalization>Layer Normalization</a>（<code>torch.nn.LayerNorm()</code>）は、最後の次元にわたって入力を正規化します。</p> <p>PyTorchの<code>torch.nn.LayerNorm()</code>の主要パラメータは<code>normalized_shape</code>で、これを正規化したい次元サイズと等しく設定できます（我々の場合はViT-Baseで<span class=arithmatex>\(D\)</span>または<code>768</code>）。</p> <h3 id=52-msa>5.2 マルチヘッド自己アテンション（MSA）層<a class=headerlink href=#52-msa title="Permanent link">&para;</a></h3> <p>自己注意とマルチヘッドアテンション（自己注意を複数回適用）の力は、<a href=https://arxiv.org/abs/1706.03762><em>Attention is all you need</em></a>研究論文で紹介された元のTransformerアーキテクチャの形で明らかになりました。</p> <p>元々テキスト入力用に設計された元の自己注意メカニズムは、単語のシーケンスを受け取り、どの単語が他の単語により多くの「注意」を払うべきかを計算します。</p> <p>画像パッチのシーケンスが入力なので、自己注意とマルチヘッドアテンションは、画像のどのパッチが別のパッチと最も関連しているかを計算し、最終的に画像の学習された表現を形成します。</p> <h3 id=53-pytorch2>5.3 PyTorch層で数式2を複製<a class=headerlink href=#53-pytorch2 title="Permanent link">&para;</a></h3> <p>数式2のLayerNorm（LN）とMulti-Head Attention（MSA）層について議論したことを実践してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a><span class=c1># 1. nn.Moduleを継承するクラスを作成</span>
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a><span class=k>class</span><span class=w> </span><span class=nc>MultiheadSelfAttentionBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;マルチヘッド自己注意ブロック（「MSAブロック」の略）を作成します。</span>
</span><span id=__span-51-4><a id=__codelineno-51-4 name=__codelineno-51-4 href=#__codelineno-51-4></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-51-5><a id=__codelineno-51-5 name=__codelineno-51-5 href=#__codelineno-51-5></a>    <span class=c1># 2. Table 1のハイパーパラメータでクラスを初期化</span>
</span><span id=__span-51-6><a id=__codelineno-51-6 name=__codelineno-51-6 href=#__codelineno-51-6></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span><span id=__span-51-7><a id=__codelineno-51-7 name=__codelineno-51-7 href=#__codelineno-51-7></a>                 <span class=n>embedding_dim</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHidden size D</span>
</span><span id=__span-51-8><a id=__codelineno-51-8 name=__codelineno-51-8 href=#__codelineno-51-8></a>                 <span class=n>num_heads</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHeads</span>
</span><span id=__span-51-9><a id=__codelineno-51-9 name=__codelineno-51-9 href=#__codelineno-51-9></a>                 <span class=n>attn_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span> <span class=c1># 論文ではMSABlocksでドロップアウトを使用していないようです</span>
</span><span id=__span-51-10><a id=__codelineno-51-10 name=__codelineno-51-10 href=#__codelineno-51-10></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-51-11><a id=__codelineno-51-11 name=__codelineno-51-11 href=#__codelineno-51-11></a>
</span><span id=__span-51-12><a id=__codelineno-51-12 name=__codelineno-51-12 href=#__codelineno-51-12></a>        <span class=c1># 3. Norm層（LN）を作成</span>
</span><span id=__span-51-13><a id=__codelineno-51-13 name=__codelineno-51-13 href=#__codelineno-51-13></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>normalized_shape</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>)</span>
</span><span id=__span-51-14><a id=__codelineno-51-14 name=__codelineno-51-14 href=#__codelineno-51-14></a>
</span><span id=__span-51-15><a id=__codelineno-51-15 name=__codelineno-51-15 href=#__codelineno-51-15></a>        <span class=c1># 4. Multi-Head Attention（MSA）層を作成</span>
</span><span id=__span-51-16><a id=__codelineno-51-16 name=__codelineno-51-16 href=#__codelineno-51-16></a>        <span class=bp>self</span><span class=o>.</span><span class=n>multihead_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span><span class=n>embed_dim</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-51-17><a id=__codelineno-51-17 name=__codelineno-51-17 href=#__codelineno-51-17></a>                                                    <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
</span><span id=__span-51-18><a id=__codelineno-51-18 name=__codelineno-51-18 href=#__codelineno-51-18></a>                                                    <span class=n>dropout</span><span class=o>=</span><span class=n>attn_dropout</span><span class=p>,</span>
</span><span id=__span-51-19><a id=__codelineno-51-19 name=__codelineno-51-19 href=#__codelineno-51-19></a>                                                    <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># バッチ次元が最初に来るか？</span>
</span><span id=__span-51-20><a id=__codelineno-51-20 name=__codelineno-51-20 href=#__codelineno-51-20></a>
</span><span id=__span-51-21><a id=__codelineno-51-21 name=__codelineno-51-21 href=#__codelineno-51-21></a>    <span class=c1># 5. データを層に通すforward()メソッドを作成</span>
</span><span id=__span-51-22><a id=__codelineno-51-22 name=__codelineno-51-22 href=#__codelineno-51-22></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-51-23><a id=__codelineno-51-23 name=__codelineno-51-23 href=#__codelineno-51-23></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-51-24><a id=__codelineno-51-24 name=__codelineno-51-24 href=#__codelineno-51-24></a>        <span class=n>attn_output</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>multihead_attn</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=c1># クエリ埋め込み</span>
</span><span id=__span-51-25><a id=__codelineno-51-25 name=__codelineno-51-25 href=#__codelineno-51-25></a>                                             <span class=n>key</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=c1># キー埋め込み</span>
</span><span id=__span-51-26><a id=__codelineno-51-26 name=__codelineno-51-26 href=#__codelineno-51-26></a>                                             <span class=n>value</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=c1># バリュー埋め込み</span>
</span><span id=__span-51-27><a id=__codelineno-51-27 name=__codelineno-51-27 href=#__codelineno-51-27></a>                                             <span class=n>need_weights</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># 重みが必要か、層の出力だけか？</span>
</span><span id=__span-51-28><a id=__codelineno-51-28 name=__codelineno-51-28 href=#__codelineno-51-28></a>        <span class=k>return</span> <span class=n>attn_output</span>
</span></code></pre></div> <p>MSABlockを作成しました！<code>MultiheadSelfAttentionBlock</code>のインスタンスを作成し、セクション4.8で作成した<code>patch_and_position_embedding</code>変数を通してテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=c1># MSABlockのインスタンスを作成</span>
</span><span id=__span-52-2><a id=__codelineno-52-2 name=__codelineno-52-2 href=#__codelineno-52-2></a><span class=n>multihead_self_attention_block</span> <span class=o>=</span> <span class=n>MultiheadSelfAttentionBlock</span><span class=p>(</span><span class=n>embedding_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1より</span>
</span><span id=__span-52-3><a id=__codelineno-52-3 name=__codelineno-52-3 href=#__codelineno-52-3></a>                                                             <span class=n>num_heads</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span> <span class=c1># Table 1より</span>
</span><span id=__span-52-4><a id=__codelineno-52-4 name=__codelineno-52-4 href=#__codelineno-52-4></a>
</span><span id=__span-52-5><a id=__codelineno-52-5 name=__codelineno-52-5 href=#__codelineno-52-5></a><span class=c1># パッチと位置画像埋め込みをMSABlockに通す</span>
</span><span id=__span-52-6><a id=__codelineno-52-6 name=__codelineno-52-6 href=#__codelineno-52-6></a><span class=n>patched_image_through_msa_block</span> <span class=o>=</span> <span class=n>multihead_self_attention_block</span><span class=p>(</span><span class=n>patch_and_position_embedding</span><span class=p>)</span>
</span><span id=__span-52-7><a id=__codelineno-52-7 name=__codelineno-52-7 href=#__codelineno-52-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MSAブロックの入力形状: </span><span class=si>{</span><span class=n>patch_and_position_embedding</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-52-8><a id=__codelineno-52-8 name=__codelineno-52-8 href=#__codelineno-52-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MSAブロックの出力形状: </span><span class=si>{</span><span class=n>patched_image_through_msa_block</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a>MSAブロックの入力形状: torch.Size([1, 197, 768])
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a>MSAブロックの出力形状: torch.Size([1, 197, 768])
</span></code></pre></div></p> <p><img alt=ViT数式2のコード実装 src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-2-in-code.png></p> <p><em>左：Multi-Head AttentionとLayerNorm層がハイライトされたViTアーキテクチャ。これらの層が論文の3.1節の数式2を構成します。右：PyTorch層を使用した数式2の複製（最後のスキップ接続を除く）。</em></p> <p>数式2を正式に複製しました（最後の残差接続を除きますが、セクション7.1で取り上げます）！</p> <h2 id=6-3mlp>6. 数式3の実装：多層パーセプトロン（MLP）<a class=headerlink href=#6-3mlp title="Permanent link">&para;</a></h2> <p>勢いに乗って続けましょう！数式3を複製します：</p> <div class=arithmatex>\[ \mathbf{z}_{\ell} =\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, \quad \ell=1 \ldots L \]</div> <p>ここでMLPは「multilayer perceptron（多層パーセプトロン）」、LNは「layer normalization」を表します。</p> <p>最後の加算はスキップ/残差接続です。</p> <p>数式3をTransformer encoderの「MLPブロック」と呼びます。</p> <p><img alt=ViT数式3のマッピング src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-3-annotated.png></p> <p><em>左：ViT論文のFigure 1で、Transformer Encoderブロック内のMLPとNorm層、および残差接続（+）がハイライトされています。右：多層パーセプトロン（MLP）層、Norm層（LN）、残差接続をViT論文の数式3の各部分にマッピング。</em></p> <h3 id=61-mlp>6.1 MLP層<a class=headerlink href=#61-mlp title="Permanent link">&para;</a></h3> <p><a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>MLP</a>という用語は、ほぼ任意の<em>複数</em>層の組み合わせを指すことができるため、非常に広範囲です。</p> <p>しかし、一般的には次のパターンに従います：</p> <p><code>線形層 -&gt; 非線形層 -&gt; 線形層 -&gt; 非線形層</code></p> <p>ViT論文の場合、MLP構造は3.1節で定義されています：</p> <blockquote> <p>MLPには、GELU非線形性を持つ2つの層が含まれています。</p> </blockquote> <p>「2つの層」は線形層（PyTorchの<a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html><code>torch.nn.Linear()</code></a>）を指し、「GELU非線形性」はGELU（Gaussian Error Linear Units）非線形活性化関数（PyTorchの<a href=https://pytorch.org/docs/stable/generated/torch.nn.GELU.html><code>torch.nn.GELU()</code></a>）です。</p> <p>MLPブロックに関するもう一つの詳細は、付録B.1（Training）まで現れません：</p> <blockquote> <p>ドロップアウトが使用される場合、qkv-projectionと位置埋め込みをパッチ埋め込みに直接追加した後を除いて、<strong>すべての密結合層の後に適用されます。</strong></p> </blockquote> <p>これは、MLPブロック内のすべての線形層（または密結合層）にドロップアウト層（PyTorchの<a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html><code>torch.nn.Dropout()</code></a>）があることを意味します。</p> <h3 id=62-pytorch3>6.2 PyTorch層で数式3を複製<a class=headerlink href=#62-pytorch3 title="Permanent link">&para;</a></h3> <p>数式3のLayerNorm（LN）とMLP層について議論したことを実践してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=c1># 1. nn.Moduleを継承するクラスを作成</span>
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a><span class=k>class</span><span class=w> </span><span class=nc>MLPBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-54-3><a id=__codelineno-54-3 name=__codelineno-54-3 href=#__codelineno-54-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;層正規化多層パーセプトロンブロック（「MLPブロック」の略）を作成します。&quot;&quot;&quot;</span>
</span><span id=__span-54-4><a id=__codelineno-54-4 name=__codelineno-54-4 href=#__codelineno-54-4></a>    <span class=c1># 2. Table 1とTable 3のハイパーパラメータでクラスを初期化</span>
</span><span id=__span-54-5><a id=__codelineno-54-5 name=__codelineno-54-5 href=#__codelineno-54-5></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span><span id=__span-54-6><a id=__codelineno-54-6 name=__codelineno-54-6 href=#__codelineno-54-6></a>                 <span class=n>embedding_dim</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHidden Size D</span>
</span><span id=__span-54-7><a id=__codelineno-54-7 name=__codelineno-54-7 href=#__codelineno-54-7></a>                 <span class=n>mlp_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのMLP size</span>
</span><span id=__span-54-8><a id=__codelineno-54-8 name=__codelineno-54-8 href=#__codelineno-54-8></a>                 <span class=n>dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span> <span class=c1># Table 3のViT-BaseのDropout</span>
</span><span id=__span-54-9><a id=__codelineno-54-9 name=__codelineno-54-9 href=#__codelineno-54-9></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-54-10><a id=__codelineno-54-10 name=__codelineno-54-10 href=#__codelineno-54-10></a>
</span><span id=__span-54-11><a id=__codelineno-54-11 name=__codelineno-54-11 href=#__codelineno-54-11></a>        <span class=c1># 3. Norm層（LN）を作成</span>
</span><span id=__span-54-12><a id=__codelineno-54-12 name=__codelineno-54-12 href=#__codelineno-54-12></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>normalized_shape</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>)</span>
</span><span id=__span-54-13><a id=__codelineno-54-13 name=__codelineno-54-13 href=#__codelineno-54-13></a>
</span><span id=__span-54-14><a id=__codelineno-54-14 name=__codelineno-54-14 href=#__codelineno-54-14></a>        <span class=c1># 4. Multilayer perceptron（MLP）層を作成</span>
</span><span id=__span-54-15><a id=__codelineno-54-15 name=__codelineno-54-15 href=#__codelineno-54-15></a>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-54-16><a id=__codelineno-54-16 name=__codelineno-54-16 href=#__codelineno-54-16></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-54-17><a id=__codelineno-54-17 name=__codelineno-54-17 href=#__codelineno-54-17></a>                      <span class=n>out_features</span><span class=o>=</span><span class=n>mlp_size</span><span class=p>),</span>
</span><span id=__span-54-18><a id=__codelineno-54-18 name=__codelineno-54-18 href=#__codelineno-54-18></a>            <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(),</span> <span class=c1># &quot;MLPには、GELU非線形性を持つ2つの層が含まれています（3.1節）。&quot;</span>
</span><span id=__span-54-19><a id=__codelineno-54-19 name=__codelineno-54-19 href=#__codelineno-54-19></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>),</span>
</span><span id=__span-54-20><a id=__codelineno-54-20 name=__codelineno-54-20 href=#__codelineno-54-20></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=n>mlp_size</span><span class=p>,</span> <span class=c1># 上の層のout_featuresと同じin_featuresを取る必要があります</span>
</span><span id=__span-54-21><a id=__codelineno-54-21 name=__codelineno-54-21 href=#__codelineno-54-21></a>                      <span class=n>out_features</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>),</span> <span class=c1># embedding_dimに戻す</span>
</span><span id=__span-54-22><a id=__codelineno-54-22 name=__codelineno-54-22 href=#__codelineno-54-22></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span> <span class=c1># &quot;ドロップアウトが使用される場合、すべての密結合層の後に適用されます..&quot;</span>
</span><span id=__span-54-23><a id=__codelineno-54-23 name=__codelineno-54-23 href=#__codelineno-54-23></a>        <span class=p>)</span>
</span><span id=__span-54-24><a id=__codelineno-54-24 name=__codelineno-54-24 href=#__codelineno-54-24></a>
</span><span id=__span-54-25><a id=__codelineno-54-25 name=__codelineno-54-25 href=#__codelineno-54-25></a>    <span class=c1># 5. データを層に通すforward()メソッドを作成</span>
</span><span id=__span-54-26><a id=__codelineno-54-26 name=__codelineno-54-26 href=#__codelineno-54-26></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-54-27><a id=__codelineno-54-27 name=__codelineno-54-27 href=#__codelineno-54-27></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-54-28><a id=__codelineno-54-28 name=__codelineno-54-28 href=#__codelineno-54-28></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-54-29><a id=__codelineno-54-29 name=__codelineno-54-29 href=#__codelineno-54-29></a>        <span class=k>return</span> <span class=n>x</span>
</span></code></pre></div> <p>MLPBlockクラスを作成しました！<code>MLPBlock</code>のインスタンスを作成し、セクション5.3で作成した<code>patched_image_through_msa_block</code>変数を通してテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a><span class=c1># MLPBlockのインスタンスを作成</span>
</span><span id=__span-55-2><a id=__codelineno-55-2 name=__codelineno-55-2 href=#__codelineno-55-2></a><span class=n>mlp_block</span> <span class=o>=</span> <span class=n>MLPBlock</span><span class=p>(</span><span class=n>embedding_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1より</span>
</span><span id=__span-55-3><a id=__codelineno-55-3 name=__codelineno-55-3 href=#__codelineno-55-3></a>                     <span class=n>mlp_size</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span> <span class=c1># Table 1より</span>
</span><span id=__span-55-4><a id=__codelineno-55-4 name=__codelineno-55-4 href=#__codelineno-55-4></a>                     <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span> <span class=c1># Table 3より</span>
</span><span id=__span-55-5><a id=__codelineno-55-5 name=__codelineno-55-5 href=#__codelineno-55-5></a>
</span><span id=__span-55-6><a id=__codelineno-55-6 name=__codelineno-55-6 href=#__codelineno-55-6></a><span class=c1># MSABlockの出力をMLPBlockに通す</span>
</span><span id=__span-55-7><a id=__codelineno-55-7 name=__codelineno-55-7 href=#__codelineno-55-7></a><span class=n>patched_image_through_mlp_block</span> <span class=o>=</span> <span class=n>mlp_block</span><span class=p>(</span><span class=n>patched_image_through_msa_block</span><span class=p>)</span>
</span><span id=__span-55-8><a id=__codelineno-55-8 name=__codelineno-55-8 href=#__codelineno-55-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MLPブロックの入力形状: </span><span class=si>{</span><span class=n>patched_image_through_msa_block</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-55-9><a id=__codelineno-55-9 name=__codelineno-55-9 href=#__codelineno-55-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MLPブロックの出力形状: </span><span class=si>{</span><span class=n>patched_image_through_mlp_block</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a>MLPブロックの入力形状: torch.Size([1, 197, 768])
</span><span id=__span-56-2><a id=__codelineno-56-2 name=__codelineno-56-2 href=#__codelineno-56-2></a>MLPブロックの出力形状: torch.Size([1, 197, 768])
</span></code></pre></div></p> <p><img alt=ViT数式3のコード実装 src=../09_pytorch_paper_replicating_files/09-vit-paper-equation-3-mapped-to-code.png></p> <p><em>左：MLPとNorm層がハイライトされたViTアーキテクチャ。これらの層が論文の3.1節の数式3を構成します。右：PyTorch層を使用した数式3の複製（最後のスキップ接続を除く）。</em></p> <p>数式3を複製しました（最後の残差接続を除きますが、セクション7.1で取り上げます）！</p> <p>これで数式2と3をPyTorchコードで作成したので、それらを組み合わせてTransformer Encoderを作成しましょう。</p> <h2 id=7-transformer-encoder>7. Transformer Encoderの作成<a class=headerlink href=#7-transformer-encoder title="Permanent link">&para;</a></h2> <p>前編で作成した<code>MultiheadSelfAttentionBlock</code>（数式2）と<code>MLPBlock</code>（数式3）を組み合わせて、ViTアーキテクチャのTransformer Encoderを作成しましょう。</p> <p>深層学習において、<a href=https://paperswithcode.com/method/autoencoder>「encoder」や「auto encoder」</a>は一般的に、入力を何らかの数値表現に「エンコード」する層のスタックを指します。</p> <p>ViT論文の3.1節によると：</p> <blockquote> <p>Transformer encoder（Vaswani et al., 2017）は、マルチヘッド自己注意（MSA）とMLPブロックの交互の層で構成されます。<strong>すべてのブロックの前にLayernorm（LN）が適用され</strong>、<strong>すべてのブロックの後に残差接続</strong>が適用されます。</p> </blockquote> <p>MSAブロックとMLPブロックは作成しましたが、残差接続はどうでしょうか？</p> <p><a href=https://paperswithcode.com/method/residual-connection>残差接続</a>（スキップ接続とも呼ばれる）は、<a href=https://arxiv.org/abs/1512.03385v1><em>Deep Residual Learning for Image Recognition</em></a>論文で初めて導入され、層の入力を後続の出力に追加することで実現されます。</p> <p>ViTアーキテクチャの場合、残差接続は、MSAブロックの入力がMSAブロックの出力に追加されてからMLPブロックに渡されることを意味します。</p> <h3 id=71-transformer-encoder>7.1 カスタム作成層を組み合わせてTransformer Encoderを作成<a class=headerlink href=#71-transformer-encoder title="Permanent link">&para;</a></h3> <p>PyTorchで以前作成した層を組み合わせて、ViT Transformer Encoderを作成しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a><span class=c1># 1. nn.Moduleを継承するクラスを作成</span>
</span><span id=__span-57-2><a id=__codelineno-57-2 name=__codelineno-57-2 href=#__codelineno-57-2></a><span class=k>class</span><span class=w> </span><span class=nc>TransformerEncoderBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-57-3><a id=__codelineno-57-3 name=__codelineno-57-3 href=#__codelineno-57-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Transformer Encoderブロックを作成します。&quot;&quot;&quot;</span>
</span><span id=__span-57-4><a id=__codelineno-57-4 name=__codelineno-57-4 href=#__codelineno-57-4></a>    <span class=c1># 2. Table 1とTable 3のハイパーパラメータでクラスを初期化</span>
</span><span id=__span-57-5><a id=__codelineno-57-5 name=__codelineno-57-5 href=#__codelineno-57-5></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span><span id=__span-57-6><a id=__codelineno-57-6 name=__codelineno-57-6 href=#__codelineno-57-6></a>                 <span class=n>embedding_dim</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHidden size D</span>
</span><span id=__span-57-7><a id=__codelineno-57-7 name=__codelineno-57-7 href=#__codelineno-57-7></a>                 <span class=n>num_heads</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHeads</span>
</span><span id=__span-57-8><a id=__codelineno-57-8 name=__codelineno-57-8 href=#__codelineno-57-8></a>                 <span class=n>mlp_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのMLP size</span>
</span><span id=__span-57-9><a id=__codelineno-57-9 name=__codelineno-57-9 href=#__codelineno-57-9></a>                 <span class=n>mlp_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=c1># Table 3のViT-Baseの密結合層のドロップアウト量</span>
</span><span id=__span-57-10><a id=__codelineno-57-10 name=__codelineno-57-10 href=#__codelineno-57-10></a>                 <span class=n>attn_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span> <span class=c1># アテンション層のドロップアウト量</span>
</span><span id=__span-57-11><a id=__codelineno-57-11 name=__codelineno-57-11 href=#__codelineno-57-11></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-57-12><a id=__codelineno-57-12 name=__codelineno-57-12 href=#__codelineno-57-12></a>
</span><span id=__span-57-13><a id=__codelineno-57-13 name=__codelineno-57-13 href=#__codelineno-57-13></a>        <span class=c1># 3. MSAブロック（数式2）を作成</span>
</span><span id=__span-57-14><a id=__codelineno-57-14 name=__codelineno-57-14 href=#__codelineno-57-14></a>        <span class=bp>self</span><span class=o>.</span><span class=n>msa_block</span> <span class=o>=</span> <span class=n>MultiheadSelfAttentionBlock</span><span class=p>(</span><span class=n>embedding_dim</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-57-15><a id=__codelineno-57-15 name=__codelineno-57-15 href=#__codelineno-57-15></a>                                                     <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
</span><span id=__span-57-16><a id=__codelineno-57-16 name=__codelineno-57-16 href=#__codelineno-57-16></a>                                                     <span class=n>attn_dropout</span><span class=o>=</span><span class=n>attn_dropout</span><span class=p>)</span>
</span><span id=__span-57-17><a id=__codelineno-57-17 name=__codelineno-57-17 href=#__codelineno-57-17></a>
</span><span id=__span-57-18><a id=__codelineno-57-18 name=__codelineno-57-18 href=#__codelineno-57-18></a>        <span class=c1># 4. MLPブロック（数式3）を作成</span>
</span><span id=__span-57-19><a id=__codelineno-57-19 name=__codelineno-57-19 href=#__codelineno-57-19></a>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp_block</span> <span class=o>=</span>  <span class=n>MLPBlock</span><span class=p>(</span><span class=n>embedding_dim</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-57-20><a id=__codelineno-57-20 name=__codelineno-57-20 href=#__codelineno-57-20></a>                                   <span class=n>mlp_size</span><span class=o>=</span><span class=n>mlp_size</span><span class=p>,</span>
</span><span id=__span-57-21><a id=__codelineno-57-21 name=__codelineno-57-21 href=#__codelineno-57-21></a>                                   <span class=n>dropout</span><span class=o>=</span><span class=n>mlp_dropout</span><span class=p>)</span>
</span><span id=__span-57-22><a id=__codelineno-57-22 name=__codelineno-57-22 href=#__codelineno-57-22></a>
</span><span id=__span-57-23><a id=__codelineno-57-23 name=__codelineno-57-23 href=#__codelineno-57-23></a>    <span class=c1># 5. forward()メソッドを作成</span>
</span><span id=__span-57-24><a id=__codelineno-57-24 name=__codelineno-57-24 href=#__codelineno-57-24></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-57-25><a id=__codelineno-57-25 name=__codelineno-57-25 href=#__codelineno-57-25></a>        <span class=c1># 6. MSAブロック用の残差接続（入力を出力に追加）</span>
</span><span id=__span-57-26><a id=__codelineno-57-26 name=__codelineno-57-26 href=#__codelineno-57-26></a>        <span class=n>x</span> <span class=o>=</span>  <span class=bp>self</span><span class=o>.</span><span class=n>msa_block</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>x</span>
</span><span id=__span-57-27><a id=__codelineno-57-27 name=__codelineno-57-27 href=#__codelineno-57-27></a>
</span><span id=__span-57-28><a id=__codelineno-57-28 name=__codelineno-57-28 href=#__codelineno-57-28></a>        <span class=c1># 7. MLPブロック用の残差接続（入力を出力に追加）</span>
</span><span id=__span-57-29><a id=__codelineno-57-29 name=__codelineno-57-29 href=#__codelineno-57-29></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp_block</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>x</span>
</span><span id=__span-57-30><a id=__codelineno-57-30 name=__codelineno-57-30 href=#__codelineno-57-30></a>
</span><span id=__span-57-31><a id=__codelineno-57-31 name=__codelineno-57-31 href=#__codelineno-57-31></a>        <span class=k>return</span> <span class=n>x</span>
</span></code></pre></div> <p>素晴らしい！Transformer Encoderブロックが作成できました！</p> <p><img alt="Transformer Encoderハイライト" src=../09_pytorch_paper_replicating_files/09-vit-paper-transformer-encoder-highlighted.png></p> <p><em>左：ViT論文のFigure 1で、ViTアーキテクチャのTransformer Encoderがハイライトされています。右：ViT論文の数式2と3にマッピングされたTransformer Encoder。</em></p> <p>レゴのように全体のアーキテクチャを一つずつピース（数式）ごとにコーディングして組み立てていく様子がわかります。</p> <p>Transformer Encoderブロックのインスタンスを作成してテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=c1># TransformerEncoderBlockのインスタンスを作成</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=n>transformer_encoder_block</span> <span class=o>=</span> <span class=n>TransformerEncoderBlock</span><span class=p>()</span>
</span><span id=__span-58-3><a id=__codelineno-58-3 name=__codelineno-58-3 href=#__codelineno-58-3></a>
</span><span id=__span-58-4><a id=__codelineno-58-4 name=__codelineno-58-4 href=#__codelineno-58-4></a><span class=c1># 入力形状(1, 197, 768) -&gt; (batch_size, num_patches, embedding_dimension)でのサマリーを表示</span>
</span><span id=__span-58-5><a id=__codelineno-58-5 name=__codelineno-58-5 href=#__codelineno-58-5></a><span class=n>summary</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>transformer_encoder_block</span><span class=p>,</span>
</span><span id=__span-58-6><a id=__codelineno-58-6 name=__codelineno-58-6 href=#__codelineno-58-6></a>        <span class=n>input_size</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>197</span><span class=p>,</span> <span class=mi>768</span><span class=p>),</span>
</span><span id=__span-58-7><a id=__codelineno-58-7 name=__codelineno-58-7 href=#__codelineno-58-7></a>        <span class=n>col_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;input_size&quot;</span><span class=p>,</span> <span class=s2>&quot;output_size&quot;</span><span class=p>,</span> <span class=s2>&quot;num_params&quot;</span><span class=p>,</span> <span class=s2>&quot;trainable&quot;</span><span class=p>],</span>
</span><span id=__span-58-8><a id=__codelineno-58-8 name=__codelineno-58-8 href=#__codelineno-58-8></a>        <span class=n>col_width</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
</span><span id=__span-58-9><a id=__codelineno-58-9 name=__codelineno-58-9 href=#__codelineno-58-9></a>        <span class=n>row_settings</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;var_names&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> すべてのパラメータをチェックしてください！入力がTransformer EncoderブロックのMSAブロックとMLPブロックの様々な層を移動する際に形状が変化し、最終的に元の形状に戻るのがわかります。</p> <h3 id=72-pytorchtransformertransformer-encoder>7.2 PyTorchのTransformer層でTransformer Encoderを作成<a class=headerlink href=#72-pytorchtransformertransformer-encoder title="Permanent link">&para;</a></h3> <p>これまで、Transformer Encoderレイヤーのコンポーネントとレイヤー自体を自分で構築してきました。</p> <p>しかし、人気と効果の高まりにより、PyTorchには<a href=https://pytorch.org/docs/stable/nn.html#transformer-layers><code>torch.nn</code>の一部として組み込みのTransformerレイヤー</a>があります。</p> <p>例えば、<a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html><code>torch.nn.TransformerEncoderLayer()</code></a>を使用して、上記と同じハイパーパラメータを設定することで、作成したばかりの<code>TransformerEncoderBlock</code>を再作成できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a><span class=c1># torch.nn.TransformerEncoderLayer()で上記と同じものを作成</span>
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a><span class=n>torch_transformer_encoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHidden size D</span>
</span><span id=__span-59-3><a id=__codelineno-59-3 name=__codelineno-59-3 href=#__codelineno-59-3></a>                                                             <span class=n>nhead</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHeads</span>
</span><span id=__span-59-4><a id=__codelineno-59-4 name=__codelineno-59-4 href=#__codelineno-59-4></a>                                                             <span class=n>dim_feedforward</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのMLP size</span>
</span><span id=__span-59-5><a id=__codelineno-59-5 name=__codelineno-59-5 href=#__codelineno-59-5></a>                                                             <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=c1># Table 3のViT-Baseの密結合層のドロップアウト量</span>
</span><span id=__span-59-6><a id=__codelineno-59-6 name=__codelineno-59-6 href=#__codelineno-59-6></a>                                                             <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;gelu&quot;</span><span class=p>,</span> <span class=c1># GELU非線形活性化</span>
</span><span id=__span-59-7><a id=__codelineno-59-7 name=__codelineno-59-7 href=#__codelineno-59-7></a>                                                             <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=c1># バッチが最初に来るか？</span>
</span><span id=__span-59-8><a id=__codelineno-59-8 name=__codelineno-59-8 href=#__codelineno-59-8></a>                                                             <span class=n>norm_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># MSA/MLP層の前または後に正規化するか？</span>
</span><span id=__span-59-9><a id=__codelineno-59-9 name=__codelineno-59-9 href=#__codelineno-59-9></a>
</span><span id=__span-59-10><a id=__codelineno-59-10 name=__codelineno-59-10 href=#__codelineno-59-10></a><span class=n>torch_transformer_encoder_layer</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a>TransformerEncoderLayer(
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a>  (self_attn): MultiheadAttention(
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a>    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
</span><span id=__span-60-4><a id=__codelineno-60-4 name=__codelineno-60-4 href=#__codelineno-60-4></a>  )
</span><span id=__span-60-5><a id=__codelineno-60-5 name=__codelineno-60-5 href=#__codelineno-60-5></a>  (linear1): Linear(in_features=768, out_features=3072, bias=True)
</span><span id=__span-60-6><a id=__codelineno-60-6 name=__codelineno-60-6 href=#__codelineno-60-6></a>  (dropout): Dropout(p=0.1, inplace=False)
</span><span id=__span-60-7><a id=__codelineno-60-7 name=__codelineno-60-7 href=#__codelineno-60-7></a>  (linear2): Linear(in_features=3072, out_features=768, bias=True)
</span><span id=__span-60-8><a id=__codelineno-60-8 name=__codelineno-60-8 href=#__codelineno-60-8></a>  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span><span id=__span-60-9><a id=__codelineno-60-9 name=__codelineno-60-9 href=#__codelineno-60-9></a>  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span><span id=__span-60-10><a id=__codelineno-60-10 name=__codelineno-60-10 href=#__codelineno-60-10></a>  (dropout1): Dropout(p=0.1, inplace=False)
</span><span id=__span-60-11><a id=__codelineno-60-11 name=__codelineno-60-11 href=#__codelineno-60-11></a>  (dropout2): Dropout(p=0.1, inplace=False)
</span><span id=__span-60-12><a id=__codelineno-60-12 name=__codelineno-60-12 href=#__codelineno-60-12></a>)
</span></code></pre></div></p> <p>「PyTorchレイヤーでTransformer Encoderをこんなに簡単に作れるなら、なぜわざわざ数式2と3を再現したのか？」と思うかもしれません。</p> <p>答えは：<strong>練習</strong>です。</p> <p>論文の一連の数式と層を複製したので、必要に応じて層を変更して何か違うことを試すことができます。</p> <h2 id=8-vit>8. すべてをまとめてViTを作成<a class=headerlink href=#8-vit title="Permanent link">&para;</a></h2> <p>これまで長い道のりを歩んできましたが、今度はパズルのすべてのピースを組み合わせる興奮的な作業の時間です！</p> <p>作成したすべてのブロックを組み合わせて、完全なViTアーキテクチャを複製します。パッチと位置埋め込みからTransformer EncoderまでMLPヘッドまで。</p> <p>しかし、まだ数式4を作成していません：</p> <div class=arithmatex>\[ \mathbf{y} =\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) \]</div> <p>心配ありません。数式4を全体のViTアーキテクチャクラスに組み込むことができます。必要なのは<code>torch.nn.LayerNorm()</code>層と<code>torch.nn.Linear()</code>層だけで、Transformer Encoderロジット出力の0番目のインデックス（<span class=arithmatex>\(\mathbf{z}_{L}^{0}\)</span>）を目標クラス数に変換します。</p> <p>完全なアーキテクチャを作成するため、複数の<code>TransformerEncoderBlock</code>を積み重ねる必要があります。これは、それらのリストを<code>torch.nn.Sequential()</code>に渡すことで実現できます。</p> <p>ViT-Baseのハイパーパラメータに焦点を当てますが、コードは他のViTバリアントに適応できるはずです。</p> <h3 id=81-vit>8.1 完全なViTアーキテクチャの実装<a class=headerlink href=#81-vit title="Permanent link">&para;</a></h3> <p>ViTを実現するために、私たちの最大のコードブロックになりますが、やり遂げましょう！</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a><span class=c1># 1. nn.Moduleを継承するViTクラスを作成</span>
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a><span class=k>class</span><span class=w> </span><span class=nc>ViT</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;デフォルトでViT-Baseハイパーパラメータを持つVision Transformerアーキテクチャを作成します。&quot;&quot;&quot;</span>
</span><span id=__span-61-4><a id=__codelineno-61-4 name=__codelineno-61-4 href=#__codelineno-61-4></a>    <span class=c1># 2. Table 1とTable 3のハイパーパラメータでクラスを初期化</span>
</span><span id=__span-61-5><a id=__codelineno-61-5 name=__codelineno-61-5 href=#__codelineno-61-5></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span><span id=__span-61-6><a id=__codelineno-61-6 name=__codelineno-61-6 href=#__codelineno-61-6></a>                 <span class=n>img_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>224</span><span class=p>,</span> <span class=c1># ViT論文のTable 3のTraining resolution</span>
</span><span id=__span-61-7><a id=__codelineno-61-7 name=__codelineno-61-7 href=#__codelineno-61-7></a>                 <span class=n>in_channels</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=c1># 入力画像のチャンネル数</span>
</span><span id=__span-61-8><a id=__codelineno-61-8 name=__codelineno-61-8 href=#__codelineno-61-8></a>                 <span class=n>patch_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=c1># パッチサイズ</span>
</span><span id=__span-61-9><a id=__codelineno-61-9 name=__codelineno-61-9 href=#__codelineno-61-9></a>                 <span class=n>num_transformer_layers</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのLayers</span>
</span><span id=__span-61-10><a id=__codelineno-61-10 name=__codelineno-61-10 href=#__codelineno-61-10></a>                 <span class=n>embedding_dim</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHidden size D</span>
</span><span id=__span-61-11><a id=__codelineno-61-11 name=__codelineno-61-11 href=#__codelineno-61-11></a>                 <span class=n>mlp_size</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのMLP size</span>
</span><span id=__span-61-12><a id=__codelineno-61-12 name=__codelineno-61-12 href=#__codelineno-61-12></a>                 <span class=n>num_heads</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=c1># Table 1のViT-BaseのHeads</span>
</span><span id=__span-61-13><a id=__codelineno-61-13 name=__codelineno-61-13 href=#__codelineno-61-13></a>                 <span class=n>attn_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1># アテンション射影のドロップアウト</span>
</span><span id=__span-61-14><a id=__codelineno-61-14 name=__codelineno-61-14 href=#__codelineno-61-14></a>                 <span class=n>mlp_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=c1># 密結合/MLP層のドロップアウト</span>
</span><span id=__span-61-15><a id=__codelineno-61-15 name=__codelineno-61-15 href=#__codelineno-61-15></a>                 <span class=n>embedding_dropout</span><span class=p>:</span><span class=nb>float</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=c1># パッチと位置埋め込みのドロップアウト</span>
</span><span id=__span-61-16><a id=__codelineno-61-16 name=__codelineno-61-16 href=#__codelineno-61-16></a>                 <span class=n>num_classes</span><span class=p>:</span><span class=nb>int</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span> <span class=c1># ImageNetのデフォルトですがカスタマイズ可能</span>
</span><span id=__span-61-17><a id=__codelineno-61-17 name=__codelineno-61-17 href=#__codelineno-61-17></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span> <span class=c1># super().__init__()を忘れずに！</span>
</span><span id=__span-61-18><a id=__codelineno-61-18 name=__codelineno-61-18 href=#__codelineno-61-18></a>
</span><span id=__span-61-19><a id=__codelineno-61-19 name=__codelineno-61-19 href=#__codelineno-61-19></a>        <span class=c1># 3. 画像サイズがパッチサイズで割り切れることを確認</span>
</span><span id=__span-61-20><a id=__codelineno-61-20 name=__codelineno-61-20 href=#__codelineno-61-20></a>        <span class=k>assert</span> <span class=n>img_size</span> <span class=o>%</span> <span class=n>patch_size</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&quot;画像サイズはパッチサイズで割り切れる必要があります。画像サイズ: </span><span class=si>{</span><span class=n>img_size</span><span class=si>}</span><span class=s2>、パッチサイズ: </span><span class=si>{</span><span class=n>patch_size</span><span class=si>}</span><span class=s2>。&quot;</span>
</span><span id=__span-61-21><a id=__codelineno-61-21 name=__codelineno-61-21 href=#__codelineno-61-21></a>
</span><span id=__span-61-22><a id=__codelineno-61-22 name=__codelineno-61-22 href=#__codelineno-61-22></a>        <span class=c1># 4. パッチ数を計算（高さ * 幅/パッチ^2）</span>
</span><span id=__span-61-23><a id=__codelineno-61-23 name=__codelineno-61-23 href=#__codelineno-61-23></a>        <span class=bp>self</span><span class=o>.</span><span class=n>num_patches</span> <span class=o>=</span> <span class=p>(</span><span class=n>img_size</span> <span class=o>*</span> <span class=n>img_size</span><span class=p>)</span> <span class=o>//</span> <span class=n>patch_size</span><span class=o>**</span><span class=mi>2</span>
</span><span id=__span-61-24><a id=__codelineno-61-24 name=__codelineno-61-24 href=#__codelineno-61-24></a>
</span><span id=__span-61-25><a id=__codelineno-61-25 name=__codelineno-61-25 href=#__codelineno-61-25></a>        <span class=c1># 5. 学習可能なクラス埋め込みを作成（パッチ埋め込みのシーケンスの前に配置する必要があります）</span>
</span><span id=__span-61-26><a id=__codelineno-61-26 name=__codelineno-61-26 href=#__codelineno-61-26></a>        <span class=bp>self</span><span class=o>.</span><span class=n>class_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>),</span>
</span><span id=__span-61-27><a id=__codelineno-61-27 name=__codelineno-61-27 href=#__codelineno-61-27></a>                                            <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-61-28><a id=__codelineno-61-28 name=__codelineno-61-28 href=#__codelineno-61-28></a>
</span><span id=__span-61-29><a id=__codelineno-61-29 name=__codelineno-61-29 href=#__codelineno-61-29></a>        <span class=c1># 6. 学習可能な位置埋め込みを作成</span>
</span><span id=__span-61-30><a id=__codelineno-61-30 name=__codelineno-61-30 href=#__codelineno-61-30></a>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_patches</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>),</span>
</span><span id=__span-61-31><a id=__codelineno-61-31 name=__codelineno-61-31 href=#__codelineno-61-31></a>                                               <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-61-32><a id=__codelineno-61-32 name=__codelineno-61-32 href=#__codelineno-61-32></a>
</span><span id=__span-61-33><a id=__codelineno-61-33 name=__codelineno-61-33 href=#__codelineno-61-33></a>        <span class=c1># 7. 埋め込みドロップアウト値を作成</span>
</span><span id=__span-61-34><a id=__codelineno-61-34 name=__codelineno-61-34 href=#__codelineno-61-34></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>embedding_dropout</span><span class=p>)</span>
</span><span id=__span-61-35><a id=__codelineno-61-35 name=__codelineno-61-35 href=#__codelineno-61-35></a>
</span><span id=__span-61-36><a id=__codelineno-61-36 name=__codelineno-61-36 href=#__codelineno-61-36></a>        <span class=c1># 8. パッチ埋め込み層を作成</span>
</span><span id=__span-61-37><a id=__codelineno-61-37 name=__codelineno-61-37 href=#__codelineno-61-37></a>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_embedding</span> <span class=o>=</span> <span class=n>PatchEmbedding</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=n>in_channels</span><span class=p>,</span>
</span><span id=__span-61-38><a id=__codelineno-61-38 name=__codelineno-61-38 href=#__codelineno-61-38></a>                                              <span class=n>patch_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span>
</span><span id=__span-61-39><a id=__codelineno-61-39 name=__codelineno-61-39 href=#__codelineno-61-39></a>                                              <span class=n>embedding_dim</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>)</span>
</span><span id=__span-61-40><a id=__codelineno-61-40 name=__codelineno-61-40 href=#__codelineno-61-40></a>
</span><span id=__span-61-41><a id=__codelineno-61-41 name=__codelineno-61-41 href=#__codelineno-61-41></a>        <span class=c1># 9. Transformer Encoderブロックを作成（nn.Sequential()を使用してTransformer Encoderブロックをスタック）</span>
</span><span id=__span-61-42><a id=__codelineno-61-42 name=__codelineno-61-42 href=#__codelineno-61-42></a>        <span class=c1># 注：&quot;*&quot;は&quot;すべて&quot;を意味します</span>
</span><span id=__span-61-43><a id=__codelineno-61-43 name=__codelineno-61-43 href=#__codelineno-61-43></a>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>TransformerEncoderBlock</span><span class=p>(</span><span class=n>embedding_dim</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-61-44><a id=__codelineno-61-44 name=__codelineno-61-44 href=#__codelineno-61-44></a>                                                                            <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
</span><span id=__span-61-45><a id=__codelineno-61-45 name=__codelineno-61-45 href=#__codelineno-61-45></a>                                                                            <span class=n>mlp_size</span><span class=o>=</span><span class=n>mlp_size</span><span class=p>,</span>
</span><span id=__span-61-46><a id=__codelineno-61-46 name=__codelineno-61-46 href=#__codelineno-61-46></a>                                                                            <span class=n>mlp_dropout</span><span class=o>=</span><span class=n>mlp_dropout</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_transformer_layers</span><span class=p>)])</span>
</span><span id=__span-61-47><a id=__codelineno-61-47 name=__codelineno-61-47 href=#__codelineno-61-47></a>
</span><span id=__span-61-48><a id=__codelineno-61-48 name=__codelineno-61-48 href=#__codelineno-61-48></a>        <span class=c1># 10. 分類器ヘッドを作成</span>
</span><span id=__span-61-49><a id=__codelineno-61-49 name=__codelineno-61-49 href=#__codelineno-61-49></a>        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-61-50><a id=__codelineno-61-50 name=__codelineno-61-50 href=#__codelineno-61-50></a>            <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>normalized_shape</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>),</span>
</span><span id=__span-61-51><a id=__codelineno-61-51 name=__codelineno-61-51 href=#__codelineno-61-51></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span>
</span><span id=__span-61-52><a id=__codelineno-61-52 name=__codelineno-61-52 href=#__codelineno-61-52></a>                      <span class=n>out_features</span><span class=o>=</span><span class=n>num_classes</span><span class=p>)</span>
</span><span id=__span-61-53><a id=__codelineno-61-53 name=__codelineno-61-53 href=#__codelineno-61-53></a>        <span class=p>)</span>
</span><span id=__span-61-54><a id=__codelineno-61-54 name=__codelineno-61-54 href=#__codelineno-61-54></a>
</span><span id=__span-61-55><a id=__codelineno-61-55 name=__codelineno-61-55 href=#__codelineno-61-55></a>    <span class=c1># 11. forward()メソッドを作成</span>
</span><span id=__span-61-56><a id=__codelineno-61-56 name=__codelineno-61-56 href=#__codelineno-61-56></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-61-57><a id=__codelineno-61-57 name=__codelineno-61-57 href=#__codelineno-61-57></a>        <span class=c1># 12. バッチサイズを取得</span>
</span><span id=__span-61-58><a id=__codelineno-61-58 name=__codelineno-61-58 href=#__codelineno-61-58></a>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span><span id=__span-61-59><a id=__codelineno-61-59 name=__codelineno-61-59 href=#__codelineno-61-59></a>
</span><span id=__span-61-60><a id=__codelineno-61-60 name=__codelineno-61-60 href=#__codelineno-61-60></a>        <span class=c1># 13. クラストークン埋め込みを作成してバッチサイズに合わせて拡張（数式1）</span>
</span><span id=__span-61-61><a id=__codelineno-61-61 name=__codelineno-61-61 href=#__codelineno-61-61></a>        <span class=n>class_token</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>class_embedding</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># &quot;-1&quot;は次元を推論することを意味</span>
</span><span id=__span-61-62><a id=__codelineno-61-62 name=__codelineno-61-62 href=#__codelineno-61-62></a>
</span><span id=__span-61-63><a id=__codelineno-61-63 name=__codelineno-61-63 href=#__codelineno-61-63></a>        <span class=c1># 14. パッチ埋め込みを作成（数式1）</span>
</span><span id=__span-61-64><a id=__codelineno-61-64 name=__codelineno-61-64 href=#__codelineno-61-64></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_embedding</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-61-65><a id=__codelineno-61-65 name=__codelineno-61-65 href=#__codelineno-61-65></a>
</span><span id=__span-61-66><a id=__codelineno-61-66 name=__codelineno-61-66 href=#__codelineno-61-66></a>        <span class=c1># 15. クラス埋め込みとパッチ埋め込みを連結（数式1）</span>
</span><span id=__span-61-67><a id=__codelineno-61-67 name=__codelineno-61-67 href=#__codelineno-61-67></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>class_token</span><span class=p>,</span> <span class=n>x</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-61-68><a id=__codelineno-61-68 name=__codelineno-61-68 href=#__codelineno-61-68></a>
</span><span id=__span-61-69><a id=__codelineno-61-69 name=__codelineno-61-69 href=#__codelineno-61-69></a>        <span class=c1># 16. 位置埋め込みをパッチ埋め込みに追加（数式1）</span>
</span><span id=__span-61-70><a id=__codelineno-61-70 name=__codelineno-61-70 href=#__codelineno-61-70></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span> <span class=o>+</span> <span class=n>x</span>
</span><span id=__span-61-71><a id=__codelineno-61-71 name=__codelineno-61-71 href=#__codelineno-61-71></a>
</span><span id=__span-61-72><a id=__codelineno-61-72 name=__codelineno-61-72 href=#__codelineno-61-72></a>        <span class=c1># 17. 埋め込みドロップアウトを実行（付録B.1）</span>
</span><span id=__span-61-73><a id=__codelineno-61-73 name=__codelineno-61-73 href=#__codelineno-61-73></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-61-74><a id=__codelineno-61-74 name=__codelineno-61-74 href=#__codelineno-61-74></a>
</span><span id=__span-61-75><a id=__codelineno-61-75 name=__codelineno-61-75 href=#__codelineno-61-75></a>        <span class=c1># 18. パッチ、位置、クラス埋め込みをtransformer encoderレイヤーに通す（数式2&amp;3）</span>
</span><span id=__span-61-76><a id=__codelineno-61-76 name=__codelineno-61-76 href=#__codelineno-61-76></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-61-77><a id=__codelineno-61-77 name=__codelineno-61-77 href=#__codelineno-61-77></a>
</span><span id=__span-61-78><a id=__codelineno-61-78 name=__codelineno-61-78 href=#__codelineno-61-78></a>        <span class=c1># 19. 0インデックスのロジットを分類器に通す（数式4）</span>
</span><span id=__span-61-79><a id=__codelineno-61-79 name=__codelineno-61-79 href=#__codelineno-61-79></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>x</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>])</span> <span class=c1># バッチの各サンプルで0インデックスで実行</span>
</span><span id=__span-61-80><a id=__codelineno-61-80 name=__codelineno-61-80 href=#__codelineno-61-80></a>
</span><span id=__span-61-81><a id=__codelineno-61-81 name=__codelineno-61-81 href=#__codelineno-61-81></a>        <span class=k>return</span> <span class=n>x</span>
</span></code></pre></div> <ol> <li>🕺💃🥳 やったー！！！vision transformerを構築しました！</li> </ol> <p>素晴らしい努力でした！ゆっくりと確実に層とブロック、入力と出力を作成し、それらをすべて組み合わせて独自のViTを構築しました！</p> <p>ViTクラスをテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-62-2><a id=__codelineno-62-2 name=__codelineno-62-2 href=#__codelineno-62-2></a>
</span><span id=__span-62-3><a id=__codelineno-62-3 name=__codelineno-62-3 href=#__codelineno-62-3></a><span class=c1># 単一画像と同じ形状のランダムテンソルを作成</span>
</span><span id=__span-62-4><a id=__codelineno-62-4 name=__codelineno-62-4 href=#__codelineno-62-4></a><span class=n>random_image_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>)</span> <span class=c1># (batch_size, color_channels, height, width)</span>
</span><span id=__span-62-5><a id=__codelineno-62-5 name=__codelineno-62-5 href=#__codelineno-62-5></a>
</span><span id=__span-62-6><a id=__codelineno-62-6 name=__codelineno-62-6 href=#__codelineno-62-6></a><span class=c1># 作業中のクラス数（pizza、steak、sushi）でViTのインスタンスを作成</span>
</span><span id=__span-62-7><a id=__codelineno-62-7 name=__codelineno-62-7 href=#__codelineno-62-7></a><span class=n>vit</span> <span class=o>=</span> <span class=n>ViT</span><span class=p>(</span><span class=n>num_classes</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>class_names</span><span class=p>))</span>
</span><span id=__span-62-8><a id=__codelineno-62-8 name=__codelineno-62-8 href=#__codelineno-62-8></a>
</span><span id=__span-62-9><a id=__codelineno-62-9 name=__codelineno-62-9 href=#__codelineno-62-9></a><span class=c1># ランダム画像テンソルをViTインスタンスに渡す</span>
</span><span id=__span-62-10><a id=__codelineno-62-10 name=__codelineno-62-10 href=#__codelineno-62-10></a><span class=n>vit</span><span class=p>(</span><span class=n>random_image_tensor</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>tensor([[0.2525, 0.8748, 1.2374]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p>優秀です！ランダム画像テンソルがViTアーキテクチャ全体を通り抜け、3つのロジット値（各クラス用に1つ）を出力しているようです。</p> <h2 id=9-vit>9. ViTモデルの訓練コードの設定<a class=headerlink href=#9-vit title="Permanent link">&para;</a></h2> <p>簡単な部分の時間です。訓練！</p> <p>なぜ簡単なのでしょうか？モデル（<code>vit</code>）からDataLoader（<code>train_dataloader</code>、<code>test_dataloader</code>）まで、必要なもののほとんどが準備できているからです。</p> <h3 id=91>9.1 オプティマイザーの作成<a class=headerlink href=#91 title="Permanent link">&para;</a></h3> <p>ViT論文で「optimizer」を検索すると、4.1節のTraining &amp; Fine-tuningで以下が述べられています：</p> <blockquote> <p>ResNetを含むすべてのモデルを、<span class=arithmatex>\(\beta_{1}=0.9, \beta_{2}=0.999\)</span>、バッチサイズ4096、高い重み減衰0.1でAdamを使用して訓練します。</p> </blockquote> <p>そこで、SGDではなく「Adam」オプティマイザー（<code>torch.optim.Adam()</code>）を選択することがわかります。</p> <h3 id=92>9.2 損失関数の作成<a class=headerlink href=#92 title="Permanent link">&para;</a></h3> <p>ViT論文で「loss」や「loss function」や「criterion」を検索しても結果が出ません。</p> <p>しかし、作業している目標問題が多クラス分類（ViT論文と同じ）なので、<code>torch.nn.CrossEntropyLoss()</code>を使用します。</p> <h3 id=93-vit>9.3 ViTモデルの訓練<a class=headerlink href=#93-vit title="Permanent link">&para;</a></h3> <p>オプティマイザーと損失関数を設定して、10エポック訓練しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular</span><span class=w> </span><span class=kn>import</span> <span class=n>engine</span>
</span><span id=__span-64-2><a id=__codelineno-64-2 name=__codelineno-64-2 href=#__codelineno-64-2></a>
</span><span id=__span-64-3><a id=__codelineno-64-3 name=__codelineno-64-3 href=#__codelineno-64-3></a><span class=c1># ViT論文のハイパーパラメータを使用してViTモデルパラメータを最適化するオプティマイザーを設定</span>
</span><span id=__span-64-4><a id=__codelineno-64-4 name=__codelineno-64-4 href=#__codelineno-64-4></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>params</span><span class=o>=</span><span class=n>vit</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span><span id=__span-64-5><a id=__codelineno-64-5 name=__codelineno-64-5 href=#__codelineno-64-5></a>                             <span class=n>lr</span><span class=o>=</span><span class=mf>3e-3</span><span class=p>,</span> <span class=c1># Table 3のViT-* ImageNet-1kのBase LR</span>
</span><span id=__span-64-6><a id=__codelineno-64-6 name=__codelineno-64-6 href=#__codelineno-64-6></a>                             <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>),</span> <span class=c1># デフォルト値、ViT論文4.1節にも記載</span>
</span><span id=__span-64-7><a id=__codelineno-64-7 name=__codelineno-64-7 href=#__codelineno-64-7></a>                             <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span> <span class=c1># ViT論文4.1節とTable 3のViT-* ImageNet-1kより</span>
</span><span id=__span-64-8><a id=__codelineno-64-8 name=__codelineno-64-8 href=#__codelineno-64-8></a>
</span><span id=__span-64-9><a id=__codelineno-64-9 name=__codelineno-64-9 href=#__codelineno-64-9></a><span class=c1># 多クラス分類用の損失関数を設定</span>
</span><span id=__span-64-10><a id=__codelineno-64-10 name=__codelineno-64-10 href=#__codelineno-64-10></a><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span><span id=__span-64-11><a id=__codelineno-64-11 name=__codelineno-64-11 href=#__codelineno-64-11></a>
</span><span id=__span-64-12><a id=__codelineno-64-12 name=__codelineno-64-12 href=#__codelineno-64-12></a><span class=c1># シードを設定</span>
</span><span id=__span-64-13><a id=__codelineno-64-13 name=__codelineno-64-13 href=#__codelineno-64-13></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-64-14><a id=__codelineno-64-14 name=__codelineno-64-14 href=#__codelineno-64-14></a>
</span><span id=__span-64-15><a id=__codelineno-64-15 name=__codelineno-64-15 href=#__codelineno-64-15></a><span class=c1># モデルを訓練し、訓練結果を辞書に保存</span>
</span><span id=__span-64-16><a id=__codelineno-64-16 name=__codelineno-64-16 href=#__codelineno-64-16></a><span class=n>results</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>vit</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span>
</span><span id=__span-64-17><a id=__codelineno-64-17 name=__codelineno-64-17 href=#__codelineno-64-17></a>                       <span class=n>train_dataloader</span><span class=o>=</span><span class=n>train_dataloader</span><span class=p>,</span>
</span><span id=__span-64-18><a id=__codelineno-64-18 name=__codelineno-64-18 href=#__codelineno-64-18></a>                       <span class=n>test_dataloader</span><span class=o>=</span><span class=n>test_dataloader</span><span class=p>,</span>
</span><span id=__span-64-19><a id=__codelineno-64-19 name=__codelineno-64-19 href=#__codelineno-64-19></a>                       <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-64-20><a id=__codelineno-64-20 name=__codelineno-64-20 href=#__codelineno-64-20></a>                       <span class=n>loss_fn</span><span class=o>=</span><span class=n>loss_fn</span><span class=p>,</span>
</span><span id=__span-64-21><a id=__codelineno-64-21 name=__codelineno-64-21 href=#__codelineno-64-21></a>                       <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span><span id=__span-64-22><a id=__codelineno-64-22 name=__codelineno-64-22 href=#__codelineno-64-22></a>                       <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span>
</span><span id=__span-64-23><a id=__codelineno-64-23 name=__codelineno-64-23 href=#__codelineno-64-23></a>                       <span class=n>writer</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>Epoch: 1 | train_loss: 5.4041 | train_acc: 0.3633 | test_loss: 5.1442 | test_acc: 0.2604
</span><span id=__span-65-2><a id=__codelineno-65-2 name=__codelineno-65-2 href=#__codelineno-65-2></a>Epoch: 2 | train_loss: 1.8898 | train_acc: 0.3359 | test_loss: 2.0569 | test_acc: 0.2604
</span><span id=__span-65-3><a id=__codelineno-65-3 name=__codelineno-65-3 href=#__codelineno-65-3></a>...
</span><span id=__span-65-4><a id=__codelineno-65-4 name=__codelineno-65-4 href=#__codelineno-65-4></a>Epoch: 10 | train_loss: 1.2541 | train_acc: 0.2617 | test_loss: 1.3891 | test_acc: 0.2604
</span></code></pre></div></p> <p>素晴らしい！ViTモデルが生き返りました！ただし、ピザ、ステーキ、寿司データセットでの結果はあまり良くないようです。</p> <h3 id=94>9.4 訓練設定で不足しているもの<a class=headerlink href=#94 title="Permanent link">&para;</a></h3> <p>元のViTアーキテクチャは、いくつかの画像分類ベンチマークで良い結果を達成します。しかし、私たちの結果（これまでのところ）はそれほど良くありません。</p> <p>これにはいくつかの理由が考えられますが、主なものは<strong>スケール</strong>です。</p> <table> <thead> <tr> <th><strong>ハイパーパラメータ値</strong></th> <th><strong>ViT論文</strong></th> <th><strong>私たちの実装</strong></th> </tr> </thead> <tbody> <tr> <td>訓練画像数</td> <td>1.3M (ImageNet-1k), 14M (ImageNet-21k), 303M (JFT)</td> <td>225</td> </tr> <tr> <td>エポック数</td> <td>7（最大データセット用）、90、300（ImageNet用）</td> <td>10</td> </tr> <tr> <td>バッチサイズ</td> <td>4096</td> <td>32</td> </tr> </tbody> </table> <p>ViTアーキテクチャは論文と同じですが、ViT論文の結果は、はるかに多くのデータとより精巧な訓練スキームを使用して達成されました。</p> <h2 id=10-torchvisionmodelsvit>10. <code>torchvision.models</code>の事前訓練済みViTを同じデータセットで使用<a class=headerlink href=#10-torchvisionmodelsvit title="Permanent link">&para;</a></h2> <p>事前訓練済みモデルの利点について議論しました。自分でViTをゼロから訓練して最適でない結果を得たので、転移学習の利点が本当に輝きます。</p> <h3 id=101>10.1 なぜ事前訓練済みモデルを使用するのか？<a class=headerlink href=#101 title="Permanent link">&para;</a></h3> <p>多くの現代機械学習研究論文で重要なことは、結果の多くが大規模データセットと膨大な計算リソースで得られることです。</p> <p>ViT論文の4.2節を読むと：</p> <blockquote> <p>最後に、パブリックImageNet-21kデータセットで事前訓練されたViT-L/16モデルは、ほとんどのデータセットでも良好に機能し、事前訓練により少ないリソースを取ります：標準的なクラウドTPUv3の8コアで<strong>約30日間</strong>で訓練できます。</p> </blockquote> <p>2022年7月現在、Google Cloudで8コアの<a href=https://cloud.google.com/tpu/pricing>TPUv3をレンタルする価格</a>は、1時間あたり8米ドルです。</p> <p>30日間連続でレンタルすると<strong>5,760米ドル</strong>の費用がかかります。</p> <h3 id=102-vit>10.2 事前訓練済みViTモデルの取得と特徴抽出器の作成<a class=headerlink href=#102-vit title="Permanent link">&para;</a></h3> <p><code>torchvision.models</code>から事前訓練済みViT-Baseを取得し、FoodVision Miniユースケース用に特徴抽出器転移学習モデルに変換します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=c1># 1. ViT-Baseの事前訓練済み重みを取得</span>
</span><span id=__span-66-2><a id=__codelineno-66-2 name=__codelineno-66-2 href=#__codelineno-66-2></a><span class=n>pretrained_vit_weights</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>ViT_B_16_Weights</span><span class=o>.</span><span class=n>DEFAULT</span> <span class=c1># torchvision &gt;= 0.13が必要</span>
</span><span id=__span-66-3><a id=__codelineno-66-3 name=__codelineno-66-3 href=#__codelineno-66-3></a>
</span><span id=__span-66-4><a id=__codelineno-66-4 name=__codelineno-66-4 href=#__codelineno-66-4></a><span class=c1># 2. 事前訓練済み重みでViTモデルインスタンスを設定</span>
</span><span id=__span-66-5><a id=__codelineno-66-5 name=__codelineno-66-5 href=#__codelineno-66-5></a><span class=n>pretrained_vit</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>vit_b_16</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=n>pretrained_vit_weights</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-66-6><a id=__codelineno-66-6 name=__codelineno-66-6 href=#__codelineno-66-6></a>
</span><span id=__span-66-7><a id=__codelineno-66-7 name=__codelineno-66-7 href=#__codelineno-66-7></a><span class=c1># 3. ベースパラメータをフリーズ</span>
</span><span id=__span-66-8><a id=__codelineno-66-8 name=__codelineno-66-8 href=#__codelineno-66-8></a><span class=k>for</span> <span class=n>parameter</span> <span class=ow>in</span> <span class=n>pretrained_vit</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span><span id=__span-66-9><a id=__codelineno-66-9 name=__codelineno-66-9 href=#__codelineno-66-9></a>    <span class=n>parameter</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-66-10><a id=__codelineno-66-10 name=__codelineno-66-10 href=#__codelineno-66-10></a>
</span><span id=__span-66-11><a id=__codelineno-66-11 name=__codelineno-66-11 href=#__codelineno-66-11></a><span class=c1># 4. 分類器ヘッドを変更（線形ヘッドで同じ初期化を保証するためシードを設定）</span>
</span><span id=__span-66-12><a id=__codelineno-66-12 name=__codelineno-66-12 href=#__codelineno-66-12></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-66-13><a id=__codelineno-66-13 name=__codelineno-66-13 href=#__codelineno-66-13></a><span class=n>pretrained_vit</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>class_names</span><span class=p>))</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></code></pre></div> <p>事前訓練済みViT特徴抽出器モデルが作成されました！</p> <h3 id=103-vit>10.3 事前訓練済みViTモデル用のデータ準備<a class=headerlink href=#103-vit title="Permanent link">&para;</a></h3> <p>事前訓練済みモデルのために画像をDataLoaderに変換しましょう。</p> <p><code>torchvision.models</code>の事前訓練済みモデルを使用する場合、そのモデルでrequiredトランスフォームを取得するために<code>transforms()</code>メソッドを呼び出すことができます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a><span class=c1># 事前訓練済みViT重みから自動変換を取得</span>
</span><span id=__span-67-2><a id=__codelineno-67-2 name=__codelineno-67-2 href=#__codelineno-67-2></a><span class=n>pretrained_vit_transforms</span> <span class=o>=</span> <span class=n>pretrained_vit_weights</span><span class=o>.</span><span class=n>transforms</span><span class=p>()</span>
</span><span id=__span-67-3><a id=__codelineno-67-3 name=__codelineno-67-3 href=#__codelineno-67-3></a><span class=nb>print</span><span class=p>(</span><span class=n>pretrained_vit_transforms</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a>ImageClassification(
</span><span id=__span-68-2><a id=__codelineno-68-2 name=__codelineno-68-2 href=#__codelineno-68-2></a>    crop_size=[224]
</span><span id=__span-68-3><a id=__codelineno-68-3 name=__codelineno-68-3 href=#__codelineno-68-3></a>    resize_size=[256]
</span><span id=__span-68-4><a id=__codelineno-68-4 name=__codelineno-68-4 href=#__codelineno-68-4></a>    mean=[0.485, 0.456, 0.406]
</span><span id=__span-68-5><a id=__codelineno-68-5 name=__codelineno-68-5 href=#__codelineno-68-5></a>    std=[0.229, 0.224, 0.225]
</span><span id=__span-68-6><a id=__codelineno-68-6 name=__codelineno-68-6 href=#__codelineno-68-6></a>    interpolation=InterpolationMode.BILINEAR
</span><span id=__span-68-7><a id=__codelineno-68-7 name=__codelineno-68-7 href=#__codelineno-68-7></a>)
</span></code></pre></div></p> <p>DataLoaderを設定します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a><span class=c1># データローダーを設定</span>
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a><span class=n>train_dataloader_pretrained</span><span class=p>,</span> <span class=n>test_dataloader_pretrained</span><span class=p>,</span> <span class=n>class_names</span> <span class=o>=</span> <span class=n>data_setup</span><span class=o>.</span><span class=n>create_dataloaders</span><span class=p>(</span>
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a>    <span class=n>train_dir</span><span class=o>=</span><span class=n>train_dir</span><span class=p>,</span>
</span><span id=__span-69-4><a id=__codelineno-69-4 name=__codelineno-69-4 href=#__codelineno-69-4></a>    <span class=n>test_dir</span><span class=o>=</span><span class=n>test_dir</span><span class=p>,</span>
</span><span id=__span-69-5><a id=__codelineno-69-5 name=__codelineno-69-5 href=#__codelineno-69-5></a>    <span class=n>transform</span><span class=o>=</span><span class=n>pretrained_vit_transforms</span><span class=p>,</span>
</span><span id=__span-69-6><a id=__codelineno-69-6 name=__codelineno-69-6 href=#__codelineno-69-6></a>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span>
</span><span id=__span-69-7><a id=__codelineno-69-7 name=__codelineno-69-7 href=#__codelineno-69-7></a><span class=p>)</span>
</span></code></pre></div> <h3 id=104-vit>10.4 特徴抽出器ViTモデルの訓練<a class=headerlink href=#104-vit title="Permanent link">&para;</a></h3> <p>特徴抽出器モデルを訓練しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular</span><span class=w> </span><span class=kn>import</span> <span class=n>engine</span>
</span><span id=__span-70-2><a id=__codelineno-70-2 name=__codelineno-70-2 href=#__codelineno-70-2></a>
</span><span id=__span-70-3><a id=__codelineno-70-3 name=__codelineno-70-3 href=#__codelineno-70-3></a><span class=c1># オプティマイザーと損失関数を作成</span>
</span><span id=__span-70-4><a id=__codelineno-70-4 name=__codelineno-70-4 href=#__codelineno-70-4></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>params</span><span class=o>=</span><span class=n>pretrained_vit</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span><span id=__span-70-5><a id=__codelineno-70-5 name=__codelineno-70-5 href=#__codelineno-70-5></a>                             <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span><span id=__span-70-6><a id=__codelineno-70-6 name=__codelineno-70-6 href=#__codelineno-70-6></a><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span><span id=__span-70-7><a id=__codelineno-70-7 name=__codelineno-70-7 href=#__codelineno-70-7></a>
</span><span id=__span-70-8><a id=__codelineno-70-8 name=__codelineno-70-8 href=#__codelineno-70-8></a><span class=c1># 事前訓練済みViT特徴抽出器モデルの分類器ヘッドを訓練</span>
</span><span id=__span-70-9><a id=__codelineno-70-9 name=__codelineno-70-9 href=#__codelineno-70-9></a><span class=n>set_seeds</span><span class=p>()</span>
</span><span id=__span-70-10><a id=__codelineno-70-10 name=__codelineno-70-10 href=#__codelineno-70-10></a><span class=n>pretrained_vit_results</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>pretrained_vit</span><span class=p>,</span>
</span><span id=__span-70-11><a id=__codelineno-70-11 name=__codelineno-70-11 href=#__codelineno-70-11></a>                                      <span class=n>train_dataloader</span><span class=o>=</span><span class=n>train_dataloader_pretrained</span><span class=p>,</span>
</span><span id=__span-70-12><a id=__codelineno-70-12 name=__codelineno-70-12 href=#__codelineno-70-12></a>                                      <span class=n>test_dataloader</span><span class=o>=</span><span class=n>test_dataloader_pretrained</span><span class=p>,</span>
</span><span id=__span-70-13><a id=__codelineno-70-13 name=__codelineno-70-13 href=#__codelineno-70-13></a>                                      <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-70-14><a id=__codelineno-70-14 name=__codelineno-70-14 href=#__codelineno-70-14></a>                                      <span class=n>loss_fn</span><span class=o>=</span><span class=n>loss_fn</span><span class=p>,</span>
</span><span id=__span-70-15><a id=__codelineno-70-15 name=__codelineno-70-15 href=#__codelineno-70-15></a>                                      <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span><span id=__span-70-16><a id=__codelineno-70-16 name=__codelineno-70-16 href=#__codelineno-70-16></a>                                      <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span>
</span><span id=__span-70-17><a id=__codelineno-70-17 name=__codelineno-70-17 href=#__codelineno-70-17></a>                                      <span class=n>writer</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a>Epoch: 1 | train_loss: 0.7665 | train_acc: 0.7227 | test_loss: 0.5432 | test_acc: 0.8665
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a>Epoch: 2 | train_loss: 0.3428 | train_acc: 0.9453 | test_loss: 0.3263 | test_acc: 0.8977
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a>...
</span><span id=__span-71-4><a id=__codelineno-71-4 name=__codelineno-71-4 href=#__codelineno-71-4></a>Epoch: 10 | train_loss: 0.0650 | train_acc: 0.9883 | test_loss: 0.1804 | test_acc: 0.9176
</span></code></pre></div></p> <p>すごい！事前訓練済みViT特徴抽出器は、同じ時間でゼロから訓練したカスタムViTモデルよりもはるかに良い性能を示しました。</p> <h3 id=105-vit>10.5 特徴抽出器ViTモデルの損失曲線をプロット<a class=headerlink href=#105-vit title="Permanent link">&para;</a></h3> <p>損失曲線を確認しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=c1># 損失曲線をプロット</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a><span class=kn>from</span><span class=w> </span><span class=nn>helper_functions</span><span class=w> </span><span class=kn>import</span> <span class=n>plot_loss_curves</span>
</span><span id=__span-72-3><a id=__codelineno-72-3 name=__codelineno-72-3 href=#__codelineno-72-3></a>
</span><span id=__span-72-4><a id=__codelineno-72-4 name=__codelineno-72-4 href=#__codelineno-72-4></a><span class=n>plot_loss_curves</span><span class=p>(</span><span class=n>pretrained_vit_results</span><span class=p>)</span>
</span></code></pre></div> <p><img alt=損失曲線 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_165_0.png></p> <p>わあ！これらは教科書に近い（本当に良い）損失曲線です！</p> <p>それが転移学習の力です！<strong>同じ</strong>モデルアーキテクチャで優れた結果を得ることができました。ただし、カスタム実装はゼロから訓練され（性能が悪い）、この特徴抽出器モデルはImageNetからの事前訓練済み重みの力を持っています。</p> <h3 id=106-vit>10.6 特徴抽出器ViTモデルの保存とファイルサイズの確認<a class=headerlink href=#106-vit title="Permanent link">&para;</a></h3> <p>ViT特徴抽出器モデルがFood Vision Mini問題で非常に良い性能を示しているようです。</p> <p>おそらく、本番環境にデプロイしてみたいと思うでしょう。まず、モデルを保存しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a><span class=c1># モデルを保存</span>
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular</span><span class=w> </span><span class=kn>import</span> <span class=n>utils</span>
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a>
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a><span class=n>utils</span><span class=o>.</span><span class=n>save_model</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>pretrained_vit</span><span class=p>,</span>
</span><span id=__span-73-5><a id=__codelineno-73-5 name=__codelineno-73-5 href=#__codelineno-73-5></a>                 <span class=n>target_dir</span><span class=o>=</span><span class=s2>&quot;models&quot;</span><span class=p>,</span>
</span><span id=__span-73-6><a id=__codelineno-73-6 name=__codelineno-73-6 href=#__codelineno-73-6></a>                 <span class=n>model_name</span><span class=o>=</span><span class=s2>&quot;09_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a>[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi.pth
</span></code></pre></div></p> <p>モデルのサイズを確認しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a><span class=kn>from</span><span class=w> </span><span class=nn>pathlib</span><span class=w> </span><span class=kn>import</span> <span class=n>Path</span>
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a>
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a><span class=c1># モデルサイズをバイトで取得してメガバイトに変換</span>
</span><span id=__span-75-4><a id=__codelineno-75-4 name=__codelineno-75-4 href=#__codelineno-75-4></a><span class=n>pretrained_vit_model_size</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&quot;models/09_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>stat</span><span class=p>()</span><span class=o>.</span><span class=n>st_size</span> <span class=o>//</span> <span class=p>(</span><span class=mi>1024</span><span class=o>*</span><span class=mi>1024</span><span class=p>)</span>
</span><span id=__span-75-5><a id=__codelineno-75-5 name=__codelineno-75-5 href=#__codelineno-75-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;事前訓練済みViT特徴抽出器モデルサイズ: </span><span class=si>{</span><span class=n>pretrained_vit_model_size</span><span class=si>}</span><span class=s2> MB&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a>事前訓練済みViT特徴抽出器モデルサイズ: 327 MB
</span></code></pre></div></p> <p>ViT特徴抽出器モデルは約327 MBのサイズになりました。</p> <p>以前のEffNetB2特徴抽出器モデルと比較してみましょう：</p> <table> <thead> <tr> <th><strong>モデル</strong></th> <th><strong>モデルサイズ (MB)</strong></th> <th><strong>テスト損失</strong></th> <th><strong>テスト精度</strong></th> </tr> </thead> <tbody> <tr> <td>EffNetB2特徴抽出器^</td> <td>29</td> <td>~0.3906</td> <td>~0.9384</td> </tr> <tr> <td>ViT特徴抽出器</td> <td>327</td> <td>~0.1084</td> <td>~0.9384</td> </tr> </tbody> </table> <p>EffNetB2モデルは、テスト損失と精度で類似した結果でViTモデルより約11倍小さいです。</p> <p>しかし、デプロイメントの観点から、これら2つのモデルを比較する場合、ViTモデルの追加精度が約11倍のモデルサイズ増加に値するかどうかを検討する必要があります。</p> <h2 id=11>11. カスタム画像で予測を行う<a class=headerlink href=#11 title="Permanent link">&para;</a></h2> <p>最後に、究極のテスト、独自のカスタムデータで予測することで終了しましょう。</p> <p>寿司の画像をダウンロードして、ViT特徴抽出器を使用して予測してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a><span class=c1># 画像で予測を行い、プロットする関数をインポート</span>
</span><span id=__span-77-2><a id=__codelineno-77-2 name=__codelineno-77-2 href=#__codelineno-77-2></a><span class=kn>from</span><span class=w> </span><span class=nn>going_modular.predictions</span><span class=w> </span><span class=kn>import</span> <span class=n>pred_and_plot_image</span>
</span><span id=__span-77-3><a id=__codelineno-77-3 name=__codelineno-77-3 href=#__codelineno-77-3></a>
</span><span id=__span-77-4><a id=__codelineno-77-4 name=__codelineno-77-4 href=#__codelineno-77-4></a><span class=c1># カスタム画像パスを設定</span>
</span><span id=__span-77-5><a id=__codelineno-77-5 name=__codelineno-77-5 href=#__codelineno-77-5></a><span class=n>custom_image_path</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&quot;data/sushi.jpg&quot;</span><span class=p>)</span>
</span><span id=__span-77-6><a id=__codelineno-77-6 name=__codelineno-77-6 href=#__codelineno-77-6></a>
</span><span id=__span-77-7><a id=__codelineno-77-7 name=__codelineno-77-7 href=#__codelineno-77-7></a><span class=c1># カスタム画像で予測</span>
</span><span id=__span-77-8><a id=__codelineno-77-8 name=__codelineno-77-8 href=#__codelineno-77-8></a><span class=n>pred_and_plot_image</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>pretrained_vit</span><span class=p>,</span>
</span><span id=__span-77-9><a id=__codelineno-77-9 name=__codelineno-77-9 href=#__codelineno-77-9></a>                    <span class=n>image_path</span><span class=o>=</span><span class=n>custom_image_path</span><span class=p>,</span>
</span><span id=__span-77-10><a id=__codelineno-77-10 name=__codelineno-77-10 href=#__codelineno-77-10></a>                    <span class=n>class_names</span><span class=o>=</span><span class=n>class_names</span><span class=p>)</span>
</span></code></pre></div> <p><img alt=予測結果 src=../09_pytorch_paper_replicating_files/09_pytorch_paper_replicating_173_0.png></p> <p><strong>実行結果:</strong> 画像が正しく「sushi」として分類されました！</p> <p>2つの親指を上げて！おめでとうございます！</p> <p>研究論文から独自のカスタム画像で使用可能なモデルコードまで、すべての道のりを歩んできました！</p> <h2 id=_6>主な要点<a class=headerlink href=#_6 title="Permanent link">&para;</a></h2> <ul> <li> <p>機械学習の爆発的普及により、進歩を詳述する新しい研究論文が毎日出てきます。<em>すべて</em>に追いつくことは不可能ですが、ここで行ったように、FoodVision Miniのためのコンピュータビジョン論文を複製するなど、独自のユースケースに絞り込むことができます。</p> </li> <li> <p>機械学習研究論文には、スマートな人々のチームによる数か月の研究が数ページに圧縮されていることが多いです（そのため、すべての詳細を抽出し、論文を完全に複製することは少し挑戦的になる可能性があります）。</p> </li> <li> <p>論文複製の目標は、機械学習研究論文（テキストと数学）を使用可能なコードに変換することです。</p> </li> <li> <p>機械学習研究論文を入力と出力（各層/ブロック/モデルに何が入って何が出るか？）と層（各層は入力をどのように操作するか？）とブロック（層の集合）に分解し、各部分をステップごとに複製することは、理解に非常に役立ちます。</p> </li> <li> <p>多くの最新モデルアーキテクチャで事前訓練済みモデルが利用可能で、転移学習の力により、これらは少ないデータで<em>非常に</em>良い性能を発揮することが多いです。</p> </li> <li> <p>より大きなモデルは一般的により良い性能を発揮しますが、より大きなフットプリントも持ちます（より多くのストレージスペースを占有し、推論により長い時間がかかる可能性があります）。</p> </li> </ul> <h2 id=_7>まとめ<a class=headerlink href=#_7 title="Permanent link">&para;</a></h2> <p>本記事では、Vision Transformerの完全な実装を通じて、機械学習論文の再現実装について包括的に学習しました。</p> <h3 id=_8><strong>実装で達成した成果：</strong><a class=headerlink href=#_8 title="Permanent link">&para;</a></h3> <ol> <li><strong>Transformer Encoderの構築</strong> - MSAブロックとMLPブロックを組み合わせた完全なEncoder</li> <li><strong>完全なViTアーキテクチャの実装</strong> - 数式1〜4をすべて含む実用的なViTモデル</li> <li><strong>転移学習の実装</strong> - 事前訓練済みモデルを活用した効率的な学習</li> <li><strong>実用的なシステムの構築</strong> - カスタム画像で予測可能な完全なシステム</li> </ol> <h3 id=_9><strong>学習ポイント：</strong><a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <ol> <li><strong>論文実装のスキル</strong> - 研究論文の数式をPyTorchコードに変換する能力</li> <li><strong>アーキテクチャ設計</strong> - レゴブロックのように層を組み合わせてモデルを構築する手法</li> <li><strong>転移学習の重要性</strong> - 事前訓練済みモデルの圧倒的な利点の理解</li> <li><strong>実用性の考慮</strong> - モデルサイズと性能のトレードオフの重要性</li> </ol> <h3 id=_10><strong>技術的達成:</strong><a class=headerlink href=#_10 title="Permanent link">&para;</a></h3> <ul> <li>ゼロからのViT実装（85,800,963パラメータ）</li> <li>転移学習による高精度モデル（test_acc: 0.9176）</li> <li>実際の画像に対する予測システムの構築</li> <li>モデルの保存とデプロイメント準備</li> </ul> <h2 id=_11>追加学習リソース<a class=headerlink href=#_11 title="Permanent link">&para;</a></h2> <ul> <li><a href=https://arxiv.org/abs/2205.01580><em>Better plain ViT baselines for ImageNet-1k</em></a> - ViTの改良版について</li> <li><a href=https://github.com/lucidrains/vit-pytorch>lucidrains/vit-pytorch</a> - 様々なViTアーキテクチャの実装集</li> <li><a href=https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>PyTorchのViT実装</a> - 公式実装</li> <li><a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ >Jay Alammarのアテンション機構解説</a></li> </ul> <h2 id=_12>注意事項<a class=headerlink href=#_12 title="Permanent link">&para;</a></h2> <ul> <li><strong>計算リソース</strong>: ViTは大きなモデルであり、十分なGPUメモリが必要です</li> <li><strong>データ要件</strong>: 最適な性能には大量のデータが必要ですが、転移学習で軽減可能</li> <li><strong>デプロイメント</strong>: モデルサイズと推論速度のバランスを考慮する必要があります</li> </ul> <p>Vision Transformerの実装を通じて、現代的な深層学習アーキテクチャの理解と実装スキルを大幅に向上させることができました。これらの知識は、他の最新アーキテクチャの理解と実装にも応用できるでしょう。</p> <h2 id=_13>参考文献<a class=headerlink href=#_13 title="Permanent link">&para;</a></h2> <ul> <li><a href=https://arxiv.org/abs/2010.11929>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li> <li><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need</a></li> <li><a href=https://arxiv.org/abs/1512.03385v1>Deep Residual Learning for Image Recognition</a></li> <li><a href=https://pytorch.org/docs/stable/index.html>PyTorch Documentation</a></li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/meta.js></script> <script src=../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>