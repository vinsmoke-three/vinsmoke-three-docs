<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Hugging Face Transformersライブラリを使用して、事前訓練済みモデルをファインチューニングする方法を学びます。TrainerAPIとカスタム訓練ループの両方を解説します。"><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/LLM/03_fine_tuning_a_pretrained_model/ rel=canonical><link href=../02_using_transformers/ rel=prev><link href=../04_the_huggingface_tokenizers_library/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>事前訓練済みモデルのファインチューニング - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 事前訓練済みモデルのファインチューニング </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PyTorch/00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../PyTorch/00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/07_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/09_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class=md-nav__item> <a href=../02_using_transformers/ class=md-nav__link> <span class=md-ellipsis> 2. Using transformers </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> データの処理 </span> </a> </li> <li class=md-nav__item> <a href=#trainer-api class=md-nav__link> <span class=md-ellipsis> Trainer APIを使用したモデルのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 完全な訓練ループ </span> </a> </li> <li class=md-nav__item> <a href=#_16 class=md-nav__link> <span class=md-ellipsis> 学習曲線の理解 </span> </a> </li> <li class=md-nav__item> <a href=#_30 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_31 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../04_the_huggingface_tokenizers_library/ class=md-nav__link> <span class=md-ellipsis> 4. Tokenizers library </span> </a> </li> <li class=md-nav__item> <a href=../05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ClassicalNLP/71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/72_masked_language_modeling/ class=md-nav__link> <span class=md-ellipsis> Masked Language Modeling </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 学習目標 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> データの処理 </span> </a> </li> <li class=md-nav__item> <a href=#trainer-api class=md-nav__link> <span class=md-ellipsis> Trainer APIを使用したモデルのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 完全な訓練ループ </span> </a> </li> <li class=md-nav__item> <a href=#_16 class=md-nav__link> <span class=md-ellipsis> 学習曲線の理解 </span> </a> </li> <li class=md-nav__item> <a href=#_30 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_31 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=_1>事前訓練済みモデルのファインチューニング<a class=headerlink href=#_1 title="Permanent link">&para;</a></h1> <h2 id=_2>概要<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <p>この記事では、Hugging Face Transformersエコシステムを使用して事前訓練済みモデルをファインチューニングする方法を学習します。モダンな機械学習の最良実践を適用しながら、高レベルなTrainer APIから低レベルなカスタム訓練ループまでを包括的に解説します。</p> <div class="admonition info"> <p class=admonition-title>参考資料</p> <p>本ドキュメントは <a href=https://huggingface.co/learn/llm-course/chapter3/1>Hugging Face LLM Course</a> を参考に、日本語で学習内容をまとめた個人的な学習ノートです。詳細な内容や最新情報については、原文も併せてご参照ください。</p> </div> <h2 id=_3>学習目標<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <p>この記事を通じて、以下のスキルを習得できます：</p> <ul> <li>Hugging Face Hubからの大規模データセットの準備と処理</li> <li>高レベルな<code>Trainer</code> APIを使用したモダンなベストプラクティスでのモデルファインチューニング</li> <li>最適化テクニックを含むカスタム訓練ループの実装</li> <li>Accelerateライブラリを活用した分散訓練の実行</li> <li>最大パフォーマンスのための最新ファインチューニング技術の適用</li> </ul> <h2 id=_4>前提知識<a class=headerlink href=#_4 title="Permanent link">&para;</a></h2> <ul> <li>Python基礎知識</li> <li>PyTorchの基本的な理解</li> <li>機械学習の基本概念（損失関数、最適化、評価指標）</li> <li>Transformersモデルの基本理解</li> </ul> <div class="admonition tip"> <p class=admonition-title>豆知識</p> <p>開始前に、データ処理について<a href=https://huggingface.co/docs/datasets/ >Datasetsドキュメント</a>を確認することをお勧めします。</p> </div> <p>この記事では、Transformersライブラリ以外のHugging Faceライブラリも紹介します。Datasets、Tokenizers、Accelerate、Evaluateライブラリがどのようにモデル訓練をより効率的かつ効果的にするかを学びます。</p> <p>各主要セクションでは異なる内容を学習します：</p> <ul> <li><strong>セクション2</strong>: モダンなデータ前処理技術と効率的なデータセット処理</li> <li><strong>セクション3</strong>: 最新機能を含む強力なTrainer APIの習得</li> <li><strong>セクション4</strong>: 一から訓練ループを実装し、Accelerateを使用した分散訓練の理解</li> </ul> <p>この記事を完了すると、高レベルAPIとカスタム訓練ループの両方を使用して、独自のデータセットでモデルをファインチューニングできるようになり、この分野の最新ベストプラクティスを適用できるようになります。</p> <p>この記事では<strong>PyTorch</strong>に特化して説明します。PyTorchはモダンな深層学習研究と本番環境の標準フレームワークとなっているためです。Hugging Faceエコシステムの最新APIとベストプラクティスを使用します。</p> <h2 id=_5>データの処理<a class=headerlink href=#_5 title="Permanent link">&para;</a></h2> <p><a href=../02_using_transformers/ >前章</a>の例を続けて、1つのバッチでシーケンス分類器を訓練する方法を示します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=kn>import</span> <span class=n>AdamW</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;bert-base-uncased&quot;</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=n>sequence</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>    <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>    <span class=s2>&quot;This course is amazing!&quot;</span><span class=p>,</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=p>]</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=n>batch</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequence</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=n>batch</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a><span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span><span class=o>.</span><span class=n>loss</span>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</span></code></pre></div></p> <p>当然ながら、2つの文だけでモデルを訓練しても良い結果は得られません。より良い結果を得るためには、より大きなデータセットを準備する必要があります。</p> <p>このセクションでは、William B. DolanとChris Brockettによる<a href=https://www.aclweb.org/anthology/I05-5002.pdf>論文</a>で紹介されたMRPC（Microsoft Research Paraphrase Corpus）データセットを例として使用します。このデータセットは5,801組の文ペアで構成され、それらが言い換え（パラフレーズ）かどうか（つまり、両方の文が同じ意味かどうか）を示すラベルが付いています。この章では小さなデータセットであるため、訓練実験が容易だという理由で選択しました。</p> <h3 id=hub>Hubからのデータセット読み込み<a class=headerlink href=#hub title="Permanent link">&para;</a></h3> <p>Hubにはモデルだけでなく、さまざまな言語の複数のデータセットも含まれています。データセットは<a href=https://huggingface.co/datasets>こちら</a>で閲覧でき、このセクションを完了した後に新しいデータセットの読み込みと処理を試すことをお勧めします（一般的なドキュメントは<a href=https://huggingface.co/docs/datasets/loading>こちら</a>）。今はMRPCデータセットに焦点を当てましょう！これは<a href=https://gluebenchmark.com/ >GLUEベンチマーク</a>を構成する10個のデータセットの1つで、10の異なるテキスト分類タスクでMLモデルのパフォーマンスを測定するために使用される学術ベンチマークです。</p> <p>Hugging Face Datasetsライブラリは、Hubのデータセットをダウンロードしてキャッシュする非常にシンプルなコマンドを提供します。MRPCデータセットは次のようにダウンロードできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>raw_datasets</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>raw_datasets</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>DatasetDict({
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    train: Dataset({
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;],
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        num_rows: 3668
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>    })
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    validation: Dataset({
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;],
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>        num_rows: 408
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    })
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    test: Dataset({
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;],
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>        num_rows: 1725
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>    })
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>})
</span></code></pre></div></p> <p>ご覧のとおり、訓練セット、検証セット、テストセットを含む<code>DatasetDict</code>オブジェクトが得られます。それぞれには複数の列（<code>sentence1</code>、<code>sentence2</code>、<code>label</code>、<code>idx</code>）と可変数の行があり、これは各セットの要素数です（つまり、訓練セットには3,668組の文ペア、検証セットには408組、テストセットには1,725組があります）。</p> <div class="admonition tip"> <p class=admonition-title>豆知識</p> <p>このコマンドはデータセットをダウンロードしてキャッシュし、デフォルトでは<em>~/.cache/huggingface/datasets</em>に保存されます。第2章で学んだように、<code>HF_HOME</code>環境変数を設定してキャッシュフォルダをカスタマイズできます。</p> </div> <p>辞書のようにインデックスを使用して、<code>raw_datasets</code>オブジェクトの各文ペアにアクセスできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>raw_train_dataset</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>raw_train_dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>{&#39;sentence1&#39;: &#39;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;,
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a> &#39;sentence2&#39;: &#39;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#39;,
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a> &#39;label&#39;: 1,
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a> &#39;idx&#39;: 0}
</span></code></pre></div></p> <p>ラベルはすでに整数になっているので、前処理は不要です。どの整数がどのラベルに対応するかを知るには、<code>raw_train_dataset</code>の<code>features</code>を調べます。これにより各列のタイプがわかります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=n>raw_train_dataset</span><span class=o>.</span><span class=n>features</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>{&#39;sentence1&#39;: Value(&#39;string&#39;),
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a> &#39;sentence2&#39;: Value(&#39;string&#39;),
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a> &#39;label&#39;: ClassLabel(names=[&#39;not_equivalent&#39;, &#39;equivalent&#39;]),
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a> &#39;idx&#39;: Value(&#39;int32&#39;)}
</span></code></pre></div></p> <p>内部的に、<code>label</code>は<code>ClassLabel</code>タイプで、整数とラベル名のマッピングは<em>names</em>フォルダに保存されています。<code>0</code>は<code>not_equivalent</code>に対応し、<code>1</code>は<code>equivalent</code>に対応します。</p> <div class="admonition tip"> <p class=admonition-title>試してみよう！</p> <p>訓練セットの15番目の要素と検証セットの87番目の要素を見てください。それらのラベルは何ですか？</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=c1># 訓練セットの15番目の要素</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=n>train_label_15th</span> <span class=o>=</span> <span class=n>raw_train_dataset</span><span class=p>[</span><span class=mi>14</span><span class=p>][</span><span class=s2>&quot;label&quot;</span><span class=p>]</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=n>class_label</span> <span class=o>=</span> <span class=n>raw_train_dataset</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=s2>&quot;label&quot;</span><span class=p>]</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=nb>print</span><span class=p>(</span><span class=n>class_label</span><span class=o>.</span><span class=n>int2str</span><span class=p>(</span><span class=n>train_label_15th</span><span class=p>))</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a><span class=c1># 検証セットの87番目の要素</span>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=n>raw_test_dataset</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;test&quot;</span><span class=p>]</span>
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a><span class=n>test_label_87th</span> <span class=o>=</span> <span class=n>raw_test_dataset</span><span class=p>[</span><span class=mi>86</span><span class=p>][</span><span class=s2>&quot;label&quot;</span><span class=p>]</span>
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a><span class=n>class_label</span> <span class=o>=</span> <span class=n>raw_test_dataset</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=s2>&quot;label&quot;</span><span class=p>]</span>
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a><span class=nb>print</span><span class=p>(</span><span class=n>class_label</span><span class=o>.</span><span class=n>int2str</span><span class=p>(</span><span class=n>test_label_87th</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>not_equivalent
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>equivalent
</span></code></pre></div></p> <h3 id=_6>データセットの前処理<a class=headerlink href=#_6 title="Permanent link">&para;</a></h3> <p>データセットを前処理するためには、テキストをモデルが理解できる数値に変換する必要があります。<a href=../02_using_transformers/ >前章</a>で学んだように、この処理はトークナイザーで行います。</p> <p>トークナイザーには1つの文または文のリストを渡すことができるため、各ペアのすべての第1文とすべての第2文を直接トークン化できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;bert-base-uncased&quot;</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a><span class=n>tokenized_sentences_1</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=s2>&quot;sentence1&quot;</span><span class=p>]))</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a><span class=n>tokenized_sentences_2</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=s2>&quot;sentence2&quot;</span><span class=p>]))</span>
</span></code></pre></div> <p>しかし、2つのシーケンスをモデルに渡して、文が言い換えかどうかの予測を得ることはできません。2つのシーケンスをペアとして処理し、適切な前処理を適用する必要があります。</p> <p>幸い、トークナイザーはシーケンスのペアも受け取ることができ、BERTモデルが期待する形式で準備できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=kn>from</span><span class=w> </span><span class=nn>pprint</span><span class=w> </span><span class=kn>import</span> <span class=n>pprint</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&quot;This is the first sentence.&quot;</span><span class=p>,</span> <span class=s2>&quot;This is the second one.&quot;</span><span class=p>)</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=n>pprint</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a> &#39;input_ids&#39;: [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996,
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>               2117, 2028, 1012, 102],
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]}
</span></code></pre></div></p> <p><a href=../02_using_transformers/ >第2章</a>で<code>input_ids</code>と<code>attention_mask</code>キーについて説明しましたが、<code>token_type_ids</code>については後回しにしていました。この例では、これがモデルに入力のどの部分が第1文でどの部分が第2文かを伝えるものです。</p> <div class="admonition tip"> <p class=admonition-title>試してみよう！</p> <p>訓練セットの15番目の要素を取り、2つの文を別々にトークン化し、ペアとしてもトークン化してください。2つの結果の違いは何ですか？</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># 訓練セットの15番目の要素</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>raw_train_dataset_15th</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=mi>14</span><span class=p>]</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=n>sentence1</span> <span class=o>=</span> <span class=n>raw_train_dataset_15th</span><span class=p>[</span><span class=s2>&quot;sentence1&quot;</span><span class=p>]</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=n>sentence2</span> <span class=o>=</span> <span class=n>raw_train_dataset_15th</span><span class=p>[</span><span class=s2>&quot;sentence2&quot;</span><span class=p>]</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a><span class=n>tokens_s1</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence1</span><span class=p>)</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=n>tokens_s2</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence2</span><span class=p>)</span>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a><span class=n>tokens_pair</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence1</span><span class=p>,</span> <span class=n>sentence2</span><span class=p>)</span>
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a>
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;sentence1:&quot;</span><span class=p>)</span>
</span><span id=__span-13-12><a id=__codelineno-13-12 name=__codelineno-13-12 href=#__codelineno-13-12></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens_s1</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-13-13><a id=__codelineno-13-13 name=__codelineno-13-13 href=#__codelineno-13-13></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;sentence2:&quot;</span><span class=p>)</span>
</span><span id=__span-13-14><a id=__codelineno-13-14 name=__codelineno-13-14 href=#__codelineno-13-14></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens_s2</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-13-15><a id=__codelineno-13-15 name=__codelineno-13-15 href=#__codelineno-13-15></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;pair:&quot;</span><span class=p>)</span>
</span><span id=__span-13-16><a id=__codelineno-13-16 name=__codelineno-13-16 href=#__codelineno-13-16></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens_pair</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>sentence1:
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>                    1, 1, 1, 1],
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a> &#39;input_ids&#39;: [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997,
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>               1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229,
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>               5467, 1012, 102],
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a>                    0, 0, 0, 0]}
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a>sentence2:
</span><span id=__span-14-10><a id=__codelineno-14-10 name=__codelineno-14-10 href=#__codelineno-14-10></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
</span><span id=__span-14-11><a id=__codelineno-14-11 name=__codelineno-14-11 href=#__codelineno-14-11></a>                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-14-12><a id=__codelineno-14-12 name=__codelineno-14-12 href=#__codelineno-14-12></a> &#39;input_ids&#39;: [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677,
</span><span id=__span-14-13><a id=__codelineno-14-13 name=__codelineno-14-13 href=#__codelineno-14-13></a>               22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018,
</span><span id=__span-14-14><a id=__codelineno-14-14 name=__codelineno-14-14 href=#__codelineno-14-14></a>               3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102],
</span><span id=__span-14-15><a id=__codelineno-14-15 name=__codelineno-14-15 href=#__codelineno-14-15></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
</span><span id=__span-14-16><a id=__codelineno-14-16 name=__codelineno-14-16 href=#__codelineno-14-16></a>                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
</span><span id=__span-14-17><a id=__codelineno-14-17 name=__codelineno-14-17 href=#__codelineno-14-17></a>pair:
</span><span id=__span-14-18><a id=__codelineno-14-18 name=__codelineno-14-18 href=#__codelineno-14-18></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
</span><span id=__span-14-19><a id=__codelineno-14-19 name=__codelineno-14-19 href=#__codelineno-14-19></a>                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
</span><span id=__span-14-20><a id=__codelineno-14-20 name=__codelineno-14-20 href=#__codelineno-14-20></a>                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-14-21><a id=__codelineno-14-21 name=__codelineno-14-21 href=#__codelineno-14-21></a> &#39;input_ids&#39;: [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997,
</span><span id=__span-14-22><a id=__codelineno-14-22 name=__codelineno-14-22 href=#__codelineno-14-22></a>               1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229,
</span><span id=__span-14-23><a id=__codelineno-14-23 name=__codelineno-14-23 href=#__codelineno-14-23></a>               5467, 1012, 102, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010,
</span><span id=__span-14-24><a id=__codelineno-14-24 name=__codelineno-14-24 href=#__codelineno-14-24></a>               1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873,
</span><span id=__span-14-25><a id=__codelineno-14-25 name=__codelineno-14-25 href=#__codelineno-14-25></a>               4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012,
</span><span id=__span-14-26><a id=__codelineno-14-26 name=__codelineno-14-26 href=#__codelineno-14-26></a>               102],
</span><span id=__span-14-27><a id=__codelineno-14-27 name=__codelineno-14-27 href=#__codelineno-14-27></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
</span><span id=__span-14-28><a id=__codelineno-14-28 name=__codelineno-14-28 href=#__codelineno-14-28></a>                    0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
</span><span id=__span-14-29><a id=__codelineno-14-29 name=__codelineno-14-29 href=#__codelineno-14-29></a>                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</span></code></pre></div></p> <p><code>input_ids</code>内のIDを単語にデコードすると：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=nb>print</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a>[&#39;[CLS]&#39;, &#39;this&#39;, &#39;is&#39;, &#39;the&#39;, &#39;first&#39;, &#39;sentence&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;this&#39;, &#39;is&#39;, &#39;the&#39;, &#39;second&#39;, &#39;one&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span></code></pre></div></p> <p>ご覧のとおり、<code>[CLS] sentence1 [SEP]</code>に対応する入力の部分はすべてトークンタイプID <code>0</code>を持ち、他の部分（<code>sentence2 [SEP]</code>に対応）はすべてトークンタイプID <code>1</code>を持ちます。</p> <p>異なるチェックポイントを選択した場合、トークン化された入力に<code>token_type_ids</code>が必ずしも含まれるとは限らないことに注意してください（例えば、DistilBERTモデルを使用した場合は返されません）。これらは、モデルが事前訓練中にそれらを見ているため、それらをどう処理するかを知っている場合にのみ返されます。</p> <p>ここで、BERTはトークンタイプIDで事前訓練されており、第1章で説明したマスク言語モデリング目的に加えて、_次文予測_と呼ばれる追加の目的があります。このタスクの目標は、文ペア間の関係をモデリングすることです。</p> <p>次文予測では、モデルに文のペア（ランダムにマスクされたトークンを含む）が提供され、第2文が第1文に続くかどうかを予測するよう求められます。タスクを自明でなくするため、半分の時間では文は元の文書で互いに続いており、残りの半分では2つの文は2つの異なる文書から来ています。</p> <p>一般的に、トークン化された入力に<code>token_type_ids</code>があるかどうか心配する必要はありません。トークナイザーとモデルに同じチェックポイントを使用する限り、トークナイザーが自分のモデルに何を提供すべきかを知っているため、すべて問題ありません。</p> <p>トークナイザーが1組の文をどう処理できるかを見たので、それを使用してデータセット全体をトークン化できます。第2章のように、第1文のリスト、次に第2文のリストを渡すことで、文ペアのリストをトークナイザーに渡すことができます。これは第2章で見たパディングと切り詰めオプションとも互換性があります。したがって、訓練データセットを前処理する1つの方法は：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=n>tokenized_dataset</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>    <span class=nb>list</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=s2>&quot;sentence1&quot;</span><span class=p>]),</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>    <span class=nb>list</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=s2>&quot;sentence2&quot;</span><span class=p>]),</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>    <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>    <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a><span class=p>)</span>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a><span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>tokenized_dataset</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a>    <span class=nb>print</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>[:</span><span class=mi>2</span><span class=p>])</span>  <span class=c1># 2サンプル</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a>input_ids [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a>token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a>attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
</span></code></pre></div></p> <p>これは良い方法ですが、辞書を返す（<code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code>というキーと、リストのリストである値を持つ）という欠点があります。また、トークン化中にデータセット全体をメモリに保存するのに十分なRAMがある場合にのみ機能します（Hugging Face Datasetsライブラリのデータセットは<a href=https://arrow.apache.org/ >Apache Arrow</a>ファイルとしてディスクに保存されるため、要求したサンプルのみがメモリに読み込まれます）。</p> <p>データをデータセットとして保持するには、<a href=https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map><code>Dataset.map()</code></a>メソッドを使用します。これにより、トークン化以上の前処理が必要な場合に追加の柔軟性も得られます。<code>map()</code>メソッドは、データセットの各要素に関数を適用することで機能するため、入力をトークン化する関数を定義しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize_function</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>    <span class=k>return</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence1&quot;</span><span class=p>],</span> <span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence2&quot;</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p>この関数は辞書（データセットの項目のような）を受け取り、<code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code>キーを持つ新しい辞書を返します。<code>example</code>辞書に複数のサンプル（各キーが文のリスト）が含まれている場合でも機能することに注意してください。これは、前に見たように、<code>tokenizer</code>が文ペアのリストで動作するためです。これにより、<code>map()</code>の呼び出しで<code>batched=True</code>オプションを使用でき、トークン化が大幅に高速化されます。<code>tokenizer</code>は<a href=https://github.com/huggingface/tokenizers>Hugging Face Tokenizers</a>ライブラリのRustで書かれたトークナイザーによってサポートされています。このトークナイザーは非常に高速ですが、一度に多くの入力を与えた場合にのみその効果を発揮します。</p> <p>今のところ、トークン化関数で<code>padding</code>引数を省略していることに注意してください。これは、すべてのサンプルを最大長にパディングするのは効率的ではないためです。バッチを構築する際にサンプルをパディングする方が良く、その場合はデータセット全体の最大長ではなく、そのバッチ内の最大長にのみパディングすれば済みます。これにより、入力の長さが非常に変動する場合に多くの時間と処理能力を節約できます！</p> <p>以下は、すべてのデータセットに一度にトークン化関数を適用する方法です。<code>map</code>の呼び出しで<code>batched=True</code>を使用しているため、関数はデータセットの各要素に個別ではなく、データセットの複数要素に一度に適用されます。これにより、より高速な前処理が可能になります。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>tokenize_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenized_datasets</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a>Map:   0%|          | 0/1725 [00:00&lt;?, ? examples/s]
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>Map: 100%|██████████| 1725/1725 [00:00&lt;00:00, 35443.43 examples/s]
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>DatasetDict({
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a>    train: Dataset({
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],
</span><span id=__span-21-7><a id=__codelineno-21-7 name=__codelineno-21-7 href=#__codelineno-21-7></a>        num_rows: 3668
</span><span id=__span-21-8><a id=__codelineno-21-8 name=__codelineno-21-8 href=#__codelineno-21-8></a>    })
</span><span id=__span-21-9><a id=__codelineno-21-9 name=__codelineno-21-9 href=#__codelineno-21-9></a>    validation: Dataset({
</span><span id=__span-21-10><a id=__codelineno-21-10 name=__codelineno-21-10 href=#__codelineno-21-10></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],
</span><span id=__span-21-11><a id=__codelineno-21-11 name=__codelineno-21-11 href=#__codelineno-21-11></a>        num_rows: 408
</span><span id=__span-21-12><a id=__codelineno-21-12 name=__codelineno-21-12 href=#__codelineno-21-12></a>    })
</span><span id=__span-21-13><a id=__codelineno-21-13 name=__codelineno-21-13 href=#__codelineno-21-13></a>    test: Dataset({
</span><span id=__span-21-14><a id=__codelineno-21-14 name=__codelineno-21-14 href=#__codelineno-21-14></a>        features: [&#39;sentence1&#39;, &#39;sentence2&#39;, &#39;label&#39;, &#39;idx&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],
</span><span id=__span-21-15><a id=__codelineno-21-15 name=__codelineno-21-15 href=#__codelineno-21-15></a>        num_rows: 1725
</span><span id=__span-21-16><a id=__codelineno-21-16 name=__codelineno-21-16 href=#__codelineno-21-16></a>    })
</span><span id=__span-21-17><a id=__codelineno-21-17 name=__codelineno-21-17 href=#__codelineno-21-17></a>})
</span></code></pre></div></p> <p>Datasetsライブラリがこの処理を適用する方法は、前処理関数によって返される辞書の各キーに対して、データセットに新しいフィールドを追加することです。</p> <p><code>tokenize_function</code>は<code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code>キーを持つ辞書を返すため、これら3つのフィールドがデータセットのすべての分割に追加されます。前処理関数が<code>map()</code>を適用したデータセット内の既存のキーに対して新しい値を返した場合、既存のフィールドも変更できることに注意してください。</p> <p>最後に行う必要があるのは、要素をバッチにまとめる際に、最も長い要素の長さまですべての例をパディングすることです — これを<em>動的パディング</em>と呼びます。</p> <h4 id=_7>動的パディング<a class=headerlink href=#_7 title="Permanent link">&para;</a></h4> <p>バッチ内でサンプルをまとめる役割を果たす関数は<em>collate関数</em>と呼ばれます。これは<code>DataLoader</code>を構築する際に渡すことができる引数で、デフォルトはサンプルをPyTorchテンソルに変換して連結する関数です（要素がリスト、タプル、または辞書の場合は再帰的に）。私たちのケースでは、すべての入力が同じサイズではないため、これは不可能です。意図的にパディングを遅らせ、各バッチで必要な分だけ適用し、過度に長い入力と大量のパディングを避けています。これにより訓練がかなり高速化されますが、TPUで訓練している場合は問題を引き起こす可能性があることに注意してください — TPUは追加のパディングが必要でも固定された形状を好みます。</p> <p>実際にこれを行うには、データセットの項目をバッチにまとめたい場合に、適切な量のパディングを適用するcollate関数を定義する必要があります。幸い、Transformersライブラリは<code>DataCollatorWithPadding</code>を通じてそのような関数を提供しています。インスタンス化時にトークナイザーを受け取り（使用するパディングトークンや、モデルがパディングを入力の左側または右側に期待するかを知るため）、必要なすべての処理を行います：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>DataCollatorWithPadding</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a><span class=n>data_collator</span> <span class=o>=</span> <span class=n>DataCollatorWithPadding</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></code></pre></div> <p>この新しいツールをテストするため、バッチにまとめたい訓練セットからいくつかのサンプルを取得しましょう。ここでは、文字列を含み、テンソルを作成できない（文字列でテンソルを作成することはできない）<code>idx</code>、<code>sentence1</code>、<code>sentence2</code>列を削除し、バッチ内の各エントリの長さを見てみます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a><span class=n>samples</span> <span class=o>=</span> <span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][:</span><span class=mi>8</span><span class=p>]</span>
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a><span class=n>samples</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>samples</span><span class=o>.</span><span class=n>items</span><span class=p>()</span> <span class=k>if</span> <span class=n>k</span> <span class=ow>not</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&quot;idx&quot;</span><span class=p>,</span> <span class=s2>&quot;sentence1&quot;</span><span class=p>,</span> <span class=s2>&quot;sentence2&quot;</span><span class=p>]}</span>
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>samples</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a>[50, 59, 47, 67, 59, 50, 62, 32]
</span></code></pre></div></p> <p>驚くことではありませんが、32から67までの様々な長さのサンプルが得られます。動的パディングとは、このバッチ内のサンプルはすべて、バッチ内の最大長である67までパディングされるべきということです。動的パディングなしでは、すべてのサンプルをデータセット全体の最大長、またはモデルが受け入れることができる最大長までパディングする必要があります。<code>data_collator</code>がバッチを適切に動的パディングしているかダブルチェックしましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=n>batch</span> <span class=o>=</span> <span class=n>data_collator</span><span class=p>(</span><span class=n>samples</span><span class=p>)</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a><span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>shape</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>batch</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a>{&#39;input_ids&#39;: torch.Size([8, 67]),
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a> &#39;token_type_ids&#39;: torch.Size([8, 67]),
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a> &#39;attention_mask&#39;: torch.Size([8, 67]),
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a> &#39;labels&#39;: torch.Size([8])}
</span></code></pre></div></p> <h2 id=trainer-api>Trainer APIを使用したモデルのファインチューニング<a class=headerlink href=#trainer-api title="Permanent link">&para;</a></h2> <p>以下のコード例は、前のセクションの例をすでに実行していることを前提としています。必要なもののショート要約を以下に示します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>DataCollatorWithPadding</span>
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a>
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a><span class=n>raw_datasets</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;bert-base-uncased&quot;</span>
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a>
</span><span id=__span-27-8><a id=__codelineno-27-8 name=__codelineno-27-8 href=#__codelineno-27-8></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize_function</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span><span id=__span-27-9><a id=__codelineno-27-9 name=__codelineno-27-9 href=#__codelineno-27-9></a>    <span class=k>return</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence1&quot;</span><span class=p>],</span><span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence2&quot;</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-27-10><a id=__codelineno-27-10 name=__codelineno-27-10 href=#__codelineno-27-10></a>
</span><span id=__span-27-11><a id=__codelineno-27-11 name=__codelineno-27-11 href=#__codelineno-27-11></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>tokenize_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-27-12><a id=__codelineno-27-12 name=__codelineno-27-12 href=#__codelineno-27-12></a><span class=n>data_collator</span> <span class=o>=</span> <span class=n>DataCollatorWithPadding</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a>Map: 100%|██████████| 1725/1725 [00:00&lt;00:00, 31091.75 examples/s]
</span></code></pre></div></p> <h3 id=_8>訓練<a class=headerlink href=#_8 title="Permanent link">&para;</a></h3> <p><code>Trainer</code>を定義する前の最初のステップは、<code>Trainer</code>が訓練と評価に使用するすべてのハイパーパラメータを含む<code>TrainingArguments</code>クラスを定義することです。提供する必要がある唯一の引数は、訓練されたモデルが保存されるディレクトリと、途中のチェックポイントです。残りはすべてデフォルトのままにでき、基本的なファインチューニングにはかなり良く機能するはずです。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>TrainingArguments</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span><span class=s2>&quot;test-trainer&quot;</span><span class=p>)</span>
</span></code></pre></div> <div class="admonition tip"> <p class=admonition-title>高度な設定</p> <p>利用可能なすべての訓練引数と最適化戦略の詳細については、<a href=https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments>TrainingArgumentsドキュメント</a>と<a href=https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu>訓練設定クックブック</a>をご確認ください。</p> </div> <p>2番目のステップはモデルを定義することです。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-30-2><a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a>
</span><span id=__span-30-3><a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</span></code></pre></div></p> <p>この事前訓練されたモデルをインスタンス化した後に警告が表示されます。これは、BERTが文ペアの分類で事前訓練されていないため、事前訓練されたモデルのヘッドが破棄され、シーケンス分類に適した新しいヘッドが代わりに追加されたためです。警告は、一部の重みが使用されなかった（破棄された事前訓練ヘッドに対応）こと、および他の重みがランダムに初期化された（新しいヘッド用）ことを示しています。モデルを訓練することを推奨して終わり、それがまさに今から行うことです。</p> <p>モデルができたら、これまでに構築したすべてのオブジェクト（<code>model</code>、<code>training_args</code>、訓練と検証データセット、<code>data_collator</code>、<code>processing_class</code>）を渡して<code>Trainer</code>を定義できます。<code>processing_class</code>パラメータは、Trainerに処理に使用するトークナイザーを伝える新しい追加機能です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>Trainer</span>
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a>
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a>    <span class=n>model</span><span class=p>,</span>
</span><span id=__span-32-5><a id=__codelineno-32-5 name=__codelineno-32-5 href=#__codelineno-32-5></a>    <span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-32-6><a id=__codelineno-32-6 name=__codelineno-32-6 href=#__codelineno-32-6></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span>
</span><span id=__span-32-7><a id=__codelineno-32-7 name=__codelineno-32-7 href=#__codelineno-32-7></a>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;validation&quot;</span><span class=p>],</span>
</span><span id=__span-32-8><a id=__codelineno-32-8 name=__codelineno-32-8 href=#__codelineno-32-8></a>    <span class=n>data_collator</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span><span id=__span-32-9><a id=__codelineno-32-9 name=__codelineno-32-9 href=#__codelineno-32-9></a>    <span class=n>processing_class</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-32-10><a id=__codelineno-32-10 name=__codelineno-32-10 href=#__codelineno-32-10></a><span class=p>)</span>
</span></code></pre></div> <p><code>processing_class</code>としてトークナイザーを渡すと、<code>Trainer</code>によって使用されるデフォルトの<code>data_collator</code>は<code>DataCollatorWithPadding</code>になります。この場合、<code>data_collator=data_collator</code>行をスキップできますが、処理パイプラインのこの重要な部分を示すためにここに含めました。</p> <p>データセットでモデルをファインチューニングするには、<code>Trainer</code>の<code>train()</code>メソッドを呼び出すだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong></p> <table> <thead> <tr> <th>Step</th> <th>Training Loss</th> </tr> </thead> <tbody> <tr> <td>500</td> <td>0.576900</td> </tr> <tr> <td>1000</td> <td>0.369200</td> </tr> </tbody> </table> <p>これによりファインチューニングが開始されます（GPUで数分程度かかります）。500ステップごとに訓練損失が報告されます。ただし、モデルがどの程度良く（または悪く）実行されているかは教えてくれません。これは以下の理由によります：</p> <ol> <li><code>TrainingArguments</code>で<code>eval_strategy</code>を<code>"steps"</code>（<code>eval_steps</code>ごとに評価）または<code>"epoch"</code>（各エポックの終わりに評価）に設定して、訓練中に評価するよう<code>Trainer</code>に指示しませんでした。</li> <li>その評価中にメトリックを計算するための<code>compute_metrics()</code>関数を<code>Trainer</code>に提供しませんでした（そうでなければ評価は損失のみを出力し、これはあまり直感的な数値ではありません）。</li> </ol> <h3 id=_9>評価<a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <p>有用な<code>compute_metrics()</code>関数を構築し、次回の訓練時に使用する方法を見てみましょう。この関数は<code>EvalPrediction</code>オブジェクト（<code>predictions</code>フィールドと<code>label_ids</code>フィールドを持つ名前付きタプル）を受け取り、文字列から浮動小数点数へのマッピングである辞書を返す必要があります（文字列は返されるメトリックの名前、浮動小数点数はその値）。モデルからいくつかの予測を得るには、<code>Trainer.predict()</code>コマンドを使用できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=n>predictions</span> <span class=o>=</span> <span class=n>trainer</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;validation&quot;</span><span class=p>])</span>
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2 href=#__codelineno-34-2></a><span class=nb>print</span><span class=p>(</span><span class=n>predictions</span><span class=o>.</span><span class=n>predictions</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>predictions</span><span class=o>.</span><span class=n>label_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a>(408, 2) (408,)
</span></code></pre></div></p> <p><code>predict()</code>メソッドの出力は、3つのフィールドを持つ別の名前付きタプルです：<code>predictions</code>、<code>label_ids</code>、<code>metrics</code>。<code>metrics</code>フィールドには、渡されたデータセットでの損失と、いくつかの時間メトリック（予測にかかった時間の合計と平均）が含まれます。<code>compute_metrics()</code>関数を完成させて<code>Trainer</code>に渡すと、そのフィールドには<code>compute_metrics()</code>によって返されるメトリックも含まれます。</p> <p>ご覧のとおり、<code>predictions</code>は408 x 2の形状を持つ2次元配列です（408は使用したデータセットの要素数）。これらは<code>predict()</code>に渡したデータセットの各要素のロジットです（前章で見たように、すべてのTransformerモデルはロジットを返します）。それらをラベルと比較できる予測に変換するには、2番目の軸で最大値のインデックスを取る必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a>
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a><span class=n>preds</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>predictions</span><span class=o>.</span><span class=n>predictions</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <p>これで、これらの<code>preds</code>をラベルと比較できます。<code>compute_metric()</code>関数を構築するために、<a href=https://github.com/huggingface/evaluate/ >Evaluate</a>ライブラリのメトリックを使用します。データセットを読み込んだのと同じように簡単に、MRPCデータセットに関連するメトリックを読み込むことができます。今回は<code>evaluate.load()</code>関数を使用します。返されるオブジェクトには、メトリック計算に使用できる<code>compute()</code>メソッドがあります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=kn>import</span><span class=w> </span><span class=nn>evaluate</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a><span class=n>metric</span> <span class=o>=</span> <span class=n>evaluate</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a><span class=n>metric</span><span class=o>.</span><span class=n>compute</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>preds</span><span class=p>,</span> <span class=n>references</span><span class=o>=</span><span class=n>predictions</span><span class=o>.</span><span class=n>label_ids</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a>{&#39;accuracy&#39;: 0.8651960784313726, &#39;f1&#39;: 0.9063032367972743}
</span></code></pre></div></p> <p>得られる正確な結果は異なる場合があります。これは、モデルヘッドのランダム初期化によって到達したメトリックが変わる可能性があるためです。ここでは、モデルが検証セットで86.52%の精度と90.63%のF1スコアを持つことがわかります。これらは、GLUEベンチマークのMRPCデータセットで結果を評価するために使用される2つのメトリックです。<a href=https://arxiv.org/pdf/1810.04805.pdf>BERTペーパー</a>の表では、ベースモデルで88.9のF1スコアが報告されています。それは<code>uncased</code>モデルでしたが、現在<code>cased</code>モデルを使用しているため、より良い結果が説明できます。</p> <p>すべてをまとめると、<code>compute_metrics()</code>関数は次のようになります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a><span class=k>def</span><span class=w> </span><span class=nf>compute_metrics</span><span class=p>(</span><span class=n>eval_preds</span><span class=p>):</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a>    <span class=n>metric</span> <span class=o>=</span> <span class=n>evaluate</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a>    <span class=n>logits</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>eval_preds</span>
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4 href=#__codelineno-39-4></a>    <span class=n>predictions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-39-5><a id=__codelineno-39-5 name=__codelineno-39-5 href=#__codelineno-39-5></a>    <span class=k>return</span> <span class=n>metric</span><span class=o>.</span><span class=n>compute</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>predictions</span><span class=p>,</span> <span class=n>references</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span>
</span></code></pre></div> <p>各エポックの終わりにメトリックを報告するために実際に使用されているのを見るために、この<code>compute_metrics()</code>関数で新しい<code>Trainer</code>を定義する方法を示します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span><span class=s2>&quot;test-trainer&quot;</span><span class=p>,</span> <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;epoch&quot;</span><span class=p>)</span>
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a>
</span><span id=__span-40-4><a id=__codelineno-40-4 name=__codelineno-40-4 href=#__codelineno-40-4></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span><span id=__span-40-5><a id=__codelineno-40-5 name=__codelineno-40-5 href=#__codelineno-40-5></a>    <span class=n>model</span><span class=p>,</span>
</span><span id=__span-40-6><a id=__codelineno-40-6 name=__codelineno-40-6 href=#__codelineno-40-6></a>    <span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-40-7><a id=__codelineno-40-7 name=__codelineno-40-7 href=#__codelineno-40-7></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span>
</span><span id=__span-40-8><a id=__codelineno-40-8 name=__codelineno-40-8 href=#__codelineno-40-8></a>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;validation&quot;</span><span class=p>],</span>
</span><span id=__span-40-9><a id=__codelineno-40-9 name=__codelineno-40-9 href=#__codelineno-40-9></a>    <span class=n>data_collator</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span><span id=__span-40-10><a id=__codelineno-40-10 name=__codelineno-40-10 href=#__codelineno-40-10></a>    <span class=n>processing_class</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-40-11><a id=__codelineno-40-11 name=__codelineno-40-11 href=#__codelineno-40-11></a>    <span class=n>compute_metrics</span><span class=o>=</span><span class=n>compute_metrics</span><span class=p>,</span>
</span><span id=__span-40-12><a id=__codelineno-40-12 name=__codelineno-40-12 href=#__codelineno-40-12></a><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</span></code></pre></div></p> <p><code>eval_strategy</code>を<code>"epoch"</code>に設定した新しい<code>TrainingArguments</code>と新しいモデルを作成することに注意してください。そうでなければ、すでに訓練したモデルの訓練を続けるだけになってしまいます。新しい訓練実行を開始するには、以下を実行します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a>|Epoch  |Training Loss  |Validation Loss    |Accuracy   |F1|
</span><span id=__span-43-2><a id=__codelineno-43-2 name=__codelineno-43-2 href=#__codelineno-43-2></a>|-|-|-|-|-|
</span><span id=__span-43-3><a id=__codelineno-43-3 name=__codelineno-43-3 href=#__codelineno-43-3></a>|1| No log| 0.381103|   0.845588    |0.890815|
</span><span id=__span-43-4><a id=__codelineno-43-4 name=__codelineno-43-4 href=#__codelineno-43-4></a>|2| 0.519900|   0.458264|   0.852941    |0.897611|
</span><span id=__span-43-5><a id=__codelineno-43-5 name=__codelineno-43-5 href=#__codelineno-43-5></a>|3| 0.292800    |0.665536   |0.857843   |0.900685|
</span></code></pre></div></p> <p>今回は、訓練損失に加えて、各エポックの終わりに検証損失とメトリックが報告されます。再び、モデルのランダムヘッド初期化のため、到達する正確な精度/F1スコアは私たちが見つけたものとは少し異なるかもしれませんが、同じような範囲にあるはずです。</p> <h3 id=_10>高度な訓練機能<a class=headerlink href=#_10 title="Permanent link">&para;</a></h3> <p><code>Trainer</code>には、モダンな深層学習のベストプラクティスをアクセスしやすくする多くの組み込み機能があります：</p> <p><strong>混合精度訓練</strong>: GPUメモリを節約し、訓練を高速化：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-44-2><a id=__codelineno-44-2 name=__codelineno-44-2 href=#__codelineno-44-2></a>    <span class=s2>&quot;test-training&quot;</span><span class=p>,</span>
</span><span id=__span-44-3><a id=__codelineno-44-3 name=__codelineno-44-3 href=#__codelineno-44-3></a>    <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;epoch&quot;</span><span class=p>,</span>
</span><span id=__span-44-4><a id=__codelineno-44-4 name=__codelineno-44-4 href=#__codelineno-44-4></a>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># 混合精度を有効化</span>
</span><span id=__span-44-5><a id=__codelineno-44-5 name=__codelineno-44-5 href=#__codelineno-44-5></a><span class=p>)</span>
</span></code></pre></div> <p><strong>勾配累積</strong>: GPUメモリが限られている場合の効果的な大きなバッチサイズ：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-45-2><a id=__codelineno-45-2 name=__codelineno-45-2 href=#__codelineno-45-2></a>    <span class=s2>&quot;test-trainer&quot;</span><span class=p>,</span>
</span><span id=__span-45-3><a id=__codelineno-45-3 name=__codelineno-45-3 href=#__codelineno-45-3></a>    <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;epoch&quot;</span><span class=p>,</span>
</span><span id=__span-45-4><a id=__codelineno-45-4 name=__codelineno-45-4 href=#__codelineno-45-4></a>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-45-5><a id=__codelineno-45-5 name=__codelineno-45-5 href=#__codelineno-45-5></a>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># 効果的なバッチサイズ = 4 * 4 = 16</span>
</span><span id=__span-45-6><a id=__codelineno-45-6 name=__codelineno-45-6 href=#__codelineno-45-6></a><span class=p>)</span>
</span></code></pre></div> <p><strong>学習率スケジューリング</strong>: Trainerはデフォルトで線形減衰を使用しますが、これをカスタマイズできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a>    <span class=s2>&quot;test-trainer&quot;</span><span class=p>,</span>
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a>    <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;epoch&quot;</span><span class=p>,</span>
</span><span id=__span-46-4><a id=__codelineno-46-4 name=__codelineno-46-4 href=#__codelineno-46-4></a>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-5</span><span class=p>,</span>
</span><span id=__span-46-5><a id=__codelineno-46-5 name=__codelineno-46-5 href=#__codelineno-46-5></a>    <span class=n>lr_scheduler_type</span><span class=o>=</span><span class=s2>&quot;cosine&quot;</span><span class=p>,</span>  <span class=c1># 異なるスケジューラーを試す</span>
</span><span id=__span-46-6><a id=__codelineno-46-6 name=__codelineno-46-6 href=#__codelineno-46-6></a><span class=p>)</span>
</span></code></pre></div> <h2 id=_11>完全な訓練ループ<a class=headerlink href=#_11 title="Permanent link">&para;</a></h2> <p>最後のセクションで行ったのと同じ結果を、<code>Trainer</code>クラスを使用せずに、モダンなPyTorchベストプラクティスでスクラッチから訓練ループを実装して達成する方法を見ていきます。再び、セクション2でデータ処理を行ったと仮定します。必要なすべてをカバーする短い要約を以下に示します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>DataCollatorWithPadding</span>
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a>
</span><span id=__span-47-4><a id=__codelineno-47-4 name=__codelineno-47-4 href=#__codelineno-47-4></a><span class=n>raw_datasets</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-47-5><a id=__codelineno-47-5 name=__codelineno-47-5 href=#__codelineno-47-5></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;bert-base-uncased&quot;</span>
</span><span id=__span-47-6><a id=__codelineno-47-6 name=__codelineno-47-6 href=#__codelineno-47-6></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-47-7><a id=__codelineno-47-7 name=__codelineno-47-7 href=#__codelineno-47-7></a>
</span><span id=__span-47-8><a id=__codelineno-47-8 name=__codelineno-47-8 href=#__codelineno-47-8></a>
</span><span id=__span-47-9><a id=__codelineno-47-9 name=__codelineno-47-9 href=#__codelineno-47-9></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize_function</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span><span id=__span-47-10><a id=__codelineno-47-10 name=__codelineno-47-10 href=#__codelineno-47-10></a>    <span class=k>return</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence1&quot;</span><span class=p>],</span> <span class=n>example</span><span class=p>[</span><span class=s2>&quot;sentence2&quot;</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-47-11><a id=__codelineno-47-11 name=__codelineno-47-11 href=#__codelineno-47-11></a>
</span><span id=__span-47-12><a id=__codelineno-47-12 name=__codelineno-47-12 href=#__codelineno-47-12></a>
</span><span id=__span-47-13><a id=__codelineno-47-13 name=__codelineno-47-13 href=#__codelineno-47-13></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>tokenize_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-47-14><a id=__codelineno-47-14 name=__codelineno-47-14 href=#__codelineno-47-14></a><span class=n>data_collator</span> <span class=o>=</span> <span class=n>DataCollatorWithPadding</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a>Map: 100%|██████████| 3668/3668 [00:00&lt;00:00, 33587.91 examples/s]
</span></code></pre></div></p> <h3 id=_12>訓練の準備<a class=headerlink href=#_12 title="Permanent link">&para;</a></h3> <p>実際に訓練ループを書く前に、いくつかのオブジェクトを定義する必要があります。最初のものは、バッチを反復処理するために使用するデータローダーです。しかし、これらのデータローダーを定義する前に、<code>tokenized_datasets</code>に少し後処理を適用し、<code>Trainer</code>が自動的に行ったいくつかのことを処理する必要があります。具体的には、以下が必要です：</p> <ul> <li>モデルが期待しない値に対応する列を削除する（<code>sentence1</code>や<code>sentence2</code>列など）</li> <li>列<code>label</code>を<code>labels</code>に名前変更する（モデルは引数が<code>labels</code>という名前であることを期待するため）</li> <li>リストではなくPyTorchテンソルを返すようにデータセットの形式を設定する</li> </ul> <p><code>tokenized_datasets</code>には、これらの各ステップに対応するメソッドがあります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>tokenized_datasets</span><span class=o>.</span><span class=n>remove_columns</span><span class=p>([</span><span class=s2>&quot;sentence1&quot;</span><span class=p>,</span> <span class=s2>&quot;sentence2&quot;</span><span class=p>,</span> <span class=s2>&quot;idx&quot;</span><span class=p>])</span>
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>tokenized_datasets</span><span class=o>.</span><span class=n>rename_column</span><span class=p>(</span><span class=s2>&quot;label&quot;</span><span class=p>,</span> <span class=s2>&quot;labels&quot;</span><span class=p>)</span>
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a><span class=n>tokenized_datasets</span><span class=o>.</span><span class=n>set_format</span><span class=p>(</span><span class=s2>&quot;torch&quot;</span><span class=p>)</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>column_names</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a>[&#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]
</span></code></pre></div></p> <p>これで完了したので、データローダーを簡単に定義できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>DataLoader</span>
</span><span id=__span-52-2><a id=__codelineno-52-2 name=__codelineno-52-2 href=#__codelineno-52-2></a>
</span><span id=__span-52-3><a id=__codelineno-52-3 name=__codelineno-52-3 href=#__codelineno-52-3></a><span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-52-4><a id=__codelineno-52-4 name=__codelineno-52-4 href=#__codelineno-52-4></a>    <span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>collate_fn</span><span class=o>=</span><span class=n>data_collator</span>
</span><span id=__span-52-5><a id=__codelineno-52-5 name=__codelineno-52-5 href=#__codelineno-52-5></a><span class=p>)</span>
</span><span id=__span-52-6><a id=__codelineno-52-6 name=__codelineno-52-6 href=#__codelineno-52-6></a>
</span><span id=__span-52-7><a id=__codelineno-52-7 name=__codelineno-52-7 href=#__codelineno-52-7></a><span class=n>eval_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-52-8><a id=__codelineno-52-8 name=__codelineno-52-8 href=#__codelineno-52-8></a>    <span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;validation&quot;</span><span class=p>],</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>collate_fn</span><span class=o>=</span><span class=n>data_collator</span>
</span><span id=__span-52-9><a id=__codelineno-52-9 name=__codelineno-52-9 href=#__codelineno-52-9></a><span class=p>)</span>
</span></code></pre></div> <p>データ処理に間違いがないことを素早く確認するために、このようにバッチを検査できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_dataloader</span><span class=p>:</span>
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a>    <span class=k>break</span>
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a><span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>shape</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>batch</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a>{&#39;labels&#39;: torch.Size([8]),
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a> &#39;input_ids&#39;: torch.Size([8, 70]),
</span><span id=__span-54-3><a id=__codelineno-54-3 name=__codelineno-54-3 href=#__codelineno-54-3></a> &#39;token_type_ids&#39;: torch.Size([8, 70]),
</span><span id=__span-54-4><a id=__codelineno-54-4 name=__codelineno-54-4 href=#__codelineno-54-4></a> &#39;attention_mask&#39;: torch.Size([8, 70])}
</span></code></pre></div></p> <p>訓練データローダーに<code>shuffle=True</code>を設定し、バッチ内の最大長にパディングしているため、実際の形状はおそらく少し異なることに注意してください。</p> <p>データ前処理が完全に終了したので（あらゆるML実践者にとって満足のいく、しかし捉えどころのない目標）、モデルに目を向けましょう。前のセクションと全く同じようにインスタンス化します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-55-2><a id=__codelineno-55-2 name=__codelineno-55-2 href=#__codelineno-55-2></a>
</span><span id=__span-55-3><a id=__codelineno-55-3 name=__codelineno-55-3 href=#__codelineno-55-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
</span><span id=__span-56-2><a id=__codelineno-56-2 name=__codelineno-56-2 href=#__codelineno-56-2></a>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</span></code></pre></div></p> <p>損失とロジットの両方を取得していることを確認するために、実際にこのモデルを1つのバッチで通します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a><span class=n>batch</span> <span class=o>=</span> <span class=n>batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&quot;mps&quot;</span><span class=p>)</span>
</span><span id=__span-57-2><a id=__codelineno-57-2 name=__codelineno-57-2 href=#__codelineno-57-2></a><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-57-3><a id=__codelineno-57-3 name=__codelineno-57-3 href=#__codelineno-57-3></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>loss</span><span class=p>,</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a>tensor(0.0035, device=&#39;mps:0&#39;, grad_fn=&lt;NllLossBackward0&gt;) torch.Size([8, 2])
</span></code></pre></div></p> <p>すべてのHugging Face Transformersモデルは、<code>labels</code>が提供されると損失を返し、ロジットも取得できます（バッチ内の各入力に対して2つ、つまり8 x 2のサイズのテンソル）。</p> <p>訓練ループを書く準備がほぼ整いました！あと2つだけ必要なものがあります：オプティマイザーと学習率スケジューラーです。<code>Trainer</code>が手動で行っていたことを再現しようとしているので、同じデフォルトを使用します。<code>Trainer</code>が使用するオプティマイザーは<code>AdamW</code>で、これはAdamと同じですが、重み減衰正則化のひねりがあります（Ilya LoshchilovとFrank Hutterによる<a href=https://arxiv.org/abs/1711.05101>"Decoupled Weight Decay Regularization"</a>を参照）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=kn>import</span> <span class=n>AdamW</span>
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a>
</span><span id=__span-59-3><a id=__codelineno-59-3 name=__codelineno-59-3 href=#__codelineno-59-3></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>)</span>
</span></code></pre></div> <div class="admonition tip"> <p class=admonition-title>モダン最適化のコツ</p> <p>さらに良いパフォーマンスのために、以下を試すことができます：</p> <ul> <li><strong>重み減衰付きAdamW</strong>: <code>AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)</code></li> <li><strong>8ビットAdam</strong>: メモリ効率的な最適化のために<code>bitsandbytes</code>を使用</li> <li><strong>異なる学習率</strong>: 大きなモデルには低い学習率（1e-5から3e-5）がしばしば良く機能します</li> </ul> </div> <p>最後に、デフォルトで使用される学習率スケジューラーは、最大値（5e-5）から0への単純な線形減衰です。適切に定義するには、実行する訓練ステップ数を知る必要があります。これは、実行したいエポック数に訓練バッチ数（訓練データローダーの長さ）を掛けたものです。<code>Trainer</code>はデフォルトで3エポックを使用するので、それに従います：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>get_scheduler</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>3</span>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a><span class=n>num_training_steps</span> <span class=o>=</span> <span class=n>num_epochs</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>)</span>
</span><span id=__span-60-4><a id=__codelineno-60-4 name=__codelineno-60-4 href=#__codelineno-60-4></a>
</span><span id=__span-60-5><a id=__codelineno-60-5 name=__codelineno-60-5 href=#__codelineno-60-5></a><span class=n>lr_scheduler</span> <span class=o>=</span> <span class=n>get_scheduler</span><span class=p>(</span>
</span><span id=__span-60-6><a id=__codelineno-60-6 name=__codelineno-60-6 href=#__codelineno-60-6></a>    <span class=s2>&quot;linear&quot;</span><span class=p>,</span>
</span><span id=__span-60-7><a id=__codelineno-60-7 name=__codelineno-60-7 href=#__codelineno-60-7></a>    <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-60-8><a id=__codelineno-60-8 name=__codelineno-60-8 href=#__codelineno-60-8></a>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span><span id=__span-60-9><a id=__codelineno-60-9 name=__codelineno-60-9 href=#__codelineno-60-9></a>    <span class=n>num_training_steps</span><span class=o>=</span><span class=n>num_training_steps</span>
</span><span id=__span-60-10><a id=__codelineno-60-10 name=__codelineno-60-10 href=#__codelineno-60-10></a><span class=p>)</span>
</span><span id=__span-60-11><a id=__codelineno-60-11 name=__codelineno-60-11 href=#__codelineno-60-11></a>
</span><span id=__span-60-12><a id=__codelineno-60-12 name=__codelineno-60-12 href=#__codelineno-60-12></a><span class=nb>print</span><span class=p>(</span><span class=n>num_training_steps</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a>1377
</span></code></pre></div></p> <h3 id=_13>訓練ループ<a class=headerlink href=#_13 title="Permanent link">&para;</a></h3> <p>最後の1つ：GPUにアクセスできる場合はそれを使用したいです（CPUでは、訓練に数分ではなく数時間かかる可能性があります）。これを行うために、モデルとバッチを配置する<code>device</code>を定義します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-62-2><a id=__codelineno-62-2 name=__codelineno-62-2 href=#__codelineno-62-2></a>
</span><span id=__span-62-3><a id=__codelineno-62-3 name=__codelineno-62-3 href=#__codelineno-62-3></a><span class=c1># CUDA用</span>
</span><span id=__span-62-4><a id=__codelineno-62-4 name=__codelineno-62-4 href=#__codelineno-62-4></a><span class=c1># device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)</span>
</span><span id=__span-62-5><a id=__codelineno-62-5 name=__codelineno-62-5 href=#__codelineno-62-5></a><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&quot;mps&quot;</span><span class=p>)</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>mps</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&quot;cpu&quot;</span><span class=p>)</span>
</span><span id=__span-62-6><a id=__codelineno-62-6 name=__codelineno-62-6 href=#__codelineno-62-6></a><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-62-7><a id=__codelineno-62-7 name=__codelineno-62-7 href=#__codelineno-62-7></a><span class=n>device</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>device(type=&#39;mps&#39;)
</span></code></pre></div></p> <p>これで訓練の準備が整いました！訓練がいつ終了するかの感覚を得るために、<code>tqdm</code>ライブラリを使用して訓練ステップ数にプログレスバーを追加します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=kn>from</span><span class=w> </span><span class=nn>tqdm</span><span class=w> </span><span class=kn>import</span> <span class=n>tqdm</span>
</span><span id=__span-64-2><a id=__codelineno-64-2 name=__codelineno-64-2 href=#__codelineno-64-2></a>
</span><span id=__span-64-3><a id=__codelineno-64-3 name=__codelineno-64-3 href=#__codelineno-64-3></a><span class=n>progress_bar</span> <span class=o>=</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_training_steps</span><span class=p>))</span>
</span><span id=__span-64-4><a id=__codelineno-64-4 name=__codelineno-64-4 href=#__codelineno-64-4></a>
</span><span id=__span-64-5><a id=__codelineno-64-5 name=__codelineno-64-5 href=#__codelineno-64-5></a><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-64-6><a id=__codelineno-64-6 name=__codelineno-64-6 href=#__codelineno-64-6></a><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span><span id=__span-64-7><a id=__codelineno-64-7 name=__codelineno-64-7 href=#__codelineno-64-7></a>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_dataloader</span><span class=p>:</span>
</span><span id=__span-64-8><a id=__codelineno-64-8 name=__codelineno-64-8 href=#__codelineno-64-8></a>        <span class=n>batch</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>batch</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span><span id=__span-64-9><a id=__codelineno-64-9 name=__codelineno-64-9 href=#__codelineno-64-9></a>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-64-10><a id=__codelineno-64-10 name=__codelineno-64-10 href=#__codelineno-64-10></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span><span id=__span-64-11><a id=__codelineno-64-11 name=__codelineno-64-11 href=#__codelineno-64-11></a>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-64-12><a id=__codelineno-64-12 name=__codelineno-64-12 href=#__codelineno-64-12></a>
</span><span id=__span-64-13><a id=__codelineno-64-13 name=__codelineno-64-13 href=#__codelineno-64-13></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-64-14><a id=__codelineno-64-14 name=__codelineno-64-14 href=#__codelineno-64-14></a>        <span class=n>lr_scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-64-15><a id=__codelineno-64-15 name=__codelineno-64-15 href=#__codelineno-64-15></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-64-16><a id=__codelineno-64-16 name=__codelineno-64-16 href=#__codelineno-64-16></a>        <span class=n>progress_bar</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>100%|██████████| 1377/1377 [01:17&lt;00:00, 14.10it/s]
</span></code></pre></div></p> <div class="admonition tip"> <p class=admonition-title>モダン訓練最適化</p> <p>訓練ループをさらに効率的にするために、以下を検討してください：</p> <ul> <li><strong>勾配クリッピング</strong>: <code>optimizer.step()</code>の前に<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code>を追加</li> <li><strong>混合精度</strong>: より高速な訓練のために<code>torch.cuda.amp.autocast()</code>と<code>GradScaler</code>を使用</li> <li><strong>勾配累積</strong>: より大きなバッチサイズをシミュレートするために複数のバッチで勾配を累積</li> <li><strong>チェックポイント</strong>: 訓練が中断された場合に再開できるよう定期的にモデルチェックポイントを保存</li> </ul> </div> <p>ご覧のとおり、訓練ループの核心部分は導入部のものとよく似ています。レポートを求めていないので、この訓練ループはモデルの調子について何も教えてくれません。そのためには評価ループを追加する必要があります。</p> <h3 id=_14>評価ループ<a class=headerlink href=#_14 title="Permanent link">&para;</a></h3> <p>前に行ったように、Hugging Face Evaluateライブラリが提供するメトリックを使用します。すでに<code>metric.compute()</code>メソッドを見ましたが、メトリックは実際に予測ループを進める際に<code>add_batch()</code>メソッドでバッチを累積できます。すべてのバッチを累積したら、<code>metric.compute()</code>で最終結果を取得できます。評価ループでこのすべてを実装する方法は次のとおりです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=kn>import</span><span class=w> </span><span class=nn>evaluate</span>
</span><span id=__span-66-2><a id=__codelineno-66-2 name=__codelineno-66-2 href=#__codelineno-66-2></a>
</span><span id=__span-66-3><a id=__codelineno-66-3 name=__codelineno-66-3 href=#__codelineno-66-3></a><span class=n>metric</span> <span class=o>=</span> <span class=n>evaluate</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&quot;glue&quot;</span><span class=p>,</span> <span class=s2>&quot;mrpc&quot;</span><span class=p>)</span>
</span><span id=__span-66-4><a id=__codelineno-66-4 name=__codelineno-66-4 href=#__codelineno-66-4></a><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span><span id=__span-66-5><a id=__codelineno-66-5 name=__codelineno-66-5 href=#__codelineno-66-5></a><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>eval_dataloader</span><span class=p>:</span>
</span><span id=__span-66-6><a id=__codelineno-66-6 name=__codelineno-66-6 href=#__codelineno-66-6></a>    <span class=n>batch</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>batch</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span><span id=__span-66-7><a id=__codelineno-66-7 name=__codelineno-66-7 href=#__codelineno-66-7></a>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-66-8><a id=__codelineno-66-8 name=__codelineno-66-8 href=#__codelineno-66-8></a>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-66-9><a id=__codelineno-66-9 name=__codelineno-66-9 href=#__codelineno-66-9></a>    <span class=n>logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span>
</span><span id=__span-66-10><a id=__codelineno-66-10 name=__codelineno-66-10 href=#__codelineno-66-10></a>    <span class=n>predictions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-66-11><a id=__codelineno-66-11 name=__codelineno-66-11 href=#__codelineno-66-11></a>    <span class=n>metric</span><span class=o>.</span><span class=n>add_batch</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>predictions</span><span class=p>,</span> <span class=n>references</span><span class=o>=</span><span class=n>batch</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>])</span>
</span><span id=__span-66-12><a id=__codelineno-66-12 name=__codelineno-66-12 href=#__codelineno-66-12></a>
</span><span id=__span-66-13><a id=__codelineno-66-13 name=__codelineno-66-13 href=#__codelineno-66-13></a><span class=n>metric</span><span class=o>.</span><span class=n>compute</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a>{&#39;accuracy&#39;: 0.8578431372549019, &#39;f1&#39;: 0.9006849315068494}
</span></code></pre></div></p> <h3 id=hugging-face-accelerate>Hugging Face Accelerateで訓練ループを強化<a class=headerlink href=#hugging-face-accelerate title="Permanent link">&para;</a></h3> <p>前に定義した訓練ループは、単一のCPUまたはGPUで正常に機能します。しかし、<a href=https://github.com/huggingface/accelerate>Hugging Face Accelerate</a>ライブラリを使用すると、わずかな調整で複数のGPUまたはTPUでの分散訓練を有効にできます。Hugging Face Accelerateは、分散訓練、混合精度、およびデバイス配置の複雑さを自動的に処理します。訓練と検証データローダーの作成から始めて、手動訓練ループがどのようになるかを以下に示します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=kn>from</span><span class=w> </span><span class=nn>accelerate</span><span class=w> </span><span class=kn>import</span> <span class=n>Accelerator</span>
</span><span id=__span-68-2><a id=__codelineno-68-2 name=__codelineno-68-2 href=#__codelineno-68-2></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=kn>import</span> <span class=n>AdamW</span>
</span><span id=__span-68-3><a id=__codelineno-68-3 name=__codelineno-68-3 href=#__codelineno-68-3></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span><span class=p>,</span> <span class=n>get_scheduler</span>
</span><span id=__span-68-4><a id=__codelineno-68-4 name=__codelineno-68-4 href=#__codelineno-68-4></a>
</span><span id=__span-68-5><a id=__codelineno-68-5 name=__codelineno-68-5 href=#__codelineno-68-5></a><span class=n>accelerator</span> <span class=o>=</span> <span class=n>Accelerator</span><span class=p>()</span>
</span><span id=__span-68-6><a id=__codelineno-68-6 name=__codelineno-68-6 href=#__codelineno-68-6></a>
</span><span id=__span-68-7><a id=__codelineno-68-7 name=__codelineno-68-7 href=#__codelineno-68-7></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-68-8><a id=__codelineno-68-8 name=__codelineno-68-8 href=#__codelineno-68-8></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>3e-5</span><span class=p>)</span>
</span><span id=__span-68-9><a id=__codelineno-68-9 name=__codelineno-68-9 href=#__codelineno-68-9></a>
</span><span id=__span-68-10><a id=__codelineno-68-10 name=__codelineno-68-10 href=#__codelineno-68-10></a><span class=n>train_dl</span><span class=p>,</span> <span class=n>eval_dl</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>accelerator</span><span class=o>.</span><span class=n>prepare</span><span class=p>(</span>
</span><span id=__span-68-11><a id=__codelineno-68-11 name=__codelineno-68-11 href=#__codelineno-68-11></a>    <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>eval_dataloader</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span>
</span><span id=__span-68-12><a id=__codelineno-68-12 name=__codelineno-68-12 href=#__codelineno-68-12></a><span class=p>)</span>
</span><span id=__span-68-13><a id=__codelineno-68-13 name=__codelineno-68-13 href=#__codelineno-68-13></a>
</span><span id=__span-68-14><a id=__codelineno-68-14 name=__codelineno-68-14 href=#__codelineno-68-14></a><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>3</span>
</span><span id=__span-68-15><a id=__codelineno-68-15 name=__codelineno-68-15 href=#__codelineno-68-15></a><span class=n>num_training_steps</span> <span class=o>=</span> <span class=n>num_epochs</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dl</span><span class=p>)</span>
</span><span id=__span-68-16><a id=__codelineno-68-16 name=__codelineno-68-16 href=#__codelineno-68-16></a><span class=n>lr_scheduler</span> <span class=o>=</span> <span class=n>get_scheduler</span><span class=p>(</span>
</span><span id=__span-68-17><a id=__codelineno-68-17 name=__codelineno-68-17 href=#__codelineno-68-17></a>    <span class=s2>&quot;linear&quot;</span><span class=p>,</span>
</span><span id=__span-68-18><a id=__codelineno-68-18 name=__codelineno-68-18 href=#__codelineno-68-18></a>    <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-68-19><a id=__codelineno-68-19 name=__codelineno-68-19 href=#__codelineno-68-19></a>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span><span id=__span-68-20><a id=__codelineno-68-20 name=__codelineno-68-20 href=#__codelineno-68-20></a>    <span class=n>num_training_steps</span><span class=o>=</span><span class=n>num_training_steps</span><span class=p>,</span>
</span><span id=__span-68-21><a id=__codelineno-68-21 name=__codelineno-68-21 href=#__codelineno-68-21></a><span class=p>)</span>
</span><span id=__span-68-22><a id=__codelineno-68-22 name=__codelineno-68-22 href=#__codelineno-68-22></a>
</span><span id=__span-68-23><a id=__codelineno-68-23 name=__codelineno-68-23 href=#__codelineno-68-23></a><span class=n>progress_bar</span> <span class=o>=</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_training_steps</span><span class=p>))</span>
</span><span id=__span-68-24><a id=__codelineno-68-24 name=__codelineno-68-24 href=#__codelineno-68-24></a>
</span><span id=__span-68-25><a id=__codelineno-68-25 name=__codelineno-68-25 href=#__codelineno-68-25></a><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-68-26><a id=__codelineno-68-26 name=__codelineno-68-26 href=#__codelineno-68-26></a><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span><span id=__span-68-27><a id=__codelineno-68-27 name=__codelineno-68-27 href=#__codelineno-68-27></a>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_dl</span><span class=p>:</span>
</span><span id=__span-68-28><a id=__codelineno-68-28 name=__codelineno-68-28 href=#__codelineno-68-28></a>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-68-29><a id=__codelineno-68-29 name=__codelineno-68-29 href=#__codelineno-68-29></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span><span id=__span-68-30><a id=__codelineno-68-30 name=__codelineno-68-30 href=#__codelineno-68-30></a>        <span class=n>accelerator</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-68-31><a id=__codelineno-68-31 name=__codelineno-68-31 href=#__codelineno-68-31></a>
</span><span id=__span-68-32><a id=__codelineno-68-32 name=__codelineno-68-32 href=#__codelineno-68-32></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-68-33><a id=__codelineno-68-33 name=__codelineno-68-33 href=#__codelineno-68-33></a>        <span class=n>lr_scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-68-34><a id=__codelineno-68-34 name=__codelineno-68-34 href=#__codelineno-68-34></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-68-35><a id=__codelineno-68-35 name=__codelineno-68-35 href=#__codelineno-68-35></a>        <span class=n>progress_bar</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <p>追加する最初の行はインポート行です。2番目の行は環境を調べて適切な分散セットアップを初期化する<code>Accelerator</code>オブジェクトをインスタンス化します。Hugging Face Accelerateがデバイス配置を処理するため、モデルをデバイスに配置する行を削除できます（または、好む場合は<code>device</code>の代わりに<code>accelerator.device</code>を使用するように変更できます）。</p> <p>その後、主な作業は、データローダー、モデル、オプティマイザーを<code>accelerator.prepare()</code>に送る行で行われます。これにより、分散訓練が意図したとおりに機能するように、これらのオブジェクトが適切なコンテナーにラップされます。残りの変更は、バッチを<code>device</code>に配置する行を削除すること（再び、これを保持したい場合は<code>accelerator.device</code>を使用するように変更するだけです）と、<code>loss.backward()</code>を<code>accelerator.backward(loss)</code>に置き換えることです。</p> <p>これを<code>train.py</code>スクリプトに置くと、あらゆる種類の分散セットアップで実行可能なスクリプトになります。分散セットアップで試すには、次のコマンドを実行します：</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a>accelerate<span class=w> </span>config
</span></code></pre></div> <p>これにより、いくつかの質問に答えるよう求められ、以下のコマンドで使用される設定ファイルに回答がダンプされます：</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a>accelerate<span class=w> </span>launch<span class=w> </span>train.py
</span></code></pre></div> <h3 id=_15>次のステップとベストプラクティス<a class=headerlink href=#_15 title="Permanent link">&para;</a></h3> <p>これで一から訓練を実装する方法を学んだので、本番使用のための追加の考慮事項をいくつか示します：</p> <p><strong>モデル評価</strong>: 精度だけでなく、複数のメトリックでモデルを常に評価してください。包括的な評価にはHugging Face Evaluateライブラリを使用してください。</p> <p><strong>ハイパーパラメータチューニング</strong>: 体系的なハイパーパラメータ最適化にはOptunaやRay Tuneなどのライブラリの使用を検討してください。</p> <p><strong>モデル監視</strong>: 訓練中は訓練メトリック、学習曲線、検証パフォーマンスを追跡してください。</p> <p><strong>モデル共有</strong>: 訓練が完了したら、コミュニティで利用できるようにHugging Face Hubでモデルを共有してください。</p> <p><strong>効率性</strong>: 大きなモデルについては、勾配チェックポイント、パラメータ効率的ファインチューニング（LoRA、AdaLoRA）、または量子化手法などの技術を検討してください。</p> <p>これでカスタム訓練ループを使用したファインチューニングの詳細な解説を終了します。ここで学んだスキルは、訓練プロセスを完全に制御する必要がある場合や、<code>Trainer</code> APIが提供するものを超えたカスタム訓練ロジックを実装したい場合に役立ちます。</p> <h2 id=_16>学習曲線の理解<a class=headerlink href=#_16 title="Permanent link">&para;</a></h2> <p><code>Trainer</code> APIとカスタム訓練ループの両方を使用してファインチューニングを実装する方法を学んだので、結果をどう解釈するかを理解することが重要です。学習曲線は、訓練中のモデルのパフォーマンスを評価し、パフォーマンスを低下させる前に潜在的な問題を特定するのに非常に有用なツールです。</p> <p>このセクションでは、精度と損失曲線の読み方と解釈方法、異なる曲線の形状がモデルの動作について何を教えてくれるか、一般的な訓練問題にどう対処するかを探ります。</p> <h3 id=_17>学習曲線とは何か？<a class=headerlink href=#_17 title="Permanent link">&para;</a></h3> <p>学習曲線は、訓練中の時間経過によるモデルのパフォーマンスメトリックの視覚的表現です。監視すべき最も重要な2つの曲線は：</p> <ul> <li><strong>損失曲線</strong>: 訓練ステップやエポックにわたってモデルの誤差（損失）がどう変化するかを示す</li> <li><strong>精度曲線</strong>: 訓練ステップやエポックにわたる正解予測の割合を示す</li> </ul> <p>これらの曲線は、モデルが効果的に学習しているかどうかを理解するのに役立ち、パフォーマンスを向上させるための調整を行う指針となります。Transformersでは、これらのメトリックは各バッチで個別に計算され、ディスクにログされます。その後、<a href=https://wandb.ai/ >Weights &amp; Biases</a>などのライブラリを使用してこれらの曲線を視覚化し、時間経過によるモデルのパフォーマンスを追跡できます。</p> <h4 id=_18>損失曲線<a class=headerlink href=#_18 title="Permanent link">&para;</a></h4> <p>損失曲線は、時間経過によるモデルの誤差の減少を示します。典型的な成功した訓練実行では、以下のような曲線が見られます：</p> <p><img alt=損失曲線 src=../03_fine_tuning_a_pretrained_model_files/1.png></p> <ul> <li><strong>高い初期損失</strong>: モデルは最適化なしで開始するため、最初は予測が悪い</li> <li><strong>損失の減少</strong>: 訓練が進むにつれて、損失は一般的に減少するはず</li> <li><strong>収束</strong>: 最終的に、損失は低い値で安定し、モデルがデータのパターンを学習したことを示す</li> </ul> <p>前の章と同様に、<code>Trainer</code> APIを使用してこれらのメトリックを追跡し、ダッシュボードで視覚化できます。以下は、Weights &amp; Biasesでこれを行う方法の例です。</p> <h4 id=_19>精度曲線<a class=headerlink href=#_19 title="Permanent link">&para;</a></h4> <p>精度曲線は、時間経過による正解予測の割合を示します。損失曲線とは異なり、精度曲線は一般的にモデルが学習するにつれて増加し、損失曲線よりも多くのステップを含むことができます。</p> <p><img alt=精度曲線 src=../03_fine_tuning_a_pretrained_model_files/2.png></p> <ul> <li><strong>低いスタート</strong>: モデルがまだデータのパターンを学習していないため、初期精度は低いはず</li> <li><strong>訓練とともに増加</strong>: モデルがパターンを学習できる場合、精度は一般的に向上するはず</li> <li><strong>プラトーを示すことがある</strong>: 精度は滑らかに増加するのではなく、しばしば離散的なジャンプで増加する</li> </ul> <div class="admonition tip"> <p class=admonition-title>精度曲線が「段階的」な理由</p> <p>損失は連続的ですが、精度は離散的な予測と真のラベルを比較することで計算されます。モデルの信頼度のわずかな改善は最終予測を変更しない可能性があり、閾値を越えるまで精度は平坦なままになります。</p> </div> <h4 id=_20>収束<a class=headerlink href=#_20 title="Permanent link">&para;</a></h4> <p>収束は、モデルのパフォーマンスが安定し、損失と精度曲線が平坦になったときに発生します。これは、モデルがデータのパターンを学習し、使用準備ができたという兆候です。簡単に言えば、訓練するたびにモデルが安定したパフォーマンスに収束することを目指しています。</p> <p><img alt=収束 src=../03_fine_tuning_a_pretrained_model_files/4.png></p> <p>モデルが収束したら、新しいデータで予測を行うために使用し、評価メトリックを参照してモデルのパフォーマンスを理解できます。</p> <h3 id=_21>学習曲線パターンの解釈<a class=headerlink href=#_21 title="Permanent link">&para;</a></h3> <p>異なる曲線の形状は、モデルの訓練の異なる側面を明らかにします。最も一般的なパターンとその意味を見てみましょう。</p> <h4 id=_22>健全な学習曲線<a class=headerlink href=#_22 title="Permanent link">&para;</a></h4> <p>適切に動作している訓練実行は、通常以下のような曲線の形状を示します：</p> <p><img alt=健全な損失曲線 src=../03_fine_tuning_a_pretrained_model_files/5.png></p> <p>上の図を見てみましょう。損失曲線（左）と対応する精度曲線（右）の両方が表示されています。これらの曲線には明確な特徴があります。</p> <p>損失曲線は、時間経過によるモデルの損失値を示しています。最初は損失が高く、その後徐々に減少し、モデルが改善していることを示します。損失値の減少は、予測された出力と真の出力の間の誤差を表すため、モデルがより良い予測を行っていることを示唆します。</p> <p>次に精度曲線に注目しましょう。これは時間経過によるモデルの精度を表しています。精度曲線は低い値から始まり、訓練が進むにつれて増加します。精度は正しく分類されたインスタンスの割合を測定します。したがって、精度曲線が上昇するにつれて、モデルがより多くの正しい予測を行っていることを表します。</p> <p>曲線の滑らかさと精度曲線の「プラトー」の存在は1つの注目すべき違いです。損失は滑らかに減少する一方、精度曲線のプラトーは連続的な増加ではなく離散的なジャンプを示します。この動作は精度の測定方法によるものです。モデルの出力がターゲットに近づくと損失は改善できますが、最終予測がまだ正しくない場合があります。しかし、精度は予測が正しくなる閾値を越えた場合にのみ改善し、プラトーを作ります。</p> <p>例えば、猫（0）と犬（1）を区別するバイナリ分類器で、モデルが犬の画像（真値1）に対して0.3を予測した場合、これは0に丸められ、正しくない分類になります。次のステップで0.4を予測した場合、まだ正しくありません。0.4は0.3よりも1に近いため損失は減少しますが、精度は変わらず、プラトーを作ります。精度は、モデルが1に丸められる0.5より大きな値を予測した場合にのみジャンプします。</p> <div class="admonition tip"> <p class=admonition-title>健全な曲線の特徴</p> <ul> <li><strong>損失の滑らかな減少</strong>: 訓練と検証の両方の損失が着実に減少</li> <li><strong>訓練/検証パフォーマンスの近接</strong>: 訓練と検証メトリック間の小さなギャップ</li> <li><strong>収束</strong>: 曲線が平坦になり、モデルがパターンを学習したことを示す</li> </ul> </div> <h4 id=_23>実践例<a class=headerlink href=#_23 title="Permanent link">&para;</a></h4> <p>学習曲線の実践例をいくつか見てみましょう。まず、訓練中に学習曲線を監視するアプローチをいくつか取り上げます。以下では、学習曲線で観察できる異なるパターンを分析します。</p> <h5 id=_24>訓練中<a class=headerlink href=#_24 title="Permanent link">&para;</a></h5> <p>訓練プロセス中（<code>trainer.train()</code>を実行した後）、これらの主要指標を監視できます：</p> <ol> <li><strong>損失収束</strong>: 損失がまだ減少しているか、プラトーに達したか？</li> <li><strong>過学習の兆候</strong>: 訓練損失が減少している間に検証損失が増加し始めているか？</li> <li><strong>学習率</strong>: 曲線が過度に不安定（LRが高すぎる）または平坦すぎる（LRが低すぎる）か？</li> <li><strong>安定性</strong>: 問題を示す突然のスパイクや下降があるか？</li> </ol> <h5 id=_25>訓練後<a class=headerlink href=#_25 title="Permanent link">&para;</a></h5> <p>訓練プロセスが完了した後、完全な曲線を分析してモデルのパフォーマンスを理解できます。</p> <ol> <li><strong>最終パフォーマンス</strong>: モデルは許容可能なパフォーマンスレベルに達したか？</li> <li><strong>効率性</strong>: より少ないエポックで同じパフォーマンスを達成できたか？</li> <li><strong>汎化</strong>: 訓練と検証のパフォーマンスはどの程度近いか？</li> <li><strong>傾向</strong>: 追加の訓練がパフォーマンスを改善する可能性があるか？</li> </ol> <div class="admonition tip"> <p class=admonition-title>W&amp;Bダッシュボード機能</p> <p>Weights &amp; Biasesは学習曲線の美しいインタラクティブなプロットを自動的に作成します。以下ができます：</p> <ul> <li>複数の実行を並べて比較</li> <li>カスタムメトリックと視覚化を追加</li> <li>異常な動作のアラートを設定</li> <li>チームと結果を共有</li> </ul> </div> <h5 id=_26>過学習<a class=headerlink href=#_26 title="Permanent link">&para;</a></h5> <p>過学習は、モデルが訓練データから過度に学習し、異なるデータ（検証セットで表される）に汎化できない場合に発生します。</p> <p><img alt=過学習 src=../03_fine_tuning_a_pretrained_model_files/10.png></p> <p><strong>症状:</strong></p> <ul> <li>検証損失が増加またはプラトーしている間に訓練損失が減少し続ける</li> <li>訓練と検証精度の大きなギャップ</li> <li>検証精度よりもはるかに高い訓練精度</li> </ul> <p><strong>過学習の解決策:</strong></p> <ul> <li><strong>正則化</strong>: ドロップアウト、重み減衰、またはその他の正則化技術を追加</li> <li><strong>早期停止</strong>: 検証パフォーマンスの改善が止まったときに訓練を停止</li> <li><strong>データ拡張</strong>: 訓練データの多様性を増加</li> <li><strong>モデル複雑度の削減</strong>: より小さなモデルまたはより少ないパラメータを使用</li> </ul> <p>以下のサンプルでは、過学習を防ぐために早期停止を使用します。<code>early_stopping_patience</code>を3に設定し、検証損失が3回連続して改善しない場合、訓練が停止されます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a><span class=c1># 早期停止で過学習を検出する例</span>
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>EarlyStoppingCallback</span>
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a>
</span><span id=__span-71-4><a id=__codelineno-71-4 name=__codelineno-71-4 href=#__codelineno-71-4></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-71-5><a id=__codelineno-71-5 name=__codelineno-71-5 href=#__codelineno-71-5></a>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&quot;./results&quot;</span><span class=p>,</span>
</span><span id=__span-71-6><a id=__codelineno-71-6 name=__codelineno-71-6 href=#__codelineno-71-6></a>    <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;steps&quot;</span><span class=p>,</span>
</span><span id=__span-71-7><a id=__codelineno-71-7 name=__codelineno-71-7 href=#__codelineno-71-7></a>    <span class=n>eval_steps</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-71-8><a id=__codelineno-71-8 name=__codelineno-71-8 href=#__codelineno-71-8></a>    <span class=n>save_strategy</span><span class=o>=</span><span class=s2>&quot;steps&quot;</span><span class=p>,</span>
</span><span id=__span-71-9><a id=__codelineno-71-9 name=__codelineno-71-9 href=#__codelineno-71-9></a>    <span class=n>save_steps</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-71-10><a id=__codelineno-71-10 name=__codelineno-71-10 href=#__codelineno-71-10></a>    <span class=n>load_best_model_at_end</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-71-11><a id=__codelineno-71-11 name=__codelineno-71-11 href=#__codelineno-71-11></a>    <span class=n>metric_for_best_model</span><span class=o>=</span><span class=s2>&quot;eval_loss&quot;</span><span class=p>,</span>
</span><span id=__span-71-12><a id=__codelineno-71-12 name=__codelineno-71-12 href=#__codelineno-71-12></a>    <span class=n>greater_is_better</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span><span id=__span-71-13><a id=__codelineno-71-13 name=__codelineno-71-13 href=#__codelineno-71-13></a>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>  <span class=c1># 高く設定するが、早期に停止</span>
</span><span id=__span-71-14><a id=__codelineno-71-14 name=__codelineno-71-14 href=#__codelineno-71-14></a><span class=p>)</span>
</span><span id=__span-71-15><a id=__codelineno-71-15 name=__codelineno-71-15 href=#__codelineno-71-15></a>
</span><span id=__span-71-16><a id=__codelineno-71-16 name=__codelineno-71-16 href=#__codelineno-71-16></a><span class=c1># 過学習を防ぐために早期停止を追加</span>
</span><span id=__span-71-17><a id=__codelineno-71-17 name=__codelineno-71-17 href=#__codelineno-71-17></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span><span id=__span-71-18><a id=__codelineno-71-18 name=__codelineno-71-18 href=#__codelineno-71-18></a>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-71-19><a id=__codelineno-71-19 name=__codelineno-71-19 href=#__codelineno-71-19></a>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-71-20><a id=__codelineno-71-20 name=__codelineno-71-20 href=#__codelineno-71-20></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span>
</span><span id=__span-71-21><a id=__codelineno-71-21 name=__codelineno-71-21 href=#__codelineno-71-21></a>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;validation&quot;</span><span class=p>],</span>
</span><span id=__span-71-22><a id=__codelineno-71-22 name=__codelineno-71-22 href=#__codelineno-71-22></a>    <span class=n>data_collator</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span><span id=__span-71-23><a id=__codelineno-71-23 name=__codelineno-71-23 href=#__codelineno-71-23></a>    <span class=n>processing_class</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-71-24><a id=__codelineno-71-24 name=__codelineno-71-24 href=#__codelineno-71-24></a>    <span class=n>compute_metrics</span><span class=o>=</span><span class=n>compute_metrics</span><span class=p>,</span>
</span><span id=__span-71-25><a id=__codelineno-71-25 name=__codelineno-71-25 href=#__codelineno-71-25></a>    <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>EarlyStoppingCallback</span><span class=p>(</span><span class=n>early_stopping_patience</span><span class=o>=</span><span class=mi>3</span><span class=p>)],</span>
</span><span id=__span-71-26><a id=__codelineno-71-26 name=__codelineno-71-26 href=#__codelineno-71-26></a><span class=p>)</span>
</span></code></pre></div> <h5 id=_27>学習不足<a class=headerlink href=#_27 title="Permanent link">&para;</a></h5> <p>学習不足は、モデルがデータの基礎パターンを捉えるには単純すぎる場合に発生します。これはいくつかの理由で起こる可能性があります：</p> <ul> <li>モデルが小さすぎるか、パターンを学習する能力が不足している</li> <li>学習率が低すぎて、学習が遅い</li> <li>データセットが小さすぎるか、問題を代表していない</li> <li>モデルが適切に正則化されていない</li> </ul> <p><img alt=学習不足 src=../03_fine_tuning_a_pretrained_model_files/7.png></p> <p><strong>症状:</strong></p> <ul> <li>訓練と検証の両方の損失が高いまま</li> <li>モデルパフォーマンスが訓練の早期にプラトーに達する</li> <li>期待よりも低い訓練精度</li> </ul> <p><strong>学習不足の解決策:</strong></p> <ul> <li><strong>モデル容量の増加</strong>: より大きなモデルまたはより多くのパラメータを使用</li> <li><strong>より長い訓練</strong>: エポック数を増加</li> <li><strong>学習率の調整</strong>: 異なる学習率を試す</li> <li><strong>データ品質の確認</strong>: データが適切に前処理されていることを確認</li> </ul> <h5 id=_28>不安定な学習曲線<a class=headerlink href=#_28 title="Permanent link">&para;</a></h5> <p>不安定な学習曲線は、モデルが効果的に学習していない場合に発生します。これはいくつかの理由で起こる可能性があります：</p> <ul> <li>学習率が高すぎて、モデルが最適パラメータを飛び越えてしまう</li> <li>バッチサイズが小さすぎて、モデルの学習が遅い</li> <li>モデルが適切に正則化されておらず、訓練データに過学習してしまう</li> <li>データセットが適切に前処理されておらず、モデルがノイズから学習してしまう</li> </ul> <p><img alt=不安定な学習曲線 src=../03_fine_tuning_a_pretrained_model_files/3.png></p> <p><strong>症状:</strong> - 損失や精度の頻繁な変動 - 曲線が高い分散や不安定性を示す - 明確な傾向のないパフォーマンスの振動</p> <p>訓練と検証の両方の曲線が不安定な動作を示します。</p> <p><img alt=不安定な学習曲線2 src=../03_fine_tuning_a_pretrained_model_files/9.png></p> <p><strong>不安定な曲線の解決策:</strong></p> <ul> <li><strong>学習率の低下</strong>: より安定した訓練のためにステップサイズを減少</li> <li><strong>バッチサイズの増加</strong>: より大きなバッチはより安定した勾配を提供</li> <li><strong>勾配クリッピング</strong>: 勾配爆発を防ぐ</li> <li><strong>より良いデータ前処理</strong>: 一貫したデータ品質を確保</li> </ul> <h3 id=_29>重要なポイント<a class=headerlink href=#_29 title="Permanent link">&para;</a></h3> <p>学習曲線の理解は、効果的な機械学習実践者になるために重要です。これらの視覚的ツールは、モデルの訓練進捗に関する即座のフィードバックを提供し、訓練をいつ停止するか、ハイパーパラメータを調整するか、異なるアプローチを試すかについて情報に基づいた決定を下すのに役立ちます。練習により、健全な学習曲線がどのように見えるか、問題が生じたときにそれらにどう対処するかについて直感的な理解を深めることができます。</p> <div class="admonition tip"> <p class=admonition-title>重要なポイント</p> <ul> <li>学習曲線はモデル訓練進捗を理解するための必須ツールです</li> <li>損失と精度曲線の両方を監視しますが、それらには異なる特徴があることを覚えておいてください</li> <li>過学習は訓練/検証パフォーマンスの乖離として現れます</li> <li>学習不足は訓練と検証データの両方で低いパフォーマンスとして現れます</li> <li>Weights &amp; Biasesのようなツールは学習曲線の追跡と分析を容易にします</li> <li>早期停止と適切な正則化はほとんどの一般的な訓練問題に対処できます</li> </ul> <p>🔬 <strong>次のステップ</strong>: 独自のファインチューニング実験で学習曲線の分析を練習してください。異なるハイパーパラメータを試し、それらが曲線の形状にどう影響するかを観察してください。この実践経験は、訓練進捗を読む直感を養う最良の方法です。</p> </div> <h2 id=_30>まとめ<a class=headerlink href=#_30 title="Permanent link">&para;</a></h2> <p>この記事では、Hugging Face Transformersエコシステムを使用した事前訓練済みモデルのファインチューニングについて包括的に学習しました。データ処理から高レベルなTrainer APIの使用、カスタム訓練ループの実装、学習曲線の解釈まで、現代の機械学習で必要なスキルを習得しました。</p> <p>学習したポイントを振り返ると、効率的なデータ処理技術、動的パディングの重要性、Trainer APIの強力な機能、分散訓練のためのAccelerateライブラリの活用方法を理解しました。また、訓練過程を監視し、過学習や学習不足などの一般的な問題を特定・解決する方法も学びました。</p> <h2 id=_31>参考資料<a class=headerlink href=#_31 title="Permanent link">&para;</a></h2> <ul> <li><a href=https://huggingface.co/docs/transformers/ >Hugging Face Transformers Documentation</a></li> <li><a href=https://huggingface.co/docs/datasets/ >Datasets Library Documentation</a></li> <li><a href=https://huggingface.co/docs/accelerate/ >Accelerate Library Documentation</a></li> <li><a href=https://gluebenchmark.com/ >GLUE Benchmark</a></li> <li><a href=https://wandb.ai/ >Weights &amp; Biases</a></li> <li><a href=https://arxiv.org/pdf/1810.04805.pdf>BERT Paper</a></li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/meta.js></script> <script src=../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>