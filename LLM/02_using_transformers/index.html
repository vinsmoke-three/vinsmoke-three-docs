<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Hugging Face Transformersライブラリを使用してTransformerモデルを活用する方法を学びます。pipelineの仕組み、モデルの読み込み、トークナイザーの使用方法、複数シーケンスの処理まで詳しく解説します。"><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/LLM/02_using_transformers/ rel=canonical><link href=../01_transformer_models/ rel=prev><link href=../03_fine_tuning_a_pretrained_model/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>Transformersライブラリの使い方 - モデルとトークナイザーの基本 - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#transformers class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Transformersライブラリの使い方 - モデルとトークナイザーの基本 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PyTorch/00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../PyTorch/00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/07_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/09_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2. Using transformers </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2. Using transformers </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> はじめに </span> </a> </li> <li class=md-nav__item> <a href=#pipeline class=md-nav__link> <span class=md-ellipsis> pipelineの仕組み </span> </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> モデル </span> </a> </li> <li class=md-nav__item> <a href=#_17 class=md-nav__link> <span class=md-ellipsis> トークナイザー </span> </a> </li> <li class=md-nav__item> <a href=#_25 class=md-nav__link> <span class=md-ellipsis> 複数シーケンスの処理 </span> </a> </li> <li class=md-nav__item> <a href=#_29 class=md-nav__link> <span class=md-ellipsis> すべてをまとめる </span> </a> </li> <li class=md-nav__item> <a href=#_32 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_34 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../03_fine_tuning_a_pretrained_model/ class=md-nav__link> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> </li> <li class=md-nav__item> <a href=../04_the_huggingface_tokenizers_library/ class=md-nav__link> <span class=md-ellipsis> 4. Tokenizers library </span> </a> </li> <li class=md-nav__item> <a href=../05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ClassicalNLP/71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/72_masked_language_modeling/ class=md-nav__link> <span class=md-ellipsis> Masked Language Modeling </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> はじめに </span> </a> </li> <li class=md-nav__item> <a href=#pipeline class=md-nav__link> <span class=md-ellipsis> pipelineの仕組み </span> </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> モデル </span> </a> </li> <li class=md-nav__item> <a href=#_17 class=md-nav__link> <span class=md-ellipsis> トークナイザー </span> </a> </li> <li class=md-nav__item> <a href=#_25 class=md-nav__link> <span class=md-ellipsis> 複数シーケンスの処理 </span> </a> </li> <li class=md-nav__item> <a href=#_29 class=md-nav__link> <span class=md-ellipsis> すべてをまとめる </span> </a> </li> <li class=md-nav__item> <a href=#_32 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> <li class=md-nav__item> <a href=#_34 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=transformers>Transformersライブラリの使い方<a class=headerlink href=#transformers title="Permanent link">&para;</a></h1> <h2 id=_1>概要<a class=headerlink href=#_1 title="Permanent link">&para;</a></h2> <p>本記事では、Hugging Face Transformersライブラリの基本的な使用方法について詳しく解説します。Transformerモデルの基本概念から、実際にモデルとトークナイザーを使用してテキスト分類を行う方法まで、実践的な内容を学習できます。</p> <div class="admonition info"> <p class=admonition-title>参考資料</p> <p>本ドキュメントは <a href=https://huggingface.co/learn/llm-course/chapter2/1>Hugging Face LLM Course</a> を参考に、日本語で学習内容をまとめた個人的な学習ノートです。詳細な内容や最新情報については、原文も併せてご参照ください。</p> </div> <p><strong>学習目標:</strong> - Transformersライブラリの基本的な使い方を理解する - pipelineの仕組みと内部動作を把握する - モデルとトークナイザーの操作方法を習得する - 複数のテキストシーケンスを効率的に処理する方法を学ぶ</p> <h2 id=_2>前提知識<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <ul> <li>Python プログラミングの基本知識</li> <li>機械学習・深層学習の基礎概念</li> <li>PyTorchの基本的な使用経験（推奨）</li> <li>自然言語処理（NLP）の基本概念</li> </ul> <h2 id=_3>はじめに<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <p>Transformerモデルは一般的に非常に大きく、数百万から数百億のパラメータを持ちます。これらのモデルの訓練と展開は複雑な作業です。さらに、ほぼ毎日新しいモデルがリリースされ、それぞれが独自の実装を持っているため、すべてを試すのは簡単な作業ではありません。</p> <p>Transformersライブラリは、この問題を解決するために作成されました。その目標は、任意のTransformerモデルを読み込み、訓練し、保存できる単一のAPIを提供することです。ライブラリの主な特徴は以下の通りです：</p> <ul> <li><strong>使いやすさ</strong>: 最先端のNLPモデルをダウンロード、読み込み、推論に使用することが、わずか2行のコードで可能</li> <li><strong>柔軟性</strong>: すべてのモデルは、その中核でシンプルなPyTorchの<code>nn.Module</code>クラスであり、それぞれの機械学習（ML）フレームワークで他のモデルと同様に扱うことが可能</li> <li><strong>シンプルさ</strong>: ライブラリ全体でほとんど抽象化が行われていない。「All in one file」が中核概念で、モデルのフォワードパスは単一のファイルで完全に定義されているため、コード自体が理解しやすく、変更しやすい</li> </ul> <p>この最後の特徴により、Transformersは他のMLライブラリとは大きく異なります。モデルは複数のファイル間で共有されるモジュールで構築されるのではなく、各モデルが独自のレイヤーを持っています。モデルをより親しみやすく理解しやすくするだけでなく、他のモデルに影響を与えることなく1つのモデルで簡単に実験できます。</p> <h2 id=pipeline>pipelineの仕組み<a class=headerlink href=#pipeline title="Permanent link">&para;</a></h2> <p>まず、完全な例から始めて、第1章で以下のコードを実行したときに裏で何が起こったかを見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&quot;sentiment-analysis&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;</span><span class=p>)</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>classifier</span><span class=p>(</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>    <span class=p>[</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>        <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>        <span class=s2>&quot;I hate this so much!&quot;</span><span class=p>,</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>    <span class=p>]</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598050713539124},
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a> {&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9994558691978455}]
</span></code></pre></div></p> <p>このpipelineは3つのステップをまとめています：前処理、モデルを通じた入力の処理、後処理：</p> <p><img alt=完全なNLPパイプライン src=../02_using_transformers_files/full_nlp_pipeline.png></p> <h3 id=_4>トークナイザーによる前処理<a class=headerlink href=#_4 title="Permanent link">&para;</a></h3> <p>他のニューラルネットワークと同様に、Transformerモデルは生のテキストを直接処理できません。そのため、パイプラインの最初のステップでは、テキスト入力をモデルが理解できる数値に変換する必要があります。</p> <p>この変換処理には<em>トークナイザー</em>を使用します。トークナイザーは以下の役割を担います：</p> <ul> <li>入力を単語、サブワード、または記号（句読点など）に分割し、これらを<em>トークン</em>と呼ぶ</li> <li>各トークンを整数にマッピング</li> <li>モデルに有用な可能性のある追加の入力を追加</li> </ul> <p>この前処理は、モデルが事前訓練されたときとまったく同じ方法で行われる必要があります。そのため、まず<a href=https://huggingface.co/models>Model Hub</a>から必要な情報をダウンロードします。</p> <p>この処理には、<code>AutoTokenizer</code>クラスとその<code>from_pretrained()</code>メソッドを使用します。モデルのチェックポイント名を指定することで、モデルのトークナイザーに関連するデータを自動的に取得し、キャッシュします（初回実行時のみダウンロードが行われます）。</p> <p><code>sentiment-analysis</code> pipelineのデフォルトのチェックポイントは<code>distilbert-base-uncased-finetuned-sst-2-english</code>なので、以下を実行します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span></code></pre></div> <p>トークナイザーを取得したら、文書を直接渡すことができ、モデルに供給する準備ができた辞書が返されます！残る作業は、入力IDのリストをテンソルに変換することだけです。</p> <p>Transformersライブラリは、バックエンドのMLフレームワークを意識せずに使用できます。モデルによってはPyTorchやFlaxを使用する場合がありますが、どちらの場合でも統一されたAPIで操作可能です。</p> <p>ただし、Transformerモデルは入力として<em>テンソル</em>のみを受け入れることに注意が必要です。テンソルに馴染みがない場合は、NumPy配列として考えるとわかりやすいでしょう。NumPy配列と同様に、テンソルもスカラー（0次元）、ベクトル（1次元）、行列（2次元）、またはより多くの次元を持つことができます。実際、テンソルはNumPy配列の概念を一般化したものと考えることができます。</p> <p>取得したいテンソルのタイプ（PyTorchまたはプレーンなNumPy）を指定するには、<code>return_tensors</code>引数を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=kn>from</span><span class=w> </span><span class=nn>pprint</span><span class=w> </span><span class=kn>import</span> <span class=n>pprint</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=n>raw_inputs</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>        <span class=s2>&quot;I hate this so much!&quot;</span><span class=p>,</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=p>]</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>raw_inputs</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=n>pprint</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>{&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a> &#39;input_ids&#39;: tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>          2607,  2026,  2878,  2166,  1012,   102],
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>             0,     0,     0,     0,     0,     0]])}
</span></code></pre></div></p> <p>出力自体は、<code>input_ids</code>と<code>attention_mask</code>の2つのキーを含む辞書です。<code>input_ids</code>には、各文のトークンの一意識別子である整数の2つの行（各文に1つずつ）が含まれています。<code>attention_mask</code>については、この章で後ほど説明します。</p> <h3 id=_5>モデルを通じた処理<a class=headerlink href=#_5 title="Permanent link">&para;</a></h3> <p>トークナイザーと同じ方法で事前訓練済みモデルをダウンロードできます。Transformersは、<code>from_pretrained()</code>メソッドも持つ<code>AutoModel</code>クラスを提供しています：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModel</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span></code></pre></div> <p>このコードスニペットでは、パイプラインで以前に使用したのと同じチェックポイントをダウンロードし（実際にはすでにキャッシュされているはずです）、それでモデルをインスタンス化しました。</p> <p>このアーキテクチャには、基本的なTransformerモジュールのみが含まれています。一部の入力が与えられると、<em>隠れ状態</em>、<em>特徴</em>とも呼ばれるものを出力します。各モデル入力に対して、<strong>Transformerモデルによるその入力の文脈的理解</strong>を表す高次元ベクトルを取得します。</p> <p>これが理解できない場合でも心配しないでください。後ですべて説明します。</p> <p>これらの隠れ状態は単独でも有用ですが、通常は<em>ヘッド</em>として知られるモデルの別の部分への入力です。第1章では、同じアーキテクチャで異なるタスクを実行できましたが、これらの各タスクには異なるヘッドが関連付けられています。</p> <h4 id=_6>高次元ベクトル？<a class=headerlink href=#_6 title="Permanent link">&para;</a></h4> <p>Transformerモジュールによって出力されるベクトルは通常大きくなります。一般的に3つの次元があります：</p> <ul> <li><strong>バッチサイズ</strong>: 一度に処理されるシーケンスの数（例では2）</li> <li><strong>シーケンス長</strong>: シーケンスの数値表現の長さ（例では16）</li> <li><strong>隠れサイズ</strong>: 各モデル入力のベクトル次元</li> </ul> <p>最後の値のために「高次元」と言われます。隠れサイズは非常に大きくなることがあります（小さなモデルでは768が一般的で、大きなモデルでは3072以上に達することもあります）。</p> <p>前処理した入力をモデルに与えると、これを確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>torch.Size([2, 16, 768])
</span></code></pre></div></p> <p>Transformersモデルの出力は<code>namedtuple</code>や辞書のように動作することに注意してください。属性（私たちが行ったように）、キー（<code>outputs["last_hidden_state"]</code>）、または探しているものの正確な位置を知っている場合はインデックス（<code>outputs[0]</code>）で要素にアクセスできます。</p> <h4 id=_7>モデルヘッド：数値を意味のあるものにする<a class=headerlink href=#_7 title="Permanent link">&para;</a></h4> <p>モデルヘッドは、隠れ状態の高次元ベクトルを入力として受け取り、それらを異なる次元に投影します。通常、1つまたは少数の線形レイヤーで構成されています：</p> <p><img alt=Transformerネットワークとそのヘッド src=../02_using_transformers_files/transformer_and_head.png></p> <p>Transformerモデルの出力は、処理されるためにモデルヘッドに直接送られます。</p> <p>この図では、モデルは埋め込みレイヤーとそれに続くレイヤーで表されています。埋め込みレイヤーは、トークン化された入力の各入力IDを、関連するトークンを表すベクトルに変換します。後続のレイヤーは、注意機構を使用してこれらのベクトルを操作し、文の最終表現を生成します。</p> <p>Transformersには多くの異なるアーキテクチャが利用可能で、それぞれが特定のタスクに取り組むように設計されています。以下は網羅的ではないリストです：</p> <ul> <li><code>*Model</code>（隠れ状態を取得）</li> <li><code>*ForCausalLM</code></li> <li><code>*ForMaskedLM</code></li> <li><code>*ForMultipleChoice</code></li> <li><code>*ForQuestionAnswering</code></li> <li><code>*ForSequenceClassification</code></li> <li><code>*ForTokenClassification</code></li> <li>その他</li> </ul> <p>この例では、シーケンス分類ヘッドを持つモデルが必要です（文書をポジティブまたはネガティブとして分類できるように）。そのため、実際には<code>AutoModel</code>クラスではなく、<code>AutoModelForSequenceClassification</code>を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <p>出力の形状を見ると、次元がはるかに低くなっています。モデルヘッドは、前に見た高次元ベクトルを入力として受け取り、2つの値（ラベルごとに1つ）を含むベクトルを出力します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>        [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>torch.Size([2, 2])
</span></code></pre></div></p> <p>2つの文書と2つのラベルがあるため、モデルから得られる結果は2×2の形状です。</p> <h3 id=_8>出力の後処理<a class=headerlink href=#_8 title="Permanent link">&para;</a></h3> <p>モデルから出力として得られる値は、それ自体では必ずしも意味をなしません。見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>tensor([[-1.5607,  1.6123],
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>        [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p>モデルが最初の文について<code>[0.0402, 0.9598]</code>、2番目の文書について<code>[0.9995, 0.0005]</code>を予測したことがわかります。これらは認識可能な確率スコアです。</p> <p>各位置に対応するラベルを取得するには、モデル設定の<code>id2label</code>属性を調べることができます（次のセクションで詳しく説明）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>{0: &#39;NEGATIVE&#39;, 1: &#39;POSITIVE&#39;}
</span></code></pre></div></p> <p>これで、モデルが以下を予測したと結論できます：</p> <ul> <li>最初の文：NEGATIVE: 0.0402、POSITIVE: 0.9598</li> <li>2番目の文：NEGATIVE: 0.9995、POSITIVE: 0.0005</li> </ul> <p>パイプラインの3つのステップ（トークナイザーによる前処理、モデルを通じた入力の処理、後処理）を正常に再現しました！それでは、これらの各ステップをより深く掘り下げてみましょう。</p> <h2 id=_9>モデル<a class=headerlink href=#_9 title="Permanent link">&para;</a></h2> <p>このセクションでは、モデルの作成と使用について詳しく見ていきます。チェックポイントから任意のモデルをインスタンス化したい場合に便利な<code>AutoModel</code>クラスを使用します。</p> <h3 id=transformer>Transformerの作成<a class=headerlink href=#transformer title="Permanent link">&para;</a></h3> <p><code>AutoModel</code>をインスタンス化するときに何が起こるかを調べることから始めましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModel</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>トークナイザーと同様に、<code>from_pretrained()</code>メソッドはHugging Face Hubからモデルデータをダウンロードしてキャッシュします。前述のように、チェックポイント名は特定のモデルアーキテクチャと重みに対応し、この場合は基本的なアーキテクチャ（12レイヤー、768隠れサイズ、12注意ヘッド）と大文字・小文字を区別する入力を持つBERTモデルです。Hubには多くのチェックポイントが利用可能です。<a href=https://huggingface.co/models>こちら</a>で探索できます。</p> <p><code>AutoModel</code>クラスとその関連クラスは、実際には指定されたチェックポイントに適したモデルアーキテクチャを取得するように設計されたシンプルなラッパーです。これは「auto」クラスで、適切なモデルアーキテクチャを推測し、正しいモデルクラスをインスタンス化します。ただし、使用したいモデルのタイプがわかっている場合は、そのアーキテクチャを定義するクラスを直接使用できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>BertModel</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>BertModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=_10>読み込みと保存<a class=headerlink href=#_10 title="Permanent link">&para;</a></h3> <p>モデルの保存は、トークナイザーの保存と同じくらい簡単です。実際、モデルには同じ<code>save_pretrained()</code>メソッドがあり、モデルの重みとアーキテクチャ設定を保存します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;models/&quot;</span><span class=p>)</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=c1># これにより、ディスクに2つのファイルが保存されます</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=o>%</span><span class=n>ls</span> <span class=n>models</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a>config.json 
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>model.safetensors
</span></code></pre></div></p> <p><em>config.json</em>ファイルを見ると、モデルアーキテクチャを構築するために必要なすべての属性が表示されます。このファイルには、チェックポイントがどこから来たのか、最後にチェックポイントを保存したときに使用していたTransformersのバージョンなどのメタデータも含まれています。</p> <p><em>model.safetensors</em>ファイルは状態辞書として知られており、モデルのすべての重みが含まれています。この2つのファイルは連携して動作します。設定ファイルはモデルアーキテクチャについて知るために必要であり、モデルの重みはモデルのパラメータです。</p> <p>保存されたモデルを再利用するには、再び<code>from_pretrained()</code>メソッドを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModel</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;models&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=_11>テキストのエンコード<a class=headerlink href=#_11 title="Permanent link">&para;</a></h3> <p>Transformerモデルは、入力を数値に変換することでテキストを処理します。ここでは、テキストがトークナイザーによって処理されるときに正確に何が起こるかを見ていきます。第1章で、トークナイザーがテキストをトークンに分割し、これらのトークンを数値に変換することをすでに見ました。簡単なトークナイザーを通してこの変換を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=kn>from</span><span class=w> </span><span class=nn>pprint</span><span class=w> </span><span class=kn>import</span> <span class=n>pprint</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&quot;Hello, I&#39;m a single sentence!&quot;</span><span class=p>)</span>
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a><span class=n>pprint</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a> &#39;input_ids&#39;: [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 1233, 106, 102],
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
</span></code></pre></div></p> <p>以下のフィールドを持つ辞書を取得します： - input_ids：トークンの数値表現 - token_type_ids：入力のどの部分が文Aで、どの部分が文Bかをモデルに伝える（次のセクションでより詳しく説明） - attention_mask：どのトークンに注意を払うべきで、どのトークンに注意を払うべきでないかを示す（すぐに詳しく説明）</p> <p>入力IDをデコードして元のテキストを取得できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a>&quot;[CLS] Hello, I &#39; m a single sentence! [SEP]&quot;
</span></code></pre></div></p> <p>トークナイザーが特別なトークン — <code>[CLS]</code>と<code>[SEP]</code> — をモデルが必要とするものとして追加したことに気づくでしょう。すべてのモデルが特別なトークンを必要とするわけではありません。これらは、モデルがそれらで事前訓練された場合に利用され、その場合、トークナイザーはモデルがこれらのトークンを期待するため、それらを追加する必要があります。</p> <p>複数の文を一度にエンコードすることもできます。バッチ化してまとめるか（これについてはすぐに説明します）、リストを渡すことで可能です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&quot;How are you?&quot;</span><span class=p>,</span> <span class=s2>&quot;I&#39;m fine, thank you!&quot;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a>{&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a> &#39;input_ids&#39;: tensor([[ 101, 1731, 1132, 1128,  136,  102,  146,  112,  182, 2503,  117, 6243,
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a>         1128,  106,  102]]),
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a> &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
</span></code></pre></div></p> <p>しかし、問題があります。2つのリストの長さが同じではありません！配列とテンソルは長方形である必要があるため、これらのリストを単純にPyTorchテンソル（またはNumPy配列）に変換することはできません。トークナイザーはそのためのオプション（パディング）を提供しています。</p> <h4 id=_12>入力のパディング<a class=headerlink href=#_12 title="Permanent link">&para;</a></h4> <p>トークナイザーに入力をパディングするよう求めると、最も長いものより短い文に特別なパディングトークンを追加することで、すべての文を同じ長さにします：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=s2>&quot;How are you?&quot;</span><span class=p>,</span> <span class=s2>&quot;I&#39;m fine, thank you!&quot;</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a>{&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
</span><span id=__span-28-2><a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a>        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),
</span><span id=__span-28-3><a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a> &#39;input_ids&#39;: tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],
</span><span id=__span-28-4><a id=__codelineno-28-4 name=__codelineno-28-4 href=#__codelineno-28-4></a>        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]),
</span><span id=__span-28-5><a id=__codelineno-28-5 name=__codelineno-28-5 href=#__codelineno-28-5></a> &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
</span><span id=__span-28-6><a id=__codelineno-28-6 name=__codelineno-28-6 href=#__codelineno-28-6></a>        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}
</span></code></pre></div></p> <p>これで長方形のテンソルができました！パディングトークンがID 0で入力IDにエンコードされ、attention maskの値も0になっていることに注意してください。これは、これらのパディングトークンがモデルによって分析されるべきではないためです。実際の文の一部ではありません。</p> <h4 id=_13>入力の切り捨て<a class=headerlink href=#_13 title="Permanent link">&para;</a></h4> <p>テンソルがモデルで処理するには大きすぎる場合があります。たとえば、BERTは最大512トークンのシーケンスでのみ事前訓練されているため、より長いシーケンスを処理できません。モデルが処理できるよりも長いシーケンスがある場合は、<code>truncation</code>パラメータで切り捨てる必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>    <span class=s2>&quot;This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.&quot;</span><span class=p>,</span>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a>    <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a><span class=p>)</span>
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a><span class=nb>print</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a>[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]
</span></code></pre></div></p> <p>パディングと切り捨ての引数を組み合わせることで、テンソルが必要な正確なサイズになることを確実にできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span>
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a>    <span class=p>[</span><span class=s2>&quot;How are you?&quot;</span><span class=p>,</span> <span class=s2>&quot;I&#39;m fine, thank you!&quot;</span><span class=p>],</span>
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>    <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-31-4><a id=__codelineno-31-4 name=__codelineno-31-4 href=#__codelineno-31-4></a>    <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-31-5><a id=__codelineno-31-5 name=__codelineno-31-5 href=#__codelineno-31-5></a>    <span class=n>max_length</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span><span id=__span-31-6><a id=__codelineno-31-6 name=__codelineno-31-6 href=#__codelineno-31-6></a>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>,</span>
</span><span id=__span-31-7><a id=__codelineno-31-7 name=__codelineno-31-7 href=#__codelineno-31-7></a><span class=p>)</span>
</span><span id=__span-31-8><a id=__codelineno-31-8 name=__codelineno-31-8 href=#__codelineno-31-8></a><span class=nb>print</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a>{&#39;input_ids&#39;: tensor([[ 101, 1731, 1132, 1128,  102],
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a>        [ 101,  146,  112,  182,  102]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0],
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a>        [0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1],
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a>        [1, 1, 1, 1, 1]])}
</span></code></pre></div></p> <h4 id=_14>特別なトークンの追加<a class=headerlink href=#_14 title="Permanent link">&para;</a></h4> <p>特別なトークン（または少なくともその概念）は、BERTおよび派生モデルにとって特に重要です。これらのトークンは、文の境界をより良く表現するために追加されます。たとえば、文の始まり（<code>[CLS]</code>）や文の間の区切り（<code>[SEP]</code>）などです。簡単な例を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a><span class=n>encoded_input</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&quot;How are you?&quot;</span><span class=p>)</span>
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>encoded_input</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a>[101, 1731, 1132, 1128, 136, 102]
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2 href=#__codelineno-34-2></a>&#39;[CLS] How are you? [SEP]&#39;
</span></code></pre></div></p> <p>これらの特別なトークンは、トークナイザーによって自動的に追加されます。すべてのモデルが特別なトークンを必要とするわけではありません。これらは主に、モデルがそれらで事前訓練された場合に使用され、その場合、トークナイザーはモデルがこれらのトークンを期待するため、それらを追加します。</p> <h4 id=_15>なぜこれらすべてが必要なのか？<a class=headerlink href=#_15 title="Permanent link">&para;</a></h4> <p>具体的な例を考えてみましょう。これらのエンコードされたシーケンスを考えてみます。 トークン化されると、以下のようになります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a><span class=n>sequences</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a>    <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span>
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a>    <span class=s2>&quot;I hate this so much!&quot;</span><span class=p>,</span>
</span><span id=__span-35-4><a id=__codelineno-35-4 name=__codelineno-35-4 href=#__codelineno-35-4></a><span class=p>]</span>
</span><span id=__span-35-5><a id=__codelineno-35-5 name=__codelineno-35-5 href=#__codelineno-35-5></a><span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span>
</span><span id=__span-35-6><a id=__codelineno-35-6 name=__codelineno-35-6 href=#__codelineno-35-6></a><span class=n>pprint</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a>[[101, 146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736,
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a>  1139, 2006, 1297, 119, 102],
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a> [101, 146, 4819, 1142, 1177, 1277, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
</span></code></pre></div></p> <p>これはエンコードされたシーケンスのリストです：リストのリストです。テンソルは長方形の形状のみを受け入れます（行列を考えてください）。この「配列」はすでに長方形の形状なので、テンソルに変換するのは簡単です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></code></pre></div> <h4 id=_16>モデルへの入力としてテンソルを使用する<a class=headerlink href=#_16 title="Permanent link">&para;</a></h4> <p>モデルでテンソルを使用することは非常に簡単です — 入力でモデルを呼び出すだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>)</span>
</span></code></pre></div> <p>モデルは多くの異なる引数を受け入れますが、入力IDのみが必要です。他の引数が何をするのか、いつ必要なのかについては後で説明しますが、まず、Transformerモデルが理解できる入力を構築するトークナイザーについて詳しく見る必要があります。</p> <h2 id=_17>トークナイザー<a class=headerlink href=#_17 title="Permanent link">&para;</a></h2> <p>トークナイザーは、NLPパイプラインのコアコンポーネントの1つです。1つの目的を果たします：テキストをモデルが処理できるデータに変換することです。モデルは数値のみを処理できるため、トークナイザーはテキスト入力を数値データに変換する必要があります。このセクションでは、トークン化パイプラインで正確に何が起こるかを探ります。</p> <p>NLPタスクでは、一般的に処理されるデータは生のテキストです。</p> <p>しかし、モデルは数値のみを処理できるため、生のテキストを数値に変換する方法を見つける必要があります。それがトークナイザーの役割であり、これを行う方法はたくさんあります。目標は、最も意味のある表現 — つまり、モデルにとって最も理にかなった表現を見つけ、可能であれば最小の表現を見つけることです。</p> <p>トークン化アルゴリズムの例をいくつか見て、トークン化について持つかもしれない疑問のいくつかに答えてみましょう。</p> <h3 id=_18>単語ベース<a class=headerlink href=#_18 title="Permanent link">&para;</a></h3> <p>思い浮かぶ最初のタイプのトークナイザーは_単語ベース_です。一般的にセットアップと使用が非常に簡単で、いくつかのルールだけで、しばしば良好な結果をもたらします。たとえば、以下の画像では、目標は生のテキストを単語に分割し、それぞれの数値表現を見つけることです：</p> <p><img alt=単語ベースのトークン化の例 src=../02_using_transformers_files/word_based_tokenization.png></p> <p>テキストを分割する方法はさまざまです。たとえば、Pythonの<code>split()</code>関数を適用して、空白を使用してテキストを単語にトークン化できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a><span class=n>tokenized_text</span> <span class=o>=</span> <span class=s2>&quot;Hello, how are you!&quot;</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokenized_text</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a>[&#39;Hello,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you!&#39;]
</span></code></pre></div></p> <p>句読点に対する追加のルールを持つ単語トークナイザーの変種もあります。この種のトークナイザーでは、かなり大きな「語彙」を得ることができます。語彙は、コーパス内の独立したトークンの総数によって定義されます。</p> <p>各単語には0から語彙のサイズまでのIDが割り当てられます。モデルはこれらのIDを使用して各単語を識別します。</p> <p>単語ベースのトークナイザーで言語を完全にカバーしたい場合、言語内の各単語の識別子が必要になり、膨大な数のトークンが生成されます。たとえば、英語には50万を超える単語があるため、各単語から入力IDへのマップを構築するには、そのたくさんのIDを追跡する必要があります。さらに、「dog」のような単語は「dogs」のような単語とは異なって表現され、モデルは最初「dog」と「dogs」が似ていることを知る方法がありません。これらの2つの単語を無関係として識別します。「run」と「running」などの他の似たような単語にも同じことが当てはまり、モデルは最初それらが似ていることを見ません。</p> <p>最後に、語彙にない単語を表すカスタムトークンが必要です。これは「未知」トークンとして知られ、しばしば「[UNK]」や「&lt;unk&gt;」として表されます。トークナイザーがこれらのトークンを多く生成している場合、一般的にそれは悪い兆候です。単語の合理的な表現を取得できず、途中で情報を失っているからです。語彙を作成する際の目標は、トークナイザーができるだけ少ない単語を未知トークンにトークン化するように行うことです。</p> <p>未知トークンの量を減らす1つの方法は、より深いレベルに行くことで、_文字ベース_のトークナイザーを使用することです。</p> <h3 id=_19>文字ベース<a class=headerlink href=#_19 title="Permanent link">&para;</a></h3> <p>文字ベースのトークナイザーは、単語ではなく文字にテキストを分割します。これには2つの主な利点があります：</p> <ul> <li>語彙がはるかに小さい</li> <li>語彙外（未知）トークンがはるかに少ない。すべての単語が文字から構築できるため</li> </ul> <p>しかし、ここでも空白と句読点に関する疑問が生じます：</p> <p><img alt=文字ベースのトークン化の例 src=../02_using_transformers_files/character_based_tokenization.png></p> <p>このアプローチも完璧ではありません。表現が単語ではなく文字に基づいているため、直感的にはあまり意味がないと主張できます。各文字は単独では多くを意味しませんが、単語の場合はそうです。しかし、これも言語によって異なります。たとえば中国語では、各文字はラテン語の文字よりも多くの情報を持っています。</p> <p>考慮すべきもう1つのことは、モデルによって処理される非常に大量のトークンになることです。単語ベースのトークナイザーでは単語が単一のトークンになるのに対し、文字に変換すると簡単に10以上のトークンになる可能性があります。</p> <p>両方の世界の最良を得るために、2つのアプローチを組み合わせた第3の技術を使用できます：<em>サブワードトークン化</em>。</p> <h3 id=_20>サブワードトークン化<a class=headerlink href=#_20 title="Permanent link">&para;</a></h3> <p>サブワードトークン化アルゴリズムは、頻繁に使用される単語はより小さなサブワードに分割されるべきではないが、稀な単語は意味のあるサブワードに分解されるべきであるという原則に依存しています。</p> <p>たとえば、「annoyingly」は稀な単語と見なされ、「annoying」と「ly」に分解される可能性があります。これらは両方とも、より頻繁に独立したサブワードとして現れる可能性が高く、同時に「annoying」と「ly」の複合的な意味によって「annoyingly」の意味が保持されます。</p> <p>サブワードトークン化アルゴリズムがシーケンス「Let's do tokenization!」をトークン化する方法を示す例です：</p> <p><img alt=サブワードトークン化アルゴリズム src=../02_using_transformers_files/bpe_subword.png></p> <p>これらのサブワードは多くの意味的意味を提供します。たとえば、上記の例では「tokenization」が「token」と「ization」に分割され、スペース効率が良い（長い単語を表すのに2つのトークンのみが必要）ながら意味的意味を持つ2つのトークンになりました。これにより、小さな語彙で比較的良好なカバレッジを持ち、未知トークンがほとんどない状態を実現できます。</p> <p>このアプローチは、サブワードを連結して（ほぼ）任意に長い複雑な単語を形成できるトルコ語などの膠着語では特に有用です。</p> <p>当然のことながら、他にも多くの技術があります。いくつか挙げると：</p> <ul> <li>GPT-2で使用されるByte-level BPE</li> <li>BERTで使用されるWordPiece</li> <li>いくつかの多言語モデルで使用されるSentencePieceまたはUnigram</li> </ul> <p>これで、APIを開始するのに十分なトークナイザーの動作に関する知識があるはずです。</p> <h3 id=_21>読み込みと保存<a class=headerlink href=#_21 title="Permanent link">&para;</a></h3> <p>トークナイザーの読み込みと保存は、モデルと同じくらい簡単です。実際、同じ2つのメソッド（<code>from_pretrained()</code>と<code>save_pretrained()</code>）に基づいています。これらのメソッドは、トークナイザーが使用するアルゴリズム（モデルの<em>アーキテクチャ</em>のようなもの）とその語彙（モデルの<em>重み</em>のようなもの）を読み込みまたは保存します。</p> <p>BERTと同じチェックポイントで訓練されたBERTトークナイザーの読み込みは、モデルの読み込みと同じ方法で行われますが、<code>BertTokenizer</code>クラスを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>BertTokenizer</span>
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a>
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><code>AutoModel</code>と同様に、<code>AutoTokenizer</code>クラスは、チェックポイント名に基づいてライブラリ内の適切なトークナイザークラスを取得し、任意のチェックポイントで直接使用できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-42-2><a id=__codelineno-42-2 name=__codelineno-42-2 href=#__codelineno-42-2></a>
</span><span id=__span-42-3><a id=__codelineno-42-3 name=__codelineno-42-3 href=#__codelineno-42-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>前のセクションで示したようにトークナイザーを使用できるようになりました：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokenizer</span><span class=p>(</span><span class=s2>&quot;Using a Transformer network is simple&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-44-2><a id=__codelineno-44-2 name=__codelineno-44-2 href=#__codelineno-44-2></a> &#39;input_ids&#39;: [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102],
</span><span id=__span-44-3><a id=__codelineno-44-3 name=__codelineno-44-3 href=#__codelineno-44-3></a> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0]}
</span></code></pre></div></p> <p>トークナイザーの保存は、モデルの保存と同じです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;models&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a>(&#39;models/tokenizer_config.json&#39;,
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a> &#39;models/special_tokens_map.json&#39;,
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a> &#39;models/vocab.txt&#39;,
</span><span id=__span-46-4><a id=__codelineno-46-4 name=__codelineno-46-4 href=#__codelineno-46-4></a> &#39;models/added_tokens.json&#39;,
</span><span id=__span-46-5><a id=__codelineno-46-5 name=__codelineno-46-5 href=#__codelineno-46-5></a> &#39;models/tokenizer.json&#39;)
</span></code></pre></div></p> <p>まず、<code>input_ids</code>がどのように生成されるかを見てみましょう。これを行うには、トークナイザーの中間メソッドを見る必要があります。</p> <h3 id=_22>エンコード<a class=headerlink href=#_22 title="Permanent link">&para;</a></h3> <p>テキストを数値に変換することは_エンコード_として知られています。エンコードは2段階のプロセスで行われます：トークン化、その後の入力IDへの変換。</p> <p>見てきたように、最初のステップはテキストを単語（または単語の一部、句読点記号など）に分割することで、通常<em>トークン</em>と呼ばれます。このプロセスを管理できる複数のルールがあり、そのためモデルが事前訓練されたときに使用されたのと同じルールを使用することを確認するために、モデルの名前を使用してトークナイザーをインスタンス化する必要があります。</p> <p>2番目のステップは、これらのトークンを数値に変換することで、テンソルを構築してモデルに供給できます。これを行うために、トークナイザーには<em>語彙</em>があり、これは<code>from_pretrained()</code>メソッドでインスタンス化するときにダウンロードする部分です。繰り返しますが、モデルが事前訓練されたときに使用されたのと同じ語彙を使用する必要があります。</p> <p>2つのステップをより良く理解するために、それらを別々に探索します。実際には、トークンの入力を直接呼び出すべきですが、トークン化パイプラインの一部を個別に実行するいくつかのメソッドを使用して、これらのステップの中間結果を示します。</p> <h4 id=_23>トークン化<a class=headerlink href=#_23 title="Permanent link">&para;</a></h4> <p>トークン化プロセスは、トークナイザーの<code>tokenize()</code>メソッドによって行われます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a>
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span><span id=__span-47-4><a id=__codelineno-47-4 name=__codelineno-47-4 href=#__codelineno-47-4></a>
</span><span id=__span-47-5><a id=__codelineno-47-5 name=__codelineno-47-5 href=#__codelineno-47-5></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;Using a Transformer network is simple&quot;</span>
</span><span id=__span-47-6><a id=__codelineno-47-6 name=__codelineno-47-6 href=#__codelineno-47-6></a>
</span><span id=__span-47-7><a id=__codelineno-47-7 name=__codelineno-47-7 href=#__codelineno-47-7></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-47-8><a id=__codelineno-47-8 name=__codelineno-47-8 href=#__codelineno-47-8></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a>[&#39;Using&#39;, &#39;a&#39;, &#39;Trans&#39;, &#39;##former&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
</span></code></pre></div></p> <p>このトークナイザーはサブワードトークナイザーです：語彙で表現できるトークンを取得するまで単語を分割します。ここでは<code>transformer</code>の場合がそうで、2つのトークン（<code>Trans</code>と<code>##former</code>）に分割されています。</p> <h4 id=id>トークンから入力IDへ<a class=headerlink href=#id title="Permanent link">&para;</a></h4> <p>入力IDへの変換は、トークナイザーの<code>convert_tokens_to_ids()</code>メソッドによって処理されます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a><span class=n>ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a>
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a>[7993, 170, 13809, 23763, 2443, 1110, 3014]
</span><span id=__span-50-2><a id=__codelineno-50-2 name=__codelineno-50-2 href=#__codelineno-50-2></a>[&#39;Using&#39;, &#39;a&#39;, &#39;Trans&#39;, &#39;##former&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
</span></code></pre></div></p> <h3 id=_24>デコード<a class=headerlink href=#_24 title="Permanent link">&para;</a></h3> <p><em>デコード</em>は逆方向に行きます：語彙インデックスから文字列を取得したいのです。これは以下のように<code>decode()</code>メソッドで行うことができます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a><span class=n>decoded_string</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>decoded_string</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a>&#39;Using a Transformer network is simple&#39;
</span></code></pre></div></p> <p><code>decode</code>メソッドは、インデックスをトークンに戻すだけでなく、同じ単語の一部だったトークンをグループ化して読みやすい文を生成することに注意してください。この動作は、新しいテキストを予測するモデル（プロンプトから生成されたテキスト、または翻訳や要約などのシーケンス間の問題）を使用するときに非常に有用です。</p> <p>これで、トークナイザーが処理できる原子操作（トークン化、IDへの変換、IDから文字列への変換）を理解できたはずです。しかし、氷山の一角をかじっただけです。次のセクションでは、アプローチを限界まで持っていき、それらを克服する方法を見ていきます。</p> <h2 id=_25>複数シーケンスの処理<a class=headerlink href=#_25 title="Permanent link">&para;</a></h2> <p>前のセクションでは、最もシンプルな使用例（小さな長さの単一シーケンスで推論を行う）を探索しました。しかし、すでにいくつかの疑問が浮かんでいます：</p> <ul> <li>複数のシーケンスをどのように処理するか？</li> <li><em>異なる長さ</em>の複数のシーケンスをどのように処理するか？</li> <li>語彙インデックスはモデルがうまく動作できる唯一の入力か？</li> <li>長すぎるシーケンスのようなものはあるか？</li> </ul> <p>これらの疑問がどのような問題を引き起こすかを見て、Transformers APIを使用してそれらをどのように解決できるかを見てみましょう。</p> <h3 id=_26>モデルは入力のバッチを期待する<a class=headerlink href=#_26 title="Permanent link">&para;</a></h3> <p>前の演習では、シーケンスが数値のリストにどのように変換されるかを見ました。この数値のリストをテンソルに変換して、モデルに送信しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a>
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4 href=#__codelineno-53-4></a>
</span><span id=__span-53-5><a id=__codelineno-53-5 name=__codelineno-53-5 href=#__codelineno-53-5></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-53-6><a id=__codelineno-53-6 name=__codelineno-53-6 href=#__codelineno-53-6></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-53-7><a id=__codelineno-53-7 name=__codelineno-53-7 href=#__codelineno-53-7></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-53-8><a id=__codelineno-53-8 name=__codelineno-53-8 href=#__codelineno-53-8></a>
</span><span id=__span-53-9><a id=__codelineno-53-9 name=__codelineno-53-9 href=#__codelineno-53-9></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a Huggingface course my whole life.&quot;</span>
</span><span id=__span-53-10><a id=__codelineno-53-10 name=__codelineno-53-10 href=#__codelineno-53-10></a>
</span><span id=__span-53-11><a id=__codelineno-53-11 name=__codelineno-53-11 href=#__codelineno-53-11></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-53-12><a id=__codelineno-53-12 name=__codelineno-53-12 href=#__codelineno-53-12></a><span class=n>ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-53-13><a id=__codelineno-53-13 name=__codelineno-53-13 href=#__codelineno-53-13></a><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span><span id=__span-53-14><a id=__codelineno-53-14 name=__codelineno-53-14 href=#__codelineno-53-14></a>
</span><span id=__span-53-15><a id=__codelineno-53-15 name=__codelineno-53-15 href=#__codelineno-53-15></a><span class=c1># この行は失敗します</span>
</span><span id=__span-53-16><a id=__codelineno-53-16 name=__codelineno-53-16 href=#__codelineno-53-16></a><span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></code></pre></div> <p>問題は、モデルに単一のシーケンスを送信したことですが、Transformersモデルはデフォルトで複数の文を期待することです。ここで、トークナイザーを<code>sequence</code>に適用したときに裏でトークナイザーが行ったすべてを実行しようとしました。しかし、よく見ると、トークナイザーは入力IDのリストをテンソルに変換しただけでなく、その上に次元を追加したことがわかります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=n>tokenized_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequence</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenized_inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a>tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
</span><span id=__span-55-2><a id=__codelineno-55-2 name=__codelineno-55-2 href=#__codelineno-55-2></a>          2607,  2026,  2878,  2166,  1012,   102]])
</span></code></pre></div></p> <p>再試行して、新しい次元を追加しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-56-2><a id=__codelineno-56-2 name=__codelineno-56-2 href=#__codelineno-56-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-56-3><a id=__codelineno-56-3 name=__codelineno-56-3 href=#__codelineno-56-3></a>
</span><span id=__span-56-4><a id=__codelineno-56-4 name=__codelineno-56-4 href=#__codelineno-56-4></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-56-5><a id=__codelineno-56-5 name=__codelineno-56-5 href=#__codelineno-56-5></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-56-6><a id=__codelineno-56-6 name=__codelineno-56-6 href=#__codelineno-56-6></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-56-7><a id=__codelineno-56-7 name=__codelineno-56-7 href=#__codelineno-56-7></a>
</span><span id=__span-56-8><a id=__codelineno-56-8 name=__codelineno-56-8 href=#__codelineno-56-8></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span>
</span><span id=__span-56-9><a id=__codelineno-56-9 name=__codelineno-56-9 href=#__codelineno-56-9></a>
</span><span id=__span-56-10><a id=__codelineno-56-10 name=__codelineno-56-10 href=#__codelineno-56-10></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-56-11><a id=__codelineno-56-11 name=__codelineno-56-11 href=#__codelineno-56-11></a><span class=n>ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-56-12><a id=__codelineno-56-12 name=__codelineno-56-12 href=#__codelineno-56-12></a>
</span><span id=__span-56-13><a id=__codelineno-56-13 name=__codelineno-56-13 href=#__codelineno-56-13></a><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>ids</span><span class=p>])</span>
</span><span id=__span-56-14><a id=__codelineno-56-14 name=__codelineno-56-14 href=#__codelineno-56-14></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Input IDs:&quot;</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>)</span>
</span><span id=__span-56-15><a id=__codelineno-56-15 name=__codelineno-56-15 href=#__codelineno-56-15></a>
</span><span id=__span-56-16><a id=__codelineno-56-16 name=__codelineno-56-16 href=#__codelineno-56-16></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span><span id=__span-56-17><a id=__codelineno-56-17 name=__codelineno-56-17 href=#__codelineno-56-17></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Logits:&quot;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a>Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,
</span><span id=__span-57-2><a id=__codelineno-57-2 name=__codelineno-57-2 href=#__codelineno-57-2></a>          2026,  2878,  2166,  1012]])
</span><span id=__span-57-3><a id=__codelineno-57-3 name=__codelineno-57-3 href=#__codelineno-57-3></a>Logits: tensor([[-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p><em>バッチング</em>は、複数の文を一度にモデルに送信する行為です。文が1つしかない場合は、単一のシーケンスでバッチを構築するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=n>batched_ids</span> <span class=o>=</span> <span class=p>[</span><span class=n>ids</span><span class=p>,</span> <span class=n>ids</span><span class=p>]</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=n>batched_input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>batched_ids</span><span class=p>)</span>
</span><span id=__span-58-3><a id=__codelineno-58-3 name=__codelineno-58-3 href=#__codelineno-58-3></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batched_input_ids</span><span class=p>)</span>
</span><span id=__span-58-4><a id=__codelineno-58-4 name=__codelineno-58-4 href=#__codelineno-58-4></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Logits:&quot;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a>Logits: tensor([[-2.7276,  2.8789],
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a>        [-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p>バッチングにより、複数の文書をモデルに供給するときにモデルが機能します。複数のシーケンスを使用することは、単一のシーケンスでバッチを構築するのと同じくらい簡単です。しかし、2番目の問題があります。2つ（またはそれ以上）の文書を一緒にバッチ化しようとするとき、それらは異なる長さの可能性があります。以前にテンソルを扱ったことがある場合、テンソルは長方形の形状である必要があることを知っているので、入力IDのリストを直接テンソルに変換することはできません。この問題を回避するために、通常入力を<em>パッド</em>します。</p> <h3 id=_27>入力のパディング<a class=headerlink href=#_27 title="Permanent link">&para;</a></h3> <p>以下のリストのリストはテンソルに変換できません：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=n>batched_ids</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>]</span>
</span><span id=__span-60-4><a id=__codelineno-60-4 name=__codelineno-60-4 href=#__codelineno-60-4></a><span class=p>]</span>
</span></code></pre></div> <p>これを回避するために、<em>パディング</em>を使用してテンソルを長方形にします。パディングは、値が少ない文に<em>パディングトークン</em>と呼ばれる特別な単語を追加することで、すべての文が同じ長さになることを確実にします。たとえば、10語の10文と20語の1文がある場合、パディングにより、すべての文が20語になります。私たちの例では、結果のテンソルは次のようになります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a><span class=n>padding_id</span> <span class=o>=</span> <span class=mi>100</span>
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a>
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a><span class=n>batched_ids</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-61-4><a id=__codelineno-61-4 name=__codelineno-61-4 href=#__codelineno-61-4></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
</span><span id=__span-61-5><a id=__codelineno-61-5 name=__codelineno-61-5 href=#__codelineno-61-5></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=n>padding_id</span><span class=p>],</span>
</span><span id=__span-61-6><a id=__codelineno-61-6 name=__codelineno-61-6 href=#__codelineno-61-6></a><span class=p>]</span>
</span></code></pre></div> <p>パディングトークンIDは<code>tokenizer.pad_token_id</code>で見つけることができます。それを使用して、2つの文を個別にモデルに送信し、一緒にバッチ化してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-62-2><a id=__codelineno-62-2 name=__codelineno-62-2 href=#__codelineno-62-2></a>
</span><span id=__span-62-3><a id=__codelineno-62-3 name=__codelineno-62-3 href=#__codelineno-62-3></a><span class=n>sequence1_ids</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>200</span><span class=p>,</span><span class=mi>200</span><span class=p>,</span><span class=mi>200</span><span class=p>]]</span>
</span><span id=__span-62-4><a id=__codelineno-62-4 name=__codelineno-62-4 href=#__codelineno-62-4></a><span class=n>sequence2_ids</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>200</span><span class=p>,</span><span class=mi>200</span><span class=p>]]</span>
</span><span id=__span-62-5><a id=__codelineno-62-5 name=__codelineno-62-5 href=#__codelineno-62-5></a><span class=n>batched_ids</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-62-6><a id=__codelineno-62-6 name=__codelineno-62-6 href=#__codelineno-62-6></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
</span><span id=__span-62-7><a id=__codelineno-62-7 name=__codelineno-62-7 href=#__codelineno-62-7></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>],</span>
</span><span id=__span-62-8><a id=__codelineno-62-8 name=__codelineno-62-8 href=#__codelineno-62-8></a><span class=p>]</span>
</span><span id=__span-62-9><a id=__codelineno-62-9 name=__codelineno-62-9 href=#__codelineno-62-9></a>
</span><span id=__span-62-10><a id=__codelineno-62-10 name=__codelineno-62-10 href=#__codelineno-62-10></a><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>sequence1_ids</span><span class=p>))</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span><span id=__span-62-11><a id=__codelineno-62-11 name=__codelineno-62-11 href=#__codelineno-62-11></a><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>sequence2_ids</span><span class=p>))</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span><span id=__span-62-12><a id=__codelineno-62-12 name=__codelineno-62-12 href=#__codelineno-62-12></a><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>batched_ids</span><span class=p>))</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)
</span><span id=__span-63-2><a id=__codelineno-63-2 name=__codelineno-63-2 href=#__codelineno-63-2></a>tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)
</span><span id=__span-63-3><a id=__codelineno-63-3 name=__codelineno-63-3 href=#__codelineno-63-3></a>tensor([[ 1.5694, -1.3895],
</span><span id=__span-63-4><a id=__codelineno-63-4 name=__codelineno-63-4 href=#__codelineno-63-4></a>        [ 1.3374, -1.2163]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p>バッチ予測のlogitsに何か問題があります：2行目は2番目の文のlogitsと同じであるべきですが、完全に異なる値を得ています！</p> <p>これは、Transformerモデルの主要な特徴が、各トークンを<em>文脈化</em>する注意レイヤーであるためです。これらは、シーケンスのすべてのトークンに注意を払うため、パディングトークンを考慮に入れます。異なる長さの個別の文をモデルに通すときや、同じ文とパディングが適用されたバッチを通すときに同じ結果を得るには、これらの注意レイヤーにパディングトークンを無視するよう伝える必要があります。これは<code>attention mask</code>を使用して行われます。</p> <h3 id=attention-mask><code>attention mask</code><a class=headerlink href=#attention-mask title="Permanent link">&para;</a></h3> <p><em><code>attention mask</code></em>は、入力IDテンソルとまったく同じ形状のテンソルで、0と1で埋められています：1は対応するトークンに注意を払うべきであることを示し、0は対応するトークンに注意を払うべきでない（つまり、モデルの注意レイヤーによって無視されるべき）ことを示します。</p> <p><code>attention mask</code>で前の例を完成させましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=n>batched_ids</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-64-2><a id=__codelineno-64-2 name=__codelineno-64-2 href=#__codelineno-64-2></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
</span><span id=__span-64-3><a id=__codelineno-64-3 name=__codelineno-64-3 href=#__codelineno-64-3></a>    <span class=p>[</span><span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>],</span>
</span><span id=__span-64-4><a id=__codelineno-64-4 name=__codelineno-64-4 href=#__codelineno-64-4></a><span class=p>]</span>
</span><span id=__span-64-5><a id=__codelineno-64-5 name=__codelineno-64-5 href=#__codelineno-64-5></a>
</span><span id=__span-64-6><a id=__codelineno-64-6 name=__codelineno-64-6 href=#__codelineno-64-6></a><span class=n>attention_mask</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-64-7><a id=__codelineno-64-7 name=__codelineno-64-7 href=#__codelineno-64-7></a>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span><span id=__span-64-8><a id=__codelineno-64-8 name=__codelineno-64-8 href=#__codelineno-64-8></a>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span><span id=__span-64-9><a id=__codelineno-64-9 name=__codelineno-64-9 href=#__codelineno-64-9></a><span class=p>]</span>
</span><span id=__span-64-10><a id=__codelineno-64-10 name=__codelineno-64-10 href=#__codelineno-64-10></a>
</span><span id=__span-64-11><a id=__codelineno-64-11 name=__codelineno-64-11 href=#__codelineno-64-11></a><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>batched_ids</span><span class=p>),</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>))</span>
</span><span id=__span-64-12><a id=__codelineno-64-12 name=__codelineno-64-12 href=#__codelineno-64-12></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>tensor([[ 1.5694, -1.3895],
</span><span id=__span-65-2><a id=__codelineno-65-2 name=__codelineno-65-2 href=#__codelineno-65-2></a>        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <p>これで、バッチの2番目の文に対して同じlogitsを得ました。</p> <p>2番目のシーケンスの最後の値がパディングIDであり、<code>attention mask</code>では0値であることに注意してください。</p> <p><strong>試してみよう！</strong></p> <p>セクション2で使用した2つの文（「I've been waiting for a HuggingFace course my whole life.」と「I hate this so much!」）にトークン化を手動で適用してください。それらをモデルに通し、セクション2と同じlogitsを得ることを確認してください。次に、パディングトークンを使用してそれらを一緒にバッチ化し、適切な<code>attention mask</code>を作成してください。モデルを通すときに同じ結果を得ることを確認してください！</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-66-2><a id=__codelineno-66-2 name=__codelineno-66-2 href=#__codelineno-66-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-66-3><a id=__codelineno-66-3 name=__codelineno-66-3 href=#__codelineno-66-3></a>
</span><span id=__span-66-4><a id=__codelineno-66-4 name=__codelineno-66-4 href=#__codelineno-66-4></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-66-5><a id=__codelineno-66-5 name=__codelineno-66-5 href=#__codelineno-66-5></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-66-6><a id=__codelineno-66-6 name=__codelineno-66-6 href=#__codelineno-66-6></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-66-7><a id=__codelineno-66-7 name=__codelineno-66-7 href=#__codelineno-66-7></a>
</span><span id=__span-66-8><a id=__codelineno-66-8 name=__codelineno-66-8 href=#__codelineno-66-8></a><span class=n>sequence1</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span>
</span><span id=__span-66-9><a id=__codelineno-66-9 name=__codelineno-66-9 href=#__codelineno-66-9></a><span class=n>sequence2</span> <span class=o>=</span> <span class=s2>&quot;I hate this so much!&quot;</span>
</span><span id=__span-66-10><a id=__codelineno-66-10 name=__codelineno-66-10 href=#__codelineno-66-10></a>
</span><span id=__span-66-11><a id=__codelineno-66-11 name=__codelineno-66-11 href=#__codelineno-66-11></a><span class=n>tokens1</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence1</span><span class=p>)</span>
</span><span id=__span-66-12><a id=__codelineno-66-12 name=__codelineno-66-12 href=#__codelineno-66-12></a><span class=n>ids1</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens1</span><span class=p>)</span>
</span><span id=__span-66-13><a id=__codelineno-66-13 name=__codelineno-66-13 href=#__codelineno-66-13></a>
</span><span id=__span-66-14><a id=__codelineno-66-14 name=__codelineno-66-14 href=#__codelineno-66-14></a><span class=n>tokens2</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence2</span><span class=p>)</span>
</span><span id=__span-66-15><a id=__codelineno-66-15 name=__codelineno-66-15 href=#__codelineno-66-15></a><span class=n>ids2</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens2</span><span class=p>)</span>
</span><span id=__span-66-16><a id=__codelineno-66-16 name=__codelineno-66-16 href=#__codelineno-66-16></a>
</span><span id=__span-66-17><a id=__codelineno-66-17 name=__codelineno-66-17 href=#__codelineno-66-17></a><span class=n>max_len</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>ids1</span><span class=p>)</span>
</span><span id=__span-66-18><a id=__codelineno-66-18 name=__codelineno-66-18 href=#__codelineno-66-18></a><span class=n>pad_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span>
</span><span id=__span-66-19><a id=__codelineno-66-19 name=__codelineno-66-19 href=#__codelineno-66-19></a>
</span><span id=__span-66-20><a id=__codelineno-66-20 name=__codelineno-66-20 href=#__codelineno-66-20></a><span class=n>ids2</span> <span class=o>=</span> <span class=n>ids2</span> <span class=o>+</span> <span class=p>[</span><span class=n>pad_id</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>max_len</span> <span class=o>-</span> <span class=nb>len</span><span class=p>(</span><span class=n>ids2</span><span class=p>))</span>
</span><span id=__span-66-21><a id=__codelineno-66-21 name=__codelineno-66-21 href=#__codelineno-66-21></a>
</span><span id=__span-66-22><a id=__codelineno-66-22 name=__codelineno-66-22 href=#__codelineno-66-22></a><span class=c1># `attention mask`</span>
</span><span id=__span-66-23><a id=__codelineno-66-23 name=__codelineno-66-23 href=#__codelineno-66-23></a><span class=n>mask1</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>ids1</span><span class=p>)</span>
</span><span id=__span-66-24><a id=__codelineno-66-24 name=__codelineno-66-24 href=#__codelineno-66-24></a><span class=n>mask2</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens2</span><span class=p>)</span> <span class=o>+</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>max_len</span> <span class=o>-</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens2</span><span class=p>))</span>
</span><span id=__span-66-25><a id=__codelineno-66-25 name=__codelineno-66-25 href=#__codelineno-66-25></a><span class=n>attention_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>mask1</span><span class=p>,</span> <span class=n>mask2</span><span class=p>])</span>
</span><span id=__span-66-26><a id=__codelineno-66-26 name=__codelineno-66-26 href=#__codelineno-66-26></a><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>ids1</span><span class=p>,</span> <span class=n>ids2</span><span class=p>])</span>
</span><span id=__span-66-27><a id=__codelineno-66-27 name=__codelineno-66-27 href=#__codelineno-66-27></a>
</span><span id=__span-66-28><a id=__codelineno-66-28 name=__codelineno-66-28 href=#__codelineno-66-28></a><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>ids1</span><span class=p>]))</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span><span id=__span-66-29><a id=__codelineno-66-29 name=__codelineno-66-29 href=#__codelineno-66-29></a><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>ids2</span><span class=p>[:</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens2</span><span class=p>)]]))</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span><span id=__span-66-30><a id=__codelineno-66-30 name=__codelineno-66-30 href=#__codelineno-66-30></a>
</span><span id=__span-66-31><a id=__codelineno-66-31 name=__codelineno-66-31 href=#__codelineno-66-31></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span>
</span><span id=__span-66-32><a id=__codelineno-66-32 name=__codelineno-66-32 href=#__codelineno-66-32></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Logits:&quot;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a>tensor([[-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;)
</span><span id=__span-67-2><a id=__codelineno-67-2 name=__codelineno-67-2 href=#__codelineno-67-2></a>tensor([[ 2.5423, -2.1265]], grad_fn=&lt;AddmmBackward0&gt;)
</span><span id=__span-67-3><a id=__codelineno-67-3 name=__codelineno-67-3 href=#__codelineno-67-3></a>Logits: tensor([[-2.7276,  2.8789],
</span><span id=__span-67-4><a id=__codelineno-67-4 name=__codelineno-67-4 href=#__codelineno-67-4></a>        [ 2.5423, -2.1265]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <h3 id=_28>より長いシーケンス<a class=headerlink href=#_28 title="Permanent link">&para;</a></h3> <p>Transformerモデルでは、モデルに渡すことができるシーケンスの長さに制限があります。ほとんどのモデルは最大512または1024トークンのシーケンスを処理し、より長いシーケンスを処理するよう求められるとクラッシュします。この問題には2つの解決策があります：</p> <ul> <li>より長いサポートされるシーケンス長を持つモデルを使用する</li> <li>シーケンスを切り捨てる</li> </ul> <p>モデルには異なるサポートされるシーケンス長があり、一部は非常に長いシーケンスの処理に特化しています。<a href=https://huggingface.co/docs/transformers/model_doc/longformer>Longformer</a>は一例で、もう一つは<a href=https://huggingface.co/docs/transformers/model_doc/led>LED</a>です。非常に長いシーケンスを必要とするタスクに取り組んでいる場合は、これらのモデルを調べることをお勧めします。</p> <p>それ以外の場合は、<code>max_sequence_length</code>パラメータを指定してシーケンスを切り捨てることをお勧めします：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=n>sequence</span> <span class=o>=</span> <span class=n>sequence</span><span class=p>[:</span><span class=n>max_sequence_length</span><span class=p>]</span>
</span></code></pre></div> <h2 id=_29>すべてをまとめる<a class=headerlink href=#_29 title="Permanent link">&para;</a></h2> <p>最後のいくつかのセクションでは、手作業でほとんどの作業を行うよう最善を尽くしてきました。トークナイザーがどのように動作するかを探索し、トークン化、入力IDへの変換、パディング、切り捨て、<code>attention mask</code>について見てきました。</p> <p>しかし、セクション2で見たように、Transformers APIは、ここで詳しく見ていく高レベル関数ですべてを処理できます。文に対して<code>tokenizer</code>を直接呼び出すと、モデルに渡す準備ができた入力が返されます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a>
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-69-4><a id=__codelineno-69-4 name=__codelineno-69-4 href=#__codelineno-69-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-69-5><a id=__codelineno-69-5 name=__codelineno-69-5 href=#__codelineno-69-5></a>
</span><span id=__span-69-6><a id=__codelineno-69-6 name=__codelineno-69-6 href=#__codelineno-69-6></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span>
</span><span id=__span-69-7><a id=__codelineno-69-7 name=__codelineno-69-7 href=#__codelineno-69-7></a>
</span><span id=__span-69-8><a id=__codelineno-69-8 name=__codelineno-69-8 href=#__codelineno-69-8></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span></code></pre></div> <p>ここで、<code>model_inputs</code>変数には、モデルが適切に動作するために必要なすべてが含まれています。DistilBERTの場合、入力IDと<code>attention mask</code>が含まれます。追加の入力を受け入れる他のモデルでは、<code>tokenizer</code>オブジェクトによってそれらも出力されます。</p> <p>以下のいくつかの例で見るように、この方法は非常に強力です。まず、単一のシーケンスをトークン化できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span>
</span><span id=__span-70-2><a id=__codelineno-70-2 name=__codelineno-70-2 href=#__codelineno-70-2></a>
</span><span id=__span-70-3><a id=__codelineno-70-3 name=__codelineno-70-3 href=#__codelineno-70-3></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-70-4><a id=__codelineno-70-4 name=__codelineno-70-4 href=#__codelineno-70-4></a>
</span><span id=__span-70-5><a id=__codelineno-70-5 name=__codelineno-70-5 href=#__codelineno-70-5></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a>{&#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a> &#39;input_ids&#39;: [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172,
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a>               2607, 2026, 2878, 2166, 1012, 102]}
</span></code></pre></div></p> <p>APIに変更なしで、一度に複数のシーケンスも処理します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=n>sequences</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span> <span class=s2>&quot;So have I!&quot;</span><span class=p>]</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a>
</span><span id=__span-72-3><a id=__codelineno-72-3 name=__codelineno-72-3 href=#__codelineno-72-3></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span><span id=__span-72-4><a id=__codelineno-72-4 name=__codelineno-72-4 href=#__codelineno-72-4></a>
</span><span id=__span-72-5><a id=__codelineno-72-5 name=__codelineno-72-5 href=#__codelineno-72-5></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a>{&#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a>                    [1, 1, 1, 1, 1, 1]],
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a> &#39;input_ids&#39;: [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172,
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a>                2607, 2026, 2878, 2166, 1012, 102],
</span><span id=__span-73-5><a id=__codelineno-73-5 name=__codelineno-73-5 href=#__codelineno-73-5></a>               [101, 2061, 2031, 1045, 999, 102]]}
</span></code></pre></div></p> <p>いくつかの目的に応じてパディングできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a><span class=c1># 最大シーケンス長までシーケンスをパディング</span>
</span><span id=__span-74-2><a id=__codelineno-74-2 name=__codelineno-74-2 href=#__codelineno-74-2></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;longest&quot;</span><span class=p>)</span>
</span><span id=__span-74-3><a id=__codelineno-74-3 name=__codelineno-74-3 href=#__codelineno-74-3></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-74-4><a id=__codelineno-74-4 name=__codelineno-74-4 href=#__codelineno-74-4></a>
</span><span id=__span-74-5><a id=__codelineno-74-5 name=__codelineno-74-5 href=#__codelineno-74-5></a><span class=c1># モデルの最大長までシーケンスをパディング</span>
</span><span id=__span-74-6><a id=__codelineno-74-6 name=__codelineno-74-6 href=#__codelineno-74-6></a><span class=c1># （BERTまたはDistilBERTの場合は512）</span>
</span><span id=__span-74-7><a id=__codelineno-74-7 name=__codelineno-74-7 href=#__codelineno-74-7></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;max_length&quot;</span><span class=p>)</span>
</span><span id=__span-74-8><a id=__codelineno-74-8 name=__codelineno-74-8 href=#__codelineno-74-8></a><span class=c1># pprint(model_inputs, compact=True)</span>
</span><span id=__span-74-9><a id=__codelineno-74-9 name=__codelineno-74-9 href=#__codelineno-74-9></a>
</span><span id=__span-74-10><a id=__codelineno-74-10 name=__codelineno-74-10 href=#__codelineno-74-10></a><span class=c1># 指定された最大長までシーケンスをパディング</span>
</span><span id=__span-74-11><a id=__codelineno-74-11 name=__codelineno-74-11 href=#__codelineno-74-11></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;max_length&quot;</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span><span id=__span-74-12><a id=__codelineno-74-12 name=__codelineno-74-12 href=#__codelineno-74-12></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a>{&#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a>                    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a> &#39;input_ids&#39;: [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172,
</span><span id=__span-75-4><a id=__codelineno-75-4 name=__codelineno-75-4 href=#__codelineno-75-4></a>                2607, 2026, 2878, 2166, 1012, 102],
</span><span id=__span-75-5><a id=__codelineno-75-5 name=__codelineno-75-5 href=#__codelineno-75-5></a>               [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
</span><span id=__span-75-6><a id=__codelineno-75-6 name=__codelineno-75-6 href=#__codelineno-75-6></a>{&#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-75-7><a id=__codelineno-75-7 name=__codelineno-75-7 href=#__codelineno-75-7></a>                    [1, 1, 1, 1, 1, 1, 0, 0]],
</span><span id=__span-75-8><a id=__codelineno-75-8 name=__codelineno-75-8 href=#__codelineno-75-8></a> &#39;input_ids&#39;: [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172,
</span><span id=__span-75-9><a id=__codelineno-75-9 name=__codelineno-75-9 href=#__codelineno-75-9></a>                2607, 2026, 2878, 2166, 1012, 102],
</span><span id=__span-75-10><a id=__codelineno-75-10 name=__codelineno-75-10 href=#__codelineno-75-10></a>               [101, 2061, 2031, 1045, 999, 102, 0, 0]]}
</span></code></pre></div></p> <p>シーケンスを切り捨てることもできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a><span class=n>sequences</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span> <span class=s2>&quot;So have I!&quot;</span><span class=p>]</span>
</span><span id=__span-76-2><a id=__codelineno-76-2 name=__codelineno-76-2 href=#__codelineno-76-2></a>
</span><span id=__span-76-3><a id=__codelineno-76-3 name=__codelineno-76-3 href=#__codelineno-76-3></a><span class=c1># モデルの最大長より長いシーケンスを切り捨て</span>
</span><span id=__span-76-4><a id=__codelineno-76-4 name=__codelineno-76-4 href=#__codelineno-76-4></a><span class=c1># （BERTまたはDistilBERTの場合は512）</span>
</span><span id=__span-76-5><a id=__codelineno-76-5 name=__codelineno-76-5 href=#__codelineno-76-5></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-76-6><a id=__codelineno-76-6 name=__codelineno-76-6 href=#__codelineno-76-6></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-76-7><a id=__codelineno-76-7 name=__codelineno-76-7 href=#__codelineno-76-7></a>
</span><span id=__span-76-8><a id=__codelineno-76-8 name=__codelineno-76-8 href=#__codelineno-76-8></a><span class=c1># 指定された最大長より長いシーケンスを切り捨て</span>
</span><span id=__span-76-9><a id=__codelineno-76-9 name=__codelineno-76-9 href=#__codelineno-76-9></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-76-10><a id=__codelineno-76-10 name=__codelineno-76-10 href=#__codelineno-76-10></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a>{&#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-77-2><a id=__codelineno-77-2 name=__codelineno-77-2 href=#__codelineno-77-2></a>                    [1, 1, 1, 1, 1, 1]],
</span><span id=__span-77-3><a id=__codelineno-77-3 name=__codelineno-77-3 href=#__codelineno-77-3></a> &#39;input_ids&#39;: [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172,
</span><span id=__span-77-4><a id=__codelineno-77-4 name=__codelineno-77-4 href=#__codelineno-77-4></a>                2607, 2026, 2878, 2166, 1012, 102],
</span><span id=__span-77-5><a id=__codelineno-77-5 name=__codelineno-77-5 href=#__codelineno-77-5></a>               [101, 2061, 2031, 1045, 999, 102]]}
</span><span id=__span-77-6><a id=__codelineno-77-6 name=__codelineno-77-6 href=#__codelineno-77-6></a>{&#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]],
</span><span id=__span-77-7><a id=__codelineno-77-7 name=__codelineno-77-7 href=#__codelineno-77-7></a> &#39;input_ids&#39;: [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102],
</span><span id=__span-77-8><a id=__codelineno-77-8 name=__codelineno-77-8 href=#__codelineno-77-8></a>               [101, 2061, 2031, 1045, 999, 102]]}
</span></code></pre></div></p> <p><code>tokenizer</code>オブジェクトは、特定のフレームワークテンソルへの変換を処理でき、その後モデルに直接送信できます。たとえば、以下のコードサンプルでは、異なるフレームワークからテンソルを返すようトークナイザーに促しています — <code>"pt"</code>はPyTorchテンソルを返し、<code>"np"</code>はNumPy配列を返します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-78-1><a id=__codelineno-78-1 name=__codelineno-78-1 href=#__codelineno-78-1></a><span class=n>sequences</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span> <span class=s2>&quot;So have I!&quot;</span><span class=p>]</span>
</span><span id=__span-78-2><a id=__codelineno-78-2 name=__codelineno-78-2 href=#__codelineno-78-2></a>
</span><span id=__span-78-3><a id=__codelineno-78-3 name=__codelineno-78-3 href=#__codelineno-78-3></a><span class=c1># PyTorchテンソルを返す</span>
</span><span id=__span-78-4><a id=__codelineno-78-4 name=__codelineno-78-4 href=#__codelineno-78-4></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-78-5><a id=__codelineno-78-5 name=__codelineno-78-5 href=#__codelineno-78-5></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-78-6><a id=__codelineno-78-6 name=__codelineno-78-6 href=#__codelineno-78-6></a>
</span><span id=__span-78-7><a id=__codelineno-78-7 name=__codelineno-78-7 href=#__codelineno-78-7></a><span class=c1># NumPy配列を返す</span>
</span><span id=__span-78-8><a id=__codelineno-78-8 name=__codelineno-78-8 href=#__codelineno-78-8></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;np&quot;</span><span class=p>)</span>
</span><span id=__span-78-9><a id=__codelineno-78-9 name=__codelineno-78-9 href=#__codelineno-78-9></a><span class=n>pprint</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-79-1><a id=__codelineno-79-1 name=__codelineno-79-1 href=#__codelineno-79-1></a>{&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-79-2><a id=__codelineno-79-2 name=__codelineno-79-2 href=#__codelineno-79-2></a>        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
</span><span id=__span-79-3><a id=__codelineno-79-3 name=__codelineno-79-3 href=#__codelineno-79-3></a> &#39;input_ids&#39;: tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
</span><span id=__span-79-4><a id=__codelineno-79-4 name=__codelineno-79-4 href=#__codelineno-79-4></a>          2607,  2026,  2878,  2166,  1012,   102],
</span><span id=__span-79-5><a id=__codelineno-79-5 name=__codelineno-79-5 href=#__codelineno-79-5></a>        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
</span><span id=__span-79-6><a id=__codelineno-79-6 name=__codelineno-79-6 href=#__codelineno-79-6></a>             0,     0,     0,     0,     0,     0]])}
</span><span id=__span-79-7><a id=__codelineno-79-7 name=__codelineno-79-7 href=#__codelineno-79-7></a>{&#39;attention_mask&#39;: array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-79-8><a id=__codelineno-79-8 name=__codelineno-79-8 href=#__codelineno-79-8></a>       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
</span><span id=__span-79-9><a id=__codelineno-79-9 name=__codelineno-79-9 href=#__codelineno-79-9></a> &#39;input_ids&#39;: array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
</span><span id=__span-79-10><a id=__codelineno-79-10 name=__codelineno-79-10 href=#__codelineno-79-10></a>        12172,  2607,  2026,  2878,  2166,  1012,   102],
</span><span id=__span-79-11><a id=__codelineno-79-11 name=__codelineno-79-11 href=#__codelineno-79-11></a>       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,
</span><span id=__span-79-12><a id=__codelineno-79-12 name=__codelineno-79-12 href=#__codelineno-79-12></a>            0,     0,     0,     0,     0,     0,     0]])}
</span></code></pre></div></p> <h3 id=_30>特別なトークン<a class=headerlink href=#_30 title="Permanent link">&para;</a></h3> <p>トークナイザーによって返される入力IDを見ると、以前のものとは少し異なっていることがわかります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-80-1><a id=__codelineno-80-1 name=__codelineno-80-1 href=#__codelineno-80-1></a><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span>
</span><span id=__span-80-2><a id=__codelineno-80-2 name=__codelineno-80-2 href=#__codelineno-80-2></a>
</span><span id=__span-80-3><a id=__codelineno-80-3 name=__codelineno-80-3 href=#__codelineno-80-3></a><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-80-4><a id=__codelineno-80-4 name=__codelineno-80-4 href=#__codelineno-80-4></a><span class=nb>print</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span><span id=__span-80-5><a id=__codelineno-80-5 name=__codelineno-80-5 href=#__codelineno-80-5></a>
</span><span id=__span-80-6><a id=__codelineno-80-6 name=__codelineno-80-6 href=#__codelineno-80-6></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sequence</span><span class=p>)</span>
</span><span id=__span-80-7><a id=__codelineno-80-7 name=__codelineno-80-7 href=#__codelineno-80-7></a><span class=n>ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-80-8><a id=__codelineno-80-8 name=__codelineno-80-8 href=#__codelineno-80-8></a><span class=nb>print</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-81-1><a id=__codelineno-81-1 name=__codelineno-81-1 href=#__codelineno-81-1></a>[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
</span><span id=__span-81-2><a id=__codelineno-81-2 name=__codelineno-81-2 href=#__codelineno-81-2></a>[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
</span></code></pre></div></p> <p>1つのトークンIDが最初に追加され、1つが最後に追加されました。上記の2つのIDシーケンスをデコードして、これが何についてかを見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-82-1><a id=__codelineno-82-1 name=__codelineno-82-1 href=#__codelineno-82-1></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>model_inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]))</span>
</span><span id=__span-82-2><a id=__codelineno-82-2 name=__codelineno-82-2 href=#__codelineno-82-2></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>ids</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-83-1><a id=__codelineno-83-1 name=__codelineno-83-1 href=#__codelineno-83-1></a>[CLS] i&#39;ve been waiting for a huggingface course my whole life. [SEP]
</span><span id=__span-83-2><a id=__codelineno-83-2 name=__codelineno-83-2 href=#__codelineno-83-2></a>i&#39;ve been waiting for a huggingface course my whole life.
</span></code></pre></div></p> <p>トークナイザーは、最初に特別な単語<code>[CLS]</code>を、最後に特別な単語<code>[SEP]</code>を追加しました。これは、モデルがそれらで事前訓練されたためで、推論で同じ結果を得るためにそれらも追加する必要があります。一部のモデルは特別な単語を追加しないか、異なるものを追加することに注意してください。モデルは最初のみ、または最後のみにこれらの特別な単語を追加する場合もあります。いずれの場合も、トークナイザーはどれが期待されているかを知っており、これを処理してくれます。</p> <h3 id=_31>まとめ：トークナイザーからモデルまで<a class=headerlink href=#_31 title="Permanent link">&para;</a></h3> <p>テキストに適用されたときに<code>tokenizer</code>オブジェクトが使用するすべての個別ステップを見てきたので、複数のシーケンス（パディング！）、非常に長いシーケンス（切り捨て！）、および複数のタイプのテンソルをそのメインAPIで処理できる方法を最後にもう一度見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-84-1><a id=__codelineno-84-1 name=__codelineno-84-1 href=#__codelineno-84-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-84-2><a id=__codelineno-84-2 name=__codelineno-84-2 href=#__codelineno-84-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span><span id=__span-84-3><a id=__codelineno-84-3 name=__codelineno-84-3 href=#__codelineno-84-3></a>
</span><span id=__span-84-4><a id=__codelineno-84-4 name=__codelineno-84-4 href=#__codelineno-84-4></a><span class=n>checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id=__span-84-5><a id=__codelineno-84-5 name=__codelineno-84-5 href=#__codelineno-84-5></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-84-6><a id=__codelineno-84-6 name=__codelineno-84-6 href=#__codelineno-84-6></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>)</span>
</span><span id=__span-84-7><a id=__codelineno-84-7 name=__codelineno-84-7 href=#__codelineno-84-7></a><span class=n>sequences</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class=p>,</span> <span class=s2>&quot;So have I!&quot;</span><span class=p>]</span>
</span><span id=__span-84-8><a id=__codelineno-84-8 name=__codelineno-84-8 href=#__codelineno-84-8></a>
</span><span id=__span-84-9><a id=__codelineno-84-9 name=__codelineno-84-9 href=#__codelineno-84-9></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-84-10><a id=__codelineno-84-10 name=__codelineno-84-10 href=#__codelineno-84-10></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-84-11><a id=__codelineno-84-11 name=__codelineno-84-11 href=#__codelineno-84-11></a>
</span><span id=__span-84-12><a id=__codelineno-84-12 name=__codelineno-84-12 href=#__codelineno-84-12></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-84-13><a id=__codelineno-84-13 name=__codelineno-84-13 href=#__codelineno-84-13></a><span class=n>pprint</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>logits</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-85-1><a id=__codelineno-85-1 name=__codelineno-85-1 href=#__codelineno-85-1></a>{&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span><span id=__span-85-2><a id=__codelineno-85-2 name=__codelineno-85-2 href=#__codelineno-85-2></a>        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
</span><span id=__span-85-3><a id=__codelineno-85-3 name=__codelineno-85-3 href=#__codelineno-85-3></a> &#39;input_ids&#39;: tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
</span><span id=__span-85-4><a id=__codelineno-85-4 name=__codelineno-85-4 href=#__codelineno-85-4></a>          2607,  2026,  2878,  2166,  1012,   102],
</span><span id=__span-85-5><a id=__codelineno-85-5 name=__codelineno-85-5 href=#__codelineno-85-5></a>        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
</span><span id=__span-85-6><a id=__codelineno-85-6 name=__codelineno-85-6 href=#__codelineno-85-6></a>             0,     0,     0,     0,     0,     0]])}
</span><span id=__span-85-7><a id=__codelineno-85-7 name=__codelineno-85-7 href=#__codelineno-85-7></a>tensor([[-1.5607,  1.6123],
</span><span id=__span-85-8><a id=__codelineno-85-8 name=__codelineno-85-8 href=#__codelineno-85-8></a>        [-3.6183,  3.9137]], grad_fn=&lt;AddmmBackward0&gt;)
</span></code></pre></div></p> <h2 id=_32>まとめ<a class=headerlink href=#_32 title="Permanent link">&para;</a></h2> <p>この記事では、Hugging Face Transformersライブラリの基本的な使用方法について詳しく学習しました。主な学習ポイントを振り返ってみましょう：</p> <h3 id=_33>学習したポイント<a class=headerlink href=#_33 title="Permanent link">&para;</a></h3> <ol> <li><strong>pipelineの仕組み</strong>: 前処理（トークン化）→ モデル処理 → 後処理の3段階構成</li> <li><strong>トークナイザーの役割</strong>: テキストを数値に変換し、パディングや切り捨てを処理</li> <li><strong>モデルの操作</strong>: 事前訓練済みモデルの読み込み、保存、推論の実行</li> <li><strong>バッチ処理</strong>: 複数シーケンスの効率的な処理と<code>attention mask</code>の重要性</li> <li><strong>統一API</strong>: <code>tokenizer()</code>関数による高レベルな操作の簡略化</li> </ol> <h2 id=_34>参考資料<a class=headerlink href=#_34 title="Permanent link">&para;</a></h2> <ul> <li><a href=https://huggingface.co/docs/transformers>Hugging Face Transformers ドキュメント</a></li> <li><a href=https://huggingface.co/models>Model Hub</a> - 事前訓練済みモデルの探索</li> <li><a href=https://huggingface.co/docs/datasets>Datasets ライブラリ</a> - データセットの処理</li> <li><a href=https://huggingface.co/docs/tokenizers>Tokenizers ライブラリ</a> - 高速トークン化</li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/meta.js></script> <script src=../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>