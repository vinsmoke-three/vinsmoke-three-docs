<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=DistilBERTを映画レビューデータでファインチューニングし、ドメイン適応によって言語モデルの性能を向上させる手法を詳しく解説します。><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/LLM/ClassicalNLP/72_masked_language_modeling/ rel=canonical><link href=../71_token_classification/ rel=prev><link href=../73_translation/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>マスク言語モデルのファインチューニング - ドメイン適応による性能向上 - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> マスク言語モデルのファインチューニング - ドメイン適応による性能向上 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../PyTorch/00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../PyTorch/00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/07_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/09_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> </li> <li class=md-nav__item> <a href=../../../PyTorch/10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../../01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class=md-nav__item> <a href=../../02_using_transformers/ class=md-nav__link> <span class=md-ellipsis> 2. Using transformers </span> </a> </li> <li class=md-nav__item> <a href=../../03_fine_tuning_a_pretrained_model/ class=md-nav__link> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> </li> <li class=md-nav__item> <a href=../../04_the_huggingface_tokenizers_library/ class=md-nav__link> <span class=md-ellipsis> 4. Tokenizers library </span> </a> </li> <li class=md-nav__item> <a href=../../05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../../06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8 checked> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=true> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Masked Language Modeling </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Masked Language Modeling </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> ドメイン適応の必要性 </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 事前学習済みモデルの選択 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> データセットの準備 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> データの前処理 </span> </a> </li> <li class=md-nav__item> <a href=#trainer-apidistilbert class=md-nav__link> <span class=md-ellipsis> Trainer APIを使ったDistilBERTのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#perplexity class=md-nav__link> <span class=md-ellipsis> 言語モデルの困惑度（Perplexity）評価 </span> </a> </li> <li class=md-nav__item> <a href=#hugging-face-acceleratedistilbert class=md-nav__link> <span class=md-ellipsis> Hugging Face Accelerateを使ったDistilBERTのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#_20 class=md-nav__link> <span class=md-ellipsis> ファインチューニング済みモデルの使用 </span> </a> </li> <li class=md-nav__item> <a href=#_21 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> ドメイン適応の必要性 </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 事前学習済みモデルの選択 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> データセットの準備 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> データの前処理 </span> </a> </li> <li class=md-nav__item> <a href=#trainer-apidistilbert class=md-nav__link> <span class=md-ellipsis> Trainer APIを使ったDistilBERTのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#perplexity class=md-nav__link> <span class=md-ellipsis> 言語モデルの困惑度（Perplexity）評価 </span> </a> </li> <li class=md-nav__item> <a href=#hugging-face-acceleratedistilbert class=md-nav__link> <span class=md-ellipsis> Hugging Face Accelerateを使ったDistilBERTのファインチューニング </span> </a> </li> <li class=md-nav__item> <a href=#_20 class=md-nav__link> <span class=md-ellipsis> ファインチューニング済みモデルの使用 </span> </a> </li> <li class=md-nav__item> <a href=#_21 class=md-nav__link> <span class=md-ellipsis> 参考資料 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=_1>マスク言語モデルのファインチューニング<a class=headerlink href=#_1 title="Permanent link">&para;</a></h1> <h2 id=_2>概要<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <p>この記事では、Transformerモデルのマスク言語モデリング（Masked Language Modeling）を使ったファインチューニングについて学習します。特に、事前学習済みのDistilBERTモデルを映画レビューデータでファインチューニングし、ドメイン適応（Domain Adaptation）を実現する手法を実践的に解説します。</p> <div class="admonition info"> <p class=admonition-title>参考資料</p> <p>本ドキュメントは <a href=https://huggingface.co/learn/llm-course/chapter7/3>Hugging Face LLM Course</a> を参考に、日本語で学習内容をまとめた個人的な学習ノートです。詳細な内容や最新情報については、原文も併せてご参照ください。</p> </div> <h2 id=_3>前提知識<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <ul> <li>Pythonプログラミングの基礎知識</li> <li>機械学習とディープラーニングの基本概念</li> <li>Transformerアーキテクチャの理解</li> <li>PyTorchまたはTensorFlowの基本的な使い方</li> </ul> <h2 id=_4>ドメイン適応の必要性<a class=headerlink href=#_4 title="Permanent link">&para;</a></h2> <p>多くのNLPアプリケーションでは、Hugging Face Hubの事前学習済みモデルを直接ファインチューニングするだけで良い結果が得られます。しかし、以下のような場合には、まず言語モデルをドメイン固有のデータでファインチューニングしてから、タスク固有のヘッドを学習させる必要があります。</p> <ul> <li><strong>法律文書</strong>: 法的専門用語が多く含まれている</li> <li><strong>科学論文</strong>: 専門的な学術用語が頻出している</li> <li><strong>医療記録</strong>: 医療固有の略語や用語が使用されている</li> </ul> <p>このような場合、BERTのような汎用Transformerモデルは、ドメイン固有の単語を稀少トークン（rare tokens）として扱い、期待する性能が得られない可能性があります。</p> <p>ドメイン内データで言語モデルをファインチューニングすることで、多くの下流タスクの性能を向上させることができ、通常この処理は一度だけ実行すれば済みます。</p> <p>この手法は<strong>ドメイン適応</strong>と呼ばれ、2018年にULMFiTによって普及しました。ULMFiTは、NLPにおける転移学習を実用化した最初のニューラルアーキテクチャ（LSTM基盤）の一つでした。</p> <h2 id=_5>事前学習済みモデルの選択<a class=headerlink href=#_5 title="Permanent link">&para;</a></h2> <p>マスク言語モデリング用の適切な事前学習済みモデルを選択しましょう。Hugging Face Hubでは、「Fill-Mask」フィルターを適用することで候補を見つけることができます。</p> <p>BERTやRoBERTaファミリのモデルが最もダウンロードされていますが、今回は<strong>DistilBERT</strong>を使用します。このモデルは、下流タスクでの性能をほとんど損なうことなく、はるかに高速に学習できます。</p> <h3 id=distilbert>DistilBERTの特徴<a class=headerlink href=#distilbert title="Permanent link">&para;</a></h3> <p>DistilBERTは<strong>知識蒸留（Knowledge Distillation）</strong>という特別な技術を使って訓練されました。</p> <ul> <li><strong>教師モデル</strong>: BERT（大規模モデル）</li> <li><strong>生徒モデル</strong>: DistilBERT（パラメータ数が大幅に削減されたモデル）</li> <li><strong>結果</strong>: 性能をほぼ維持しながら、約2倍の高速化を実現している</li> </ul> <p>まず、<code>AutoModelForMaskedLM</code>クラスを使ってDistilBERTをダウンロードしましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForMaskedLM</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=c1># DistilBERTの事前学習済みモデルを読み込み</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>model_checkpoint</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased&quot;</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForMaskedLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_checkpoint</span><span class=p>)</span>
</span></code></pre></div> <p>モデルのパラメータ数を確認してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># モデルのパラメータ数を計算（百万単位）</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>distilbert_num_parameters</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>num_parameters</span><span class=p>()</span> <span class=o>/</span> <span class=mi>1_000_000</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; DistilBERT パラメータ数: </span><span class=si>{</span><span class=nb>round</span><span class=p>(</span><span class=n>distilbert_num_parameters</span><span class=p>)</span><span class=si>}</span><span class=s2>M&#39;&quot;</span><span class=p>)</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; BERT パラメータ数: 110M&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>&#39;&gt;&gt;&gt; DistilBERT パラメータ数: 67M&#39;
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>&#39;&gt;&gt;&gt; BERT パラメータ数: 110M&#39;
</span></code></pre></div></p> <p>約6700万のパラメータを持つDistilBERTは、BERTベースモデルの約2分の1のサイズで、学習時間も約2倍高速化されます。</p> <h3 id=_6>マスク予測のテスト<a class=headerlink href=#_6 title="Permanent link">&para;</a></h3> <p>DistilBERTがどのようなトークンを予測するか確認してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># テスト用のサンプルテキスト</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>text</span> <span class=o>=</span> <span class=s2>&quot;This is a great [MASK].&quot;</span>
</span></code></pre></div> <p>人間が考えると、<code>[MASK]</code>の位置には「day」、「ride」、「painting」など様々な可能性があります。事前学習済みモデルの予測は、学習に使用されたコーパスに依存します。DistilBERTは、BERTと同様に英語Wikipediaおよび BookCorpusデータセットで事前学習されているため、これらのドメインを反映した予測が期待されます。</p> <p>マスクを予測するためには、DistilBERTのトークナイザーも必要です。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># 対応するトークナイザーを読み込み</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_checkpoint</span><span class=p>)</span>
</span></code></pre></div> <p>トークナイザーとモデルを使って、上位5つの候補を予測してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=c1># テキストをトークン化してモデルに入力</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=n>token_logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a><span class=c1># [MASK]トークンの位置を特定し、そのロジットを抽出</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=n>mask_token_index</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span> <span class=o>==</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>mask_token_id</span><span class=p>)[</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a><span class=n>mask_token_logits</span> <span class=o>=</span> <span class=n>token_logits</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>mask_token_index</span><span class=p>,</span> <span class=p>:]</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=c1># 最も高いロジットを持つ上位5つの[MASK]候補を取得</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=n>top_5_tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>mask_token_logits</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>indices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a><span class=c1># 結果を表示</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a><span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=n>top_5_tokens</span><span class=p>:</span>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; </span><span class=si>{</span><span class=n>text</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>mask_token</span><span class=p>,</span><span class=w> </span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>token</span><span class=p>]))</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>&#39;&gt;&gt;&gt; This is a great deal.&#39;
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>&#39;&gt;&gt;&gt; This is a great success.&#39;
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>&#39;&gt;&gt;&gt; This is a great adventure.&#39;
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>&#39;&gt;&gt;&gt; This is a great idea.&#39;
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>&#39;&gt;&gt;&gt; This is a great feat.&#39;
</span></code></pre></div></p> <p>出力から分かるように、モデルの予測は日常的な用語を反映しており、これは英語Wikipediaという基盤を考えると驚くことではありません。次に、このドメインをより特化したもの（極めて偏った映画レビュー）に変更する方法を見ていきましょう。</p> <h2 id=_7>データセットの準備<a class=headerlink href=#_7 title="Permanent link">&para;</a></h2> <p>ドメイン適応を実演するために、有名な<strong>Large Movie Review Dataset</strong>（IMDbデータセット）を使用します。これは映画レビューのコーパスで、感情分析モデルのベンチマークによく使用されます。</p> <p>このコーパスでDistilBERTをファインチューニングすることで、言語モデルが事前学習で使用したWikipediaの事実的データから、映画レビューのより主観的な要素へと語彙を適応させることが期待されます。</p> <p>Hugging Face Hubからデータを取得しましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=c1># IMDbデータセットを読み込み</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=n>imdb_dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;imdb&quot;</span><span class=p>)</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=n>imdb_dataset</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>DatasetDict({
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>    train: Dataset({
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>        features: [&#39;text&#39;, &#39;label&#39;],
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>        num_rows: 25000
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>    })
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>    test: Dataset({
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a>        features: [&#39;text&#39;, &#39;label&#39;],
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a>        num_rows: 25000
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a>    })
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a>    unsupervised: Dataset({
</span><span id=__span-8-11><a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a>        features: [&#39;text&#39;, &#39;label&#39;],
</span><span id=__span-8-12><a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a>        num_rows: 50000
</span><span id=__span-8-13><a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a>    })
</span><span id=__span-8-14><a id=__codelineno-8-14 name=__codelineno-8-14 href=#__codelineno-8-14></a>})
</span></code></pre></div></p> <p><code>train</code>と<code>test</code>分割にはそれぞれ25,000件のレビューが含まれ、<code>unsupervised</code>という名前のラベルなし分割には50,000件のレビューが含まれています。</p> <p>データの内容を確認してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=c1># ランダムに3つのサンプルを選択</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=n>sample</span> <span class=o>=</span> <span class=n>imdb_dataset</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>))</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a><span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>sample</span><span class=p>:</span>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&#39;&gt;&gt;&gt; レビュー: </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; ラベル: </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;label&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>&#39;&gt;&gt;&gt; レビュー: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier&#39;s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have &quot;clairvoyance&quot;. People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it&#39;s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...&#39;
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>&#39;&gt;&gt;&gt; ラベル: 1&#39;
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>&#39;&gt;&gt;&gt; レビュー: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called &quot;when you stub your toe on the moon&quot; It reminds me of Sinatra&#39;s song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir &quot;Saggy&quot; Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.&#39;
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>&#39;&gt;&gt;&gt; ラベル: 1&#39;
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>&#39;&gt;&gt;&gt; レビュー: George P. Cosmatos&#39; &quot;Rambo: First Blood Part II&quot; is pure wish-fulfillment. The United States clearly didn&#39;t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn&#39;t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. &quot;We Were Soldiers&quot;) also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. &quot;Do we get to win this time?&quot; Yes, you do.&#39;
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>&#39;&gt;&gt;&gt; ラベル: 0&#39;
</span></code></pre></div></p> <p>これらは確実に映画レビューです。言語モデリングにはラベルは必要ありませんが、<code>0</code>がネガティブレビュー、<code>1</code>がポジティブレビューを表していることが分かります。</p> <h2 id=_8>データの前処理<a class=headerlink href=#_8 title="Permanent link">&para;</a></h2> <p>自動回帰言語モデリングとマスク言語モデリングの両方において、一般的な前処理ステップは、すべての例を連結してから、全体のコーパスを等しいサイズのチャンクに分割することです。これは、単純に個別の例をトークン化する通常のアプローチとは大きく異なります。</p> <p>なぜすべてを連結するのでしょうか。個別の例が長すぎると切り捨てられ、言語モデリングタスクに有用な情報が失われる可能性があるためです。</p> <h3 id=_9>トークン化の実装<a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <p>まず、コーパスを通常通りトークン化しますが、トークナイザーで<code>truncation=True</code>オプションは設定<strong>しません</strong>。後で全単語マスキングに必要なword IDも取得します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize_function</span><span class=p>(</span><span class=n>examples</span><span class=p>):</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>    <span class=c1># テキストをトークン化（切り捨てなし）</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>    <span class=n>result</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>examples</span><span class=p>[</span><span class=s2>&quot;text&quot;</span><span class=p>])</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>    <span class=c1># 高速トークナイザーの場合、word IDsを取得</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>    <span class=k>if</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>is_fast</span><span class=p>:</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>        <span class=n>result</span><span class=p>[</span><span class=s2>&quot;word_ids&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=n>result</span><span class=o>.</span><span class=n>word_ids</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>result</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]))]</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a>    <span class=k>return</span> <span class=n>result</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a><span class=c1># 高速マルチスレッド処理を有効化</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a><span class=n>tokenized_datasets</span> <span class=o>=</span> <span class=n>imdb_dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a>    <span class=n>tokenize_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>remove_columns</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;text&quot;</span><span class=p>,</span> <span class=s2>&quot;label&quot;</span><span class=p>]</span>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a><span class=p>)</span>
</span><span id=__span-11-13><a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a><span class=n>tokenized_datasets</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>DatasetDict({
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>    train: Dataset({
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;],
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>        num_rows: 25000
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    })
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>    test: Dataset({
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;],
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>        num_rows: 25000
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a>    })
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>    unsupervised: Dataset({
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;],
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>        num_rows: 50000
</span><span id=__span-12-13><a id=__codelineno-12-13 name=__codelineno-12-13 href=#__codelineno-12-13></a>    })
</span><span id=__span-12-14><a id=__codelineno-12-14 name=__codelineno-12-14 href=#__codelineno-12-14></a>})
</span></code></pre></div></p> <p>DistilBERTはBERT系モデルなので、エンコードされたテキストは<code>input_ids</code>、<code>attention_mask</code>、および追加した<code>word_ids</code>から構成されています。</p> <h3 id=_10>チャンクサイズの決定<a class=headerlink href=#_10 title="Permanent link">&para;</a></h3> <p>映画レビューをトークン化したので、次にそれらをすべてグループ化し、結果をチャンクに分割します。チャンクのサイズはどの程度にすべきでしょうか。これは最終的に利用可能なGPUメモリ量によって決まりますが、良い出発点はモデルの最大コンテキストサイズを確認することです。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># モデルの最大コンテキストサイズを確認</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>model_max_length</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>512
</span></code></pre></div></p> <p>この値は、チェックポイントに関連付けられた<em>tokenizer_config.json</em>ファイルから取得されています。この場合、BERTと同様にコンテキストサイズが512トークンであることが分かります。</p> <h3 id=_11>連結処理のデモンストレーション<a class=headerlink href=#_11 title="Permanent link">&para;</a></h3> <p>連結がどのように機能するかを示すために、トークン化された訓練セットからいくつかのレビューを取得し、レビューごとのトークン数を出力してみましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=c1># スライシングにより各特徴のリストのリストを生成</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=n>tokenized_samples</span> <span class=o>=</span> <span class=n>tokenized_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][:</span><span class=mi>3</span><span class=p>]</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a><span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>sample</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>tokenized_samples</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]):</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; レビュー </span><span class=si>{</span><span class=n>idx</span><span class=si>}</span><span class=s2> の長さ: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>sample</span><span class=p>)</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a>&#39;&gt;&gt;&gt; レビュー 0 の長さ: 363&#39;
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>&#39;&gt;&gt;&gt; レビュー 1 の長さ: 304&#39;
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>&#39;&gt;&gt;&gt; レビュー 2 の長さ: 133&#39;
</span></code></pre></div></p> <p>これらすべての例を、次のような単純な辞書内包表記で連結できます。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=c1># すべての例を連結</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a><span class=n>concatenated_examples</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>    <span class=n>k</span><span class=p>:</span> <span class=nb>sum</span><span class=p>(</span><span class=n>tokenized_samples</span><span class=p>[</span><span class=n>k</span><span class=p>],</span> <span class=p>[])</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>tokenized_samples</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a><span class=p>}</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a><span class=n>total_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>concatenated_examples</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; 連結されたレビューの長さ: </span><span class=si>{</span><span class=n>total_length</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a>&#39;&gt;&gt;&gt; 連結されたレビューの長さ: 800&#39;
</span></code></pre></div></p> <p>素晴らしいです。総長さが合っています。次に、連結されたレビューを<code>chunk_size</code>で指定されたサイズのチャンクに分割しましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=c1># チャンクサイズを設定</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=n>chunk_size</span> <span class=o>=</span> <span class=mi>128</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a><span class=c1># 連結された例の特徴ごとにチャンクを作成</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a><span class=n>chunks</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a>    <span class=n>k</span><span class=p>:</span> <span class=p>[</span><span class=n>t</span><span class=p>[</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=n>chunk_size</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>total_length</span><span class=p>,</span> <span class=n>chunk_size</span><span class=p>)]</span>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a>    <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>concatenated_examples</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a><span class=p>}</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a><span class=c1># 各チャンクの長さを確認</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a><span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>chunks</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]:</span>
</span><span id=__span-19-12><a id=__codelineno-19-12 name=__codelineno-19-12 href=#__codelineno-19-12></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&#39;&gt;&gt;&gt; チャンク長: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>chunk</span><span class=p>)</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a>&#39;&gt;&gt;&gt; チャンク長: 128&#39;
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a>&#39;&gt;&gt;&gt; チャンク長: 32&#39;
</span></code></pre></div></p> <p>この例で見られるように、最後のチャンクは通常、最大チャンクサイズよりも小さくなります。これに対処する主な戦略は2つあります。</p> <ol> <li><strong>削除</strong>: 最後のチャンクが<code>chunk_size</code>より小さい場合は削除する</li> <li><strong>パディング</strong>: 最後のチャンクの長さが<code>chunk_size</code>と等しくなるまでパディングする</li> </ol> <p>ここでは最初のアプローチを採用します。上記のロジックをすべて一つの関数にまとめて、トークン化されたデータセットに適用できるようにしましょう。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=k>def</span><span class=w> </span><span class=nf>group_texts</span><span class=p>(</span><span class=n>examples</span><span class=p>):</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>    <span class=c1># すべてのテキストを連結</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>    <span class=n>concatenated_examples</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=nb>sum</span><span class=p>(</span><span class=n>examples</span><span class=p>[</span><span class=n>k</span><span class=p>],</span> <span class=p>[])</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>examples</span><span class=o>.</span><span class=n>keys</span><span class=p>()}</span>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>    <span class=c1># 連結されたテキストの長さを計算</span>
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a>    <span class=n>total_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>concatenated_examples</span><span class=p>[</span><span class=nb>list</span><span class=p>(</span><span class=n>examples</span><span class=o>.</span><span class=n>keys</span><span class=p>())[</span><span class=mi>0</span><span class=p>]])</span>
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a>    <span class=c1># 最後のチャンクがchunk_sizeより小さい場合は削除</span>
</span><span id=__span-21-7><a id=__codelineno-21-7 name=__codelineno-21-7 href=#__codelineno-21-7></a>    <span class=n>total_length</span> <span class=o>=</span> <span class=p>(</span><span class=n>total_length</span> <span class=o>//</span> <span class=n>chunk_size</span><span class=p>)</span> <span class=o>*</span> <span class=n>chunk_size</span>
</span><span id=__span-21-8><a id=__codelineno-21-8 name=__codelineno-21-8 href=#__codelineno-21-8></a>    <span class=c1># max_lenのチャンクに分割</span>
</span><span id=__span-21-9><a id=__codelineno-21-9 name=__codelineno-21-9 href=#__codelineno-21-9></a>    <span class=n>result</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-21-10><a id=__codelineno-21-10 name=__codelineno-21-10 href=#__codelineno-21-10></a>        <span class=n>k</span><span class=p>:</span> <span class=p>[</span><span class=n>t</span><span class=p>[</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=n>chunk_size</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>total_length</span><span class=p>,</span> <span class=n>chunk_size</span><span class=p>)]</span>
</span><span id=__span-21-11><a id=__codelineno-21-11 name=__codelineno-21-11 href=#__codelineno-21-11></a>        <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>concatenated_examples</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-21-12><a id=__codelineno-21-12 name=__codelineno-21-12 href=#__codelineno-21-12></a>    <span class=p>}</span>
</span><span id=__span-21-13><a id=__codelineno-21-13 name=__codelineno-21-13 href=#__codelineno-21-13></a>    <span class=c1># 新しいlabelsカラムを作成</span>
</span><span id=__span-21-14><a id=__codelineno-21-14 name=__codelineno-21-14 href=#__codelineno-21-14></a>    <span class=n>result</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span><span id=__span-21-15><a id=__codelineno-21-15 name=__codelineno-21-15 href=#__codelineno-21-15></a>    <span class=k>return</span> <span class=n>result</span>
</span></code></pre></div> <p><code>group_texts()</code>の最後のステップで、<code>input_ids</code>のコピーである新しい<code>labels</code>カラムを作成していることに注目してください。これは、マスク言語モデリングでは、入力バッチ内のランダムにマスクされたトークンを予測することが目的であり、<code>labels</code>カラムを作成することで、言語モデルが学習するための正解を提供するためです。</p> <p>信頼できる<code>Dataset.map()</code>関数を使用して、<code>group_texts()</code>をトークン化されたデータセットに適用しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=c1># グループ化されたテキストに変換</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a><span class=n>lm_datasets</span> <span class=o>=</span> <span class=n>tokenized_datasets</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>group_texts</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a><span class=n>lm_datasets</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>DatasetDict({
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>    train: Dataset({
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;, &#39;labels&#39;],
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>        num_rows: 61291
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a>    })
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a>    test: Dataset({
</span><span id=__span-23-7><a id=__codelineno-23-7 name=__codelineno-23-7 href=#__codelineno-23-7></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;, &#39;labels&#39;],
</span><span id=__span-23-8><a id=__codelineno-23-8 name=__codelineno-23-8 href=#__codelineno-23-8></a>        num_rows: 59904
</span><span id=__span-23-9><a id=__codelineno-23-9 name=__codelineno-23-9 href=#__codelineno-23-9></a>    })
</span><span id=__span-23-10><a id=__codelineno-23-10 name=__codelineno-23-10 href=#__codelineno-23-10></a>    unsupervised: Dataset({
</span><span id=__span-23-11><a id=__codelineno-23-11 name=__codelineno-23-11 href=#__codelineno-23-11></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;, &#39;labels&#39;],
</span><span id=__span-23-12><a id=__codelineno-23-12 name=__codelineno-23-12 href=#__codelineno-23-12></a>        num_rows: 122957
</span><span id=__span-23-13><a id=__codelineno-23-13 name=__codelineno-23-13 href=#__codelineno-23-13></a>    })
</span><span id=__span-23-14><a id=__codelineno-23-14 name=__codelineno-23-14 href=#__codelineno-23-14></a>})
</span></code></pre></div></p> <p>テキストをグループ化してからチャンク化することで、元の<code>train</code>および<code>test</code>分割の25,000よりもはるかに多くの例が生成されました。これは、元のコーパスの複数の例にまたがる<strong>連続するトークン</strong>を含む例ができたためです。</p> <p>チャンクの一つで特別な<code>[SEP]</code>と<code>[CLS]</code>トークンを探すことで、これを明示的に確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=c1># チャンクの内容を確認</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>lm_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=mi>1</span><span class=p>][</span><span class=s2>&quot;input_ids&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a>&quot;as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. &lt; br / &gt; &lt; br / &gt; what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it &#39; s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,&quot;
</span></code></pre></div></p> <p>この例では、高校映画に関するレビューとホームレスに関するレビューという、2つの重複する映画レビューが確認できます。</p> <p>マスク言語モデリング用のラベルがどのようになっているかも確認してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=c1># ラベルの内容を確認</span>
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>lm_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=mi>1</span><span class=p>][</span><span class=s2>&quot;labels&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a>&quot;as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. &lt; br / &gt; &lt; br / &gt; what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it &#39; s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,&quot;
</span></code></pre></div></p> <p>上記の<code>group_texts()</code>関数から期待されるように、これはデコードされた<code>input_ids</code>と同じに見えます。しかし、どうやってモデルが何かを学習できるのでしょうか？重要なステップが欠けています：入力のランダムな位置に<code>[MASK]</code>トークンを挿入することです！ファインチューニング中に特別なデータコレーターを使用してこれをその場で行う方法を見ていきましょう。</p> <h2 id=trainer-apidistilbert>Trainer APIを使ったDistilBERTのファインチューニング<a class=headerlink href=#trainer-apidistilbert title="Permanent link">&para;</a></h2> <p>マスク言語モデルのファインチューニングは、第3章で行った系列分類モデルのファインチューニングとほぼ同じです。唯一の違いは、各テキストバッチでいくつかのトークンをランダムにマスクできる特別なデータコレーターが必要なことです。</p> <p>幸い、Hugging Face Transformersには、まさにこのタスク用の専用<code>DataCollatorForLanguageModeling</code>が用意されています。トークナイザーと、マスクするトークンの割合を指定する<code>mlm_probability</code>引数を渡すだけです。BERTで使用され、文献でも一般的な15%を選択します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>DataCollatorForLanguageModeling</span>
</span><span id=__span-28-2><a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a>
</span><span id=__span-28-3><a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a><span class=c1># マスク言語モデリング用のデータコレーターを作成</span>
</span><span id=__span-28-4><a id=__codelineno-28-4 name=__codelineno-28-4 href=#__codelineno-28-4></a><span class=c1># 15%の確率でトークンをマスク</span>
</span><span id=__span-28-5><a id=__codelineno-28-5 name=__codelineno-28-5 href=#__codelineno-28-5></a><span class=n>data_collator</span> <span class=o>=</span> <span class=n>DataCollatorForLanguageModeling</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span> <span class=n>mlm_probability</span><span class=o>=</span><span class=mf>0.15</span><span class=p>)</span>
</span></code></pre></div> <h3 id=_12>ランダムマスキングの動作確認<a class=headerlink href=#_12 title="Permanent link">&para;</a></h3> <p>ランダムマスキングがどのように機能するかを確認するために、いくつかの例をデータコレーターに供給してみましょう。これは<code>dict</code>のリストを期待し、各<code>dict</code>は連続するテキストの単一のチャンクを表します。このデータコレーターは<code>"word_ids"</code>キーを期待しないため、削除してからバッチをコレーターに供給します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=c1># サンプルデータを準備</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a><span class=n>samples</span> <span class=o>=</span> <span class=p>[</span><span class=n>lm_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>)]</span>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a><span class=k>for</span> <span class=n>sample</span> <span class=ow>in</span> <span class=n>samples</span><span class=p>:</span>
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a>    <span class=n>_</span> <span class=o>=</span> <span class=n>sample</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;word_ids&quot;</span><span class=p>)</span>
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a><span class=c1># データコレーターを適用してマスキング結果を確認</span>
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a><span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>data_collator</span><span class=p>(</span><span class=n>samples</span><span class=p>)[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]:</span>
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&#39;&gt;&gt;&gt; </span><span class=si>{</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>chunk</span><span class=p>)</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a>&#39;&gt;&gt;&gt; [CLS] i rented i am curious - yellow from my [MASK] store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was [MASK] byÂ² 1915 s. customs if [MASK] ever tried to enter this country [MASK] therefore [MASK] heiress fan of films considered &quot; controversial &quot; i [MASK] had to see this for myself. &lt; br collier &gt; [MASK] br / &gt; the plot [MASK] centered around a [MASK] swedish drama student named lena who wants to learn everything she can about life. in [MASK] she wants to focus her attention 407 to [MASK] some sort of documentary on what the [MASK] totalede thought about certain political issues such&#39;
</span><span id=__span-30-2><a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a>
</span><span id=__span-30-3><a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a>&#39;&gt;&gt;&gt; as the vietnam war and race issues in the united states. in between asking politicians and [MASK] den [MASK]ns of stockholm about [MASK] opinions on politics, she namesake sex with her drama teacher, classmates, and married men. &lt; br / &gt; &lt; br / [MASK] [MASK] kills me [MASK] i am [MASK] - yellow is that 40 years ago, this was considered pornographic. really, the [MASK] and nudity scenes are few and far between, even then it [MASK] s not [MASK] like some cheaply made porno. while my countrymen [MASK] find [MASK] shocking, in reality [MASK] and nudity are a major staple in swedish [MASK]. even ing caucasian bergman,&#39;
</span></code></pre></div></p> <p>素晴らしい！うまく機能しています。<code>[MASK]</code>トークンがテキストの様々な場所にランダムに挿入されているのが確認できます。これらが、訓練中にモデルが予測しなければならないトークンになります。データコレーターの美しさは、すべてのバッチで<code>[MASK]</code>の挿入をランダム化することです！</p> <h3 id=whole-word-masking>全単語マスキング（Whole Word Masking）<a class=headerlink href=#whole-word-masking title="Permanent link">&para;</a></h3> <p>マスク言語モデリングで使用できる一つの技術は、個別のトークンではなく、単語全体を一緒にマスクすることです。このアプローチは<strong>全単語マスキング</strong>と呼ばれます。</p> <p>全単語マスキングを使用したい場合は、データコレーターを自分で構築する必要があります。データコレーターは、サンプルのリストを受け取ってバッチに変換する関数です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=kn>import</span><span class=w> </span><span class=nn>collections</span>
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>default_data_collator</span>
</span><span id=__span-31-4><a id=__codelineno-31-4 name=__codelineno-31-4 href=#__codelineno-31-4></a>
</span><span id=__span-31-5><a id=__codelineno-31-5 name=__codelineno-31-5 href=#__codelineno-31-5></a><span class=c1># 全単語マスキングの確率</span>
</span><span id=__span-31-6><a id=__codelineno-31-6 name=__codelineno-31-6 href=#__codelineno-31-6></a><span class=n>wwm_probability</span> <span class=o>=</span> <span class=mf>0.2</span>
</span><span id=__span-31-7><a id=__codelineno-31-7 name=__codelineno-31-7 href=#__codelineno-31-7></a>
</span><span id=__span-31-8><a id=__codelineno-31-8 name=__codelineno-31-8 href=#__codelineno-31-8></a><span class=k>def</span><span class=w> </span><span class=nf>whole_word_masking_data_collator</span><span class=p>(</span><span class=n>features</span><span class=p>):</span>
</span><span id=__span-31-9><a id=__codelineno-31-9 name=__codelineno-31-9 href=#__codelineno-31-9></a>    <span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=n>features</span><span class=p>:</span>
</span><span id=__span-31-10><a id=__codelineno-31-10 name=__codelineno-31-10 href=#__codelineno-31-10></a>        <span class=n>word_ids</span> <span class=o>=</span> <span class=n>feature</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;word_ids&quot;</span><span class=p>)</span>
</span><span id=__span-31-11><a id=__codelineno-31-11 name=__codelineno-31-11 href=#__codelineno-31-11></a>
</span><span id=__span-31-12><a id=__codelineno-31-12 name=__codelineno-31-12 href=#__codelineno-31-12></a>        <span class=c1># 単語と対応するトークンインデックス間のマップを作成</span>
</span><span id=__span-31-13><a id=__codelineno-31-13 name=__codelineno-31-13 href=#__codelineno-31-13></a>        <span class=n>mapping</span> <span class=o>=</span> <span class=n>collections</span><span class=o>.</span><span class=n>defaultdict</span><span class=p>(</span><span class=nb>list</span><span class=p>)</span>
</span><span id=__span-31-14><a id=__codelineno-31-14 name=__codelineno-31-14 href=#__codelineno-31-14></a>        <span class=n>current_word_index</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>
</span><span id=__span-31-15><a id=__codelineno-31-15 name=__codelineno-31-15 href=#__codelineno-31-15></a>        <span class=n>current_word</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-31-16><a id=__codelineno-31-16 name=__codelineno-31-16 href=#__codelineno-31-16></a>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>word_ids</span><span class=p>):</span>
</span><span id=__span-31-17><a id=__codelineno-31-17 name=__codelineno-31-17 href=#__codelineno-31-17></a>            <span class=k>if</span> <span class=n>word_id</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-31-18><a id=__codelineno-31-18 name=__codelineno-31-18 href=#__codelineno-31-18></a>                <span class=k>if</span> <span class=n>word_id</span> <span class=o>!=</span> <span class=n>current_word</span><span class=p>:</span>
</span><span id=__span-31-19><a id=__codelineno-31-19 name=__codelineno-31-19 href=#__codelineno-31-19></a>                    <span class=n>current_word</span> <span class=o>=</span> <span class=n>word_id</span>
</span><span id=__span-31-20><a id=__codelineno-31-20 name=__codelineno-31-20 href=#__codelineno-31-20></a>                    <span class=n>current_word_index</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-31-21><a id=__codelineno-31-21 name=__codelineno-31-21 href=#__codelineno-31-21></a>                <span class=n>mapping</span><span class=p>[</span><span class=n>current_word_index</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span><span id=__span-31-22><a id=__codelineno-31-22 name=__codelineno-31-22 href=#__codelineno-31-22></a>
</span><span id=__span-31-23><a id=__codelineno-31-23 name=__codelineno-31-23 href=#__codelineno-31-23></a>        <span class=c1># 単語をランダムにマスク</span>
</span><span id=__span-31-24><a id=__codelineno-31-24 name=__codelineno-31-24 href=#__codelineno-31-24></a>        <span class=n>mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>wwm_probability</span><span class=p>,</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>mapping</span><span class=p>),))</span>
</span><span id=__span-31-25><a id=__codelineno-31-25 name=__codelineno-31-25 href=#__codelineno-31-25></a>        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>feature</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span>
</span><span id=__span-31-26><a id=__codelineno-31-26 name=__codelineno-31-26 href=#__codelineno-31-26></a>        <span class=n>labels</span> <span class=o>=</span> <span class=n>feature</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span>
</span><span id=__span-31-27><a id=__codelineno-31-27 name=__codelineno-31-27 href=#__codelineno-31-27></a>        <span class=n>new_labels</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>100</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span>
</span><span id=__span-31-28><a id=__codelineno-31-28 name=__codelineno-31-28 href=#__codelineno-31-28></a>        <span class=k>for</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>mask</span><span class=p>)[</span><span class=mi>0</span><span class=p>]:</span>
</span><span id=__span-31-29><a id=__codelineno-31-29 name=__codelineno-31-29 href=#__codelineno-31-29></a>            <span class=n>word_id</span> <span class=o>=</span> <span class=n>word_id</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span><span id=__span-31-30><a id=__codelineno-31-30 name=__codelineno-31-30 href=#__codelineno-31-30></a>            <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>mapping</span><span class=p>[</span><span class=n>word_id</span><span class=p>]:</span>
</span><span id=__span-31-31><a id=__codelineno-31-31 name=__codelineno-31-31 href=#__codelineno-31-31></a>                <span class=n>new_labels</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span><span id=__span-31-32><a id=__codelineno-31-32 name=__codelineno-31-32 href=#__codelineno-31-32></a>                <span class=n>input_ids</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>mask_token_id</span>
</span><span id=__span-31-33><a id=__codelineno-31-33 name=__codelineno-31-33 href=#__codelineno-31-33></a>        <span class=n>feature</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>new_labels</span>
</span><span id=__span-31-34><a id=__codelineno-31-34 name=__codelineno-31-34 href=#__codelineno-31-34></a>
</span><span id=__span-31-35><a id=__codelineno-31-35 name=__codelineno-31-35 href=#__codelineno-31-35></a>    <span class=k>return</span> <span class=n>default_data_collator</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></code></pre></div> <p>次に、前と同じサンプルで試してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=c1># 全単語マスキングの動作確認</span>
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a><span class=n>samples</span> <span class=o>=</span> <span class=p>[</span><span class=n>lm_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>)]</span>
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a><span class=n>batch</span> <span class=o>=</span> <span class=n>whole_word_masking_data_collator</span><span class=p>(</span><span class=n>samples</span><span class=p>)</span>
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a>
</span><span id=__span-32-5><a id=__codelineno-32-5 name=__codelineno-32-5 href=#__codelineno-32-5></a><span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]:</span>
</span><span id=__span-32-6><a id=__codelineno-32-6 name=__codelineno-32-6 href=#__codelineno-32-6></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&#39;&gt;&gt;&gt; </span><span class=si>{</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>chunk</span><span class=p>)</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a>&#39;&gt;&gt;&gt; [CLS] i [MASK] i am curious - yellow from [MASK] video store because [MASK] all the controversy that surrounded it [MASK] [MASK] [MASK] first [MASK] in 1967 [MASK] [MASK] [MASK] heard that at first [MASK] [MASK] seized by u [MASK] s. [MASK] if it ever [MASK] [MASK] enter this country, therefore being a fan [MASK] films considered &quot; controversial [MASK] i really [MASK] to see this for [MASK]. [MASK] br / &gt; &lt; [MASK] / &gt; [MASK] plot is [MASK] around a young swedish drama student named lena [MASK] wants to [MASK] everything [MASK] can [MASK] life. in particular she wants to focus her attentions to making some sort of documentary on what the average [MASK] [MASK] thought about certain political issues [MASK]&#39;
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a>
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a>&#39;&gt;&gt;&gt; as [MASK] vietnam [MASK] and race issues in the united states. in between asking [MASK] and ordinary denizens of [MASK] about [MASK] opinions on politics, she has sex with her drama teacher, classmates, and [MASK] men. &lt; [MASK] / &gt; [MASK] br / &gt; [MASK] kills me [MASK] [MASK] am curious - [MASK] [MASK] that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and [MASK] between, even then it [MASK] [MASK] not shot like some cheaply made [MASK] [MASK]. while my countrymen [MASK] find it shocking, [MASK] [MASK] sex and nudity are a major staple in [MASK] cinema. even ingmar [MASK],&#39;
</span></code></pre></div></p> <h3 id=_13>データセットのダウンサンプリング<a class=headerlink href=#_13 title="Permanent link">&para;</a></h3> <p>2つのデータコレーターができたので、残りのファインチューニングステップは標準的です。心配しないでください、それでもかなり良い言語モデルが得られます！</p> <p>Hugging Face Datasetsでデータセットをダウンサンプリングする簡単な方法は、<code>Dataset.train_test_split()</code>関数を使用することです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=c1># データセットサイズを設定</span>
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2 href=#__codelineno-34-2></a><span class=n>train_size</span> <span class=o>=</span> <span class=mi>10_000</span>
</span><span id=__span-34-3><a id=__codelineno-34-3 name=__codelineno-34-3 href=#__codelineno-34-3></a><span class=n>test_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.1</span> <span class=o>*</span> <span class=n>train_size</span><span class=p>)</span>
</span><span id=__span-34-4><a id=__codelineno-34-4 name=__codelineno-34-4 href=#__codelineno-34-4></a>
</span><span id=__span-34-5><a id=__codelineno-34-5 name=__codelineno-34-5 href=#__codelineno-34-5></a><span class=c1># ダウンサンプリングを実行</span>
</span><span id=__span-34-6><a id=__codelineno-34-6 name=__codelineno-34-6 href=#__codelineno-34-6></a><span class=n>downsampled_dataset</span> <span class=o>=</span> <span class=n>lm_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>train_test_split</span><span class=p>(</span>
</span><span id=__span-34-7><a id=__codelineno-34-7 name=__codelineno-34-7 href=#__codelineno-34-7></a>    <span class=n>train_size</span><span class=o>=</span><span class=n>train_size</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=n>test_size</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span>
</span><span id=__span-34-8><a id=__codelineno-34-8 name=__codelineno-34-8 href=#__codelineno-34-8></a><span class=p>)</span>
</span><span id=__span-34-9><a id=__codelineno-34-9 name=__codelineno-34-9 href=#__codelineno-34-9></a><span class=n>downsampled_dataset</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a>DatasetDict({
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a>    train: Dataset({
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;, &#39;labels&#39;],
</span><span id=__span-35-4><a id=__codelineno-35-4 name=__codelineno-35-4 href=#__codelineno-35-4></a>        num_rows: 10000
</span><span id=__span-35-5><a id=__codelineno-35-5 name=__codelineno-35-5 href=#__codelineno-35-5></a>    })
</span><span id=__span-35-6><a id=__codelineno-35-6 name=__codelineno-35-6 href=#__codelineno-35-6></a>    test: Dataset({
</span><span id=__span-35-7><a id=__codelineno-35-7 name=__codelineno-35-7 href=#__codelineno-35-7></a>        features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;word_ids&#39;, &#39;labels&#39;],
</span><span id=__span-35-8><a id=__codelineno-35-8 name=__codelineno-35-8 href=#__codelineno-35-8></a>        num_rows: 1000
</span><span id=__span-35-9><a id=__codelineno-35-9 name=__codelineno-35-9 href=#__codelineno-35-9></a>    })
</span><span id=__span-35-10><a id=__codelineno-35-10 name=__codelineno-35-10 href=#__codelineno-35-10></a>})
</span></code></pre></div></p> <p>これにより、訓練セットのサイズが10,000例、検証セットが1,000例に設定された新しい<code>train</code>と<code>test</code>分割が自動的に作成されました。</p> <h3 id=_14>訓練引数の設定<a class=headerlink href=#_14 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>TrainingArguments</span>
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a>
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span><span id=__span-36-4><a id=__codelineno-36-4 name=__codelineno-36-4 href=#__codelineno-36-4></a><span class=c1># エポックごとに訓練損失を表示</span>
</span><span id=__span-36-5><a id=__codelineno-36-5 name=__codelineno-36-5 href=#__codelineno-36-5></a><span class=n>logging_steps</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>])</span> <span class=o>//</span> <span class=n>batch_size</span>
</span><span id=__span-36-6><a id=__codelineno-36-6 name=__codelineno-36-6 href=#__codelineno-36-6></a><span class=n>model_name</span> <span class=o>=</span> <span class=n>model_checkpoint</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&quot;/&quot;</span><span class=p>)[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-36-7><a id=__codelineno-36-7 name=__codelineno-36-7 href=#__codelineno-36-7></a>
</span><span id=__span-36-8><a id=__codelineno-36-8 name=__codelineno-36-8 href=#__codelineno-36-8></a><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span><span id=__span-36-9><a id=__codelineno-36-9 name=__codelineno-36-9 href=#__codelineno-36-9></a>    <span class=n>output_dir</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>-finetuned-imdb&quot;</span><span class=p>,</span>
</span><span id=__span-36-10><a id=__codelineno-36-10 name=__codelineno-36-10 href=#__codelineno-36-10></a>    <span class=n>overwrite_output_dir</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-36-11><a id=__codelineno-36-11 name=__codelineno-36-11 href=#__codelineno-36-11></a>    <span class=n>eval_strategy</span><span class=o>=</span><span class=s2>&quot;epoch&quot;</span><span class=p>,</span>
</span><span id=__span-36-12><a id=__codelineno-36-12 name=__codelineno-36-12 href=#__codelineno-36-12></a>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-5</span><span class=p>,</span>
</span><span id=__span-36-13><a id=__codelineno-36-13 name=__codelineno-36-13 href=#__codelineno-36-13></a>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
</span><span id=__span-36-14><a id=__codelineno-36-14 name=__codelineno-36-14 href=#__codelineno-36-14></a>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
</span><span id=__span-36-15><a id=__codelineno-36-15 name=__codelineno-36-15 href=#__codelineno-36-15></a>    <span class=n>per_device_eval_batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
</span><span id=__span-36-16><a id=__codelineno-36-16 name=__codelineno-36-16 href=#__codelineno-36-16></a>    <span class=n>push_to_hub</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span><span id=__span-36-17><a id=__codelineno-36-17 name=__codelineno-36-17 href=#__codelineno-36-17></a>    <span class=n>logging_steps</span><span class=o>=</span><span class=n>logging_steps</span><span class=p>,</span>
</span><span id=__span-36-18><a id=__codelineno-36-18 name=__codelineno-36-18 href=#__codelineno-36-18></a><span class=p>)</span>
</span></code></pre></div> <p>ここでは、エポックごとに訓練損失を追跡するために<code>logging_steps</code>を含め、いくつかのデフォルトオプションを調整しました。</p> <h3 id=trainer>Trainerの初期化と実行<a class=headerlink href=#trainer title="Permanent link">&para;</a></h3> <p>必要な材料がすべて揃ったので、<code>Trainer</code>をインスタンス化できます。ここでは標準の<code>data_collator</code>を使用していますが、演習として全単語マスキングコレーターを試して結果を比較することもできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>Trainer</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a>
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span><span id=__span-37-4><a id=__codelineno-37-4 name=__codelineno-37-4 href=#__codelineno-37-4></a>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span><span id=__span-37-5><a id=__codelineno-37-5 name=__codelineno-37-5 href=#__codelineno-37-5></a>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span><span id=__span-37-6><a id=__codelineno-37-6 name=__codelineno-37-6 href=#__codelineno-37-6></a>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span>
</span><span id=__span-37-7><a id=__codelineno-37-7 name=__codelineno-37-7 href=#__codelineno-37-7></a>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;test&quot;</span><span class=p>],</span>
</span><span id=__span-37-8><a id=__codelineno-37-8 name=__codelineno-37-8 href=#__codelineno-37-8></a>    <span class=n>data_collator</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span><span id=__span-37-9><a id=__codelineno-37-9 name=__codelineno-37-9 href=#__codelineno-37-9></a>    <span class=n>processing_class</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-37-10><a id=__codelineno-37-10 name=__codelineno-37-10 href=#__codelineno-37-10></a><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
</span><span id=__span-38-2><a id=__codelineno-38-2 name=__codelineno-38-2 href=#__codelineno-38-2></a>To disable this warning, you can either:
</span><span id=__span-38-3><a id=__codelineno-38-3 name=__codelineno-38-3 href=#__codelineno-38-3></a>    - Avoid using `tokenizers` before the fork if possible
</span><span id=__span-38-4><a id=__codelineno-38-4 name=__codelineno-38-4 href=#__codelineno-38-4></a>    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</span><span id=__span-38-5><a id=__codelineno-38-5 name=__codelineno-38-5 href=#__codelineno-38-5></a>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
</span><span id=__span-38-6><a id=__codelineno-38-6 name=__codelineno-38-6 href=#__codelineno-38-6></a>To disable this warning, you can either:
</span><span id=__span-38-7><a id=__codelineno-38-7 name=__codelineno-38-7 href=#__codelineno-38-7></a>    - Avoid using `tokenizers` before the fork if possible
</span><span id=__span-38-8><a id=__codelineno-38-8 name=__codelineno-38-8 href=#__codelineno-38-8></a>    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</span></code></pre></div></p> <h2 id=perplexity>言語モデルの困惑度（Perplexity）評価<a class=headerlink href=#perplexity title="Permanent link">&para;</a></h2> <p>テキスト分類や質問応答のような他のタスクとは異なり、言語モデリングではラベル付きコーパスが与えられません。では、良い言語モデルとは何を決定するのでしょうか？</p> <p>スマートフォンの自動修正機能のように、良い言語モデルは文法的に正しい文に高い確率を割り当て、意味のない文に低い確率を割り当てるものです。これをより良く理解するために、オンラインで「自動修正の失敗」のセット全体を見つけることができ、人の電話のモデルがかなり面白い（そしてしばしば不適切な）補完を生成した例が示されています。</p> <p>テストセットが主に文法的に正しい文で構成されていると仮定すると、言語モデルの品質を測定する一つの方法は、テストセット内のすべての文の次の単語に割り当てる確率を計算することです。</p> <p>高い確率は、モデルが未見の例に対して「驚いていない」または「困惑していない」ことを示し、言語の基本的な文法パターンを学習していることを示唆します。</p> <p>困惑度には様々な数学的定義がありますが、ここで使用するものは<strong>クロスエントロピー損失の指数</strong>として定義されます。したがって、<code>Trainer.evaluate()</code>関数を使用してテストセットでクロスエントロピー損失を計算し、その結果の指数を取ることで、事前学習済みモデルの困惑度を計算できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a><span class=kn>import</span><span class=w> </span><span class=nn>math</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a>
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a><span class=c1># 事前学習済みモデルの困惑度を評価</span>
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4 href=#__codelineno-39-4></a><span class=c1># eval_results = trainer.evaluate()</span>
</span><span id=__span-39-5><a id=__codelineno-39-5 name=__codelineno-39-5 href=#__codelineno-39-5></a><span class=c1># print(f&quot;&gt;&gt;&gt; 困惑度: {math.exp(eval_results[&#39;eval_loss&#39;]):.2f}&quot;)</span>
</span><span id=__span-39-6><a id=__codelineno-39-6 name=__codelineno-39-6 href=#__codelineno-39-6></a><span class=c1># &gt;&gt;&gt; 困惑度: 21.75</span>
</span></code></pre></div> <p>困惑度スコアが低いほど、言語モデルが優れていることを意味します。ここでは、開始モデルがやや大きな値を持っていることが分かります。ファインチューニングによってこれを下げることができるか見てみましょう。</p> <p>まず、訓練ループを実行します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a><span class=c1># 訓練の実行</span>
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a><span class=c1># trainer.train()</span>
</span></code></pre></div> <p>その後、前と同様にテストセットで結果の困惑度を計算します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a><span class=c1># ファインチューニング後の困惑度を評価</span>
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a><span class=c1># eval_results = trainer.evaluate()</span>
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a><span class=c1># print(f&quot;&gt;&gt;&gt; 困惑度: {math.exp(eval_results[&#39;eval_loss&#39;]):.2f}&quot;)</span>
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a><span class=c1># &gt;&gt;&gt; 困惑度: 11.32</span>
</span></code></pre></div> <p>素晴らしいです。これは困惑度の大幅な削減で、モデルが映画レビューのドメインについて何かを学習したことを示しています。</p> <h2 id=hugging-face-acceleratedistilbert>Hugging Face Accelerateを使ったDistilBERTのファインチューニング<a class=headerlink href=#hugging-face-acceleratedistilbert title="Permanent link">&para;</a></h2> <p><code>Trainer</code>で見たように、マスク言語モデルのファインチューニングは、第3章のテキスト分類例と非常に似ています。実際、唯一の微妙な点は特別なデータコレーターの使用であり、このセクションの前半で既にそれをカバーしました！</p> <p>しかし、<code>DataCollatorForLanguageModeling</code>は各評価でもランダムマスキングを適用するため、各訓練実行で困惑度スコアにいくらかの変動が見られました。</p> <p>このランダム性の源を排除する一つの方法は、テストセット全体に<strong>一度</strong>マスキングを適用し、評価中にHugging Face Transformersのデフォルトデータコレーターを使用することです。</p> <h3 id=_15>固定マスキング関数の実装<a class=headerlink href=#_15 title="Permanent link">&para;</a></h3> <p>これがどのように機能するかを確認するために、<code>DataCollatorForLanguageModeling</code>との最初の遭遇に似た、バッチにマスキングを適用する簡単な関数を実装しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=k>def</span><span class=w> </span><span class=nf>insert_random_mask</span><span class=p>(</span><span class=n>batch</span><span class=p>):</span>
</span><span id=__span-42-2><a id=__codelineno-42-2 name=__codelineno-42-2 href=#__codelineno-42-2></a>    <span class=n>features</span> <span class=o>=</span> <span class=p>[</span><span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>t</span><span class=p>))</span> <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>batch</span><span class=o>.</span><span class=n>values</span><span class=p>())]</span>
</span><span id=__span-42-3><a id=__codelineno-42-3 name=__codelineno-42-3 href=#__codelineno-42-3></a>    <span class=n>masked_inputs</span> <span class=o>=</span> <span class=n>data_collator</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span><span id=__span-42-4><a id=__codelineno-42-4 name=__codelineno-42-4 href=#__codelineno-42-4></a>    <span class=c1># データセットの各カラムに新しい「masked」カラムを作成</span>
</span><span id=__span-42-5><a id=__codelineno-42-5 name=__codelineno-42-5 href=#__codelineno-42-5></a>    <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;masked_&quot;</span> <span class=o>+</span> <span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>masked_inputs</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></code></pre></div> <p>次に、この関数をテストセットに適用し、マスクされていないカラムを削除してマスクされたものに置き換えることができます。全単語マスキングを使用する場合は、上記の<code>data_collator</code>を適切なものに置き換え、ここで最初の行を削除する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a><span class=c1># データセットからword_idsカラムを削除</span>
</span><span id=__span-43-2><a id=__codelineno-43-2 name=__codelineno-43-2 href=#__codelineno-43-2></a><span class=n>downsampled_dataset</span> <span class=o>=</span> <span class=n>downsampled_dataset</span><span class=o>.</span><span class=n>remove_columns</span><span class=p>([</span><span class=s2>&quot;word_ids&quot;</span><span class=p>])</span>
</span><span id=__span-43-3><a id=__codelineno-43-3 name=__codelineno-43-3 href=#__codelineno-43-3></a>
</span><span id=__span-43-4><a id=__codelineno-43-4 name=__codelineno-43-4 href=#__codelineno-43-4></a><span class=c1># テストセットに固定マスキングを適用</span>
</span><span id=__span-43-5><a id=__codelineno-43-5 name=__codelineno-43-5 href=#__codelineno-43-5></a><span class=n>eval_dataset</span> <span class=o>=</span> <span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;test&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span>
</span><span id=__span-43-6><a id=__codelineno-43-6 name=__codelineno-43-6 href=#__codelineno-43-6></a>    <span class=n>insert_random_mask</span><span class=p>,</span>
</span><span id=__span-43-7><a id=__codelineno-43-7 name=__codelineno-43-7 href=#__codelineno-43-7></a>    <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-43-8><a id=__codelineno-43-8 name=__codelineno-43-8 href=#__codelineno-43-8></a>    <span class=n>remove_columns</span><span class=o>=</span><span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;test&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>column_names</span><span class=p>,</span>
</span><span id=__span-43-9><a id=__codelineno-43-9 name=__codelineno-43-9 href=#__codelineno-43-9></a><span class=p>)</span>
</span><span id=__span-43-10><a id=__codelineno-43-10 name=__codelineno-43-10 href=#__codelineno-43-10></a>
</span><span id=__span-43-11><a id=__codelineno-43-11 name=__codelineno-43-11 href=#__codelineno-43-11></a><span class=c1># カラム名を変更してマスクされたバージョンを標準名に</span>
</span><span id=__span-43-12><a id=__codelineno-43-12 name=__codelineno-43-12 href=#__codelineno-43-12></a><span class=n>eval_dataset</span> <span class=o>=</span> <span class=n>eval_dataset</span><span class=o>.</span><span class=n>rename_columns</span><span class=p>(</span>
</span><span id=__span-43-13><a id=__codelineno-43-13 name=__codelineno-43-13 href=#__codelineno-43-13></a>    <span class=p>{</span>
</span><span id=__span-43-14><a id=__codelineno-43-14 name=__codelineno-43-14 href=#__codelineno-43-14></a>        <span class=s2>&quot;masked_input_ids&quot;</span><span class=p>:</span> <span class=s2>&quot;input_ids&quot;</span><span class=p>,</span>
</span><span id=__span-43-15><a id=__codelineno-43-15 name=__codelineno-43-15 href=#__codelineno-43-15></a>        <span class=s2>&quot;masked_attention_mask&quot;</span><span class=p>:</span> <span class=s2>&quot;attention_mask&quot;</span><span class=p>,</span>
</span><span id=__span-43-16><a id=__codelineno-43-16 name=__codelineno-43-16 href=#__codelineno-43-16></a>        <span class=s2>&quot;masked_labels&quot;</span><span class=p>:</span> <span class=s2>&quot;labels&quot;</span><span class=p>,</span>
</span><span id=__span-43-17><a id=__codelineno-43-17 name=__codelineno-43-17 href=#__codelineno-43-17></a>    <span class=p>}</span>
</span><span id=__span-43-18><a id=__codelineno-43-18 name=__codelineno-43-18 href=#__codelineno-43-18></a><span class=p>)</span>
</span></code></pre></div> <h3 id=_16>データローダーの設定<a class=headerlink href=#_16 title="Permanent link">&para;</a></h3> <p>通常通りデータローダーを設定できますが、評価セットには Hugging Face Transformersの<code>default_data_collator</code>を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>DataLoader</span>
</span><span id=__span-44-2><a id=__codelineno-44-2 name=__codelineno-44-2 href=#__codelineno-44-2></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>default_data_collator</span>
</span><span id=__span-44-3><a id=__codelineno-44-3 name=__codelineno-44-3 href=#__codelineno-44-3></a>
</span><span id=__span-44-4><a id=__codelineno-44-4 name=__codelineno-44-4 href=#__codelineno-44-4></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span><span id=__span-44-5><a id=__codelineno-44-5 name=__codelineno-44-5 href=#__codelineno-44-5></a>
</span><span id=__span-44-6><a id=__codelineno-44-6 name=__codelineno-44-6 href=#__codelineno-44-6></a><span class=c1># 訓練用データローダー（ランダムマスキング付き）</span>
</span><span id=__span-44-7><a id=__codelineno-44-7 name=__codelineno-44-7 href=#__codelineno-44-7></a><span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-44-8><a id=__codelineno-44-8 name=__codelineno-44-8 href=#__codelineno-44-8></a>    <span class=n>downsampled_dataset</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>],</span>
</span><span id=__span-44-9><a id=__codelineno-44-9 name=__codelineno-44-9 href=#__codelineno-44-9></a>    <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-44-10><a id=__codelineno-44-10 name=__codelineno-44-10 href=#__codelineno-44-10></a>    <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
</span><span id=__span-44-11><a id=__codelineno-44-11 name=__codelineno-44-11 href=#__codelineno-44-11></a>    <span class=n>collate_fn</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span><span id=__span-44-12><a id=__codelineno-44-12 name=__codelineno-44-12 href=#__codelineno-44-12></a><span class=p>)</span>
</span><span id=__span-44-13><a id=__codelineno-44-13 name=__codelineno-44-13 href=#__codelineno-44-13></a>
</span><span id=__span-44-14><a id=__codelineno-44-14 name=__codelineno-44-14 href=#__codelineno-44-14></a><span class=c1># 評価用データローダー（固定マスキング）</span>
</span><span id=__span-44-15><a id=__codelineno-44-15 name=__codelineno-44-15 href=#__codelineno-44-15></a><span class=n>eval_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-44-16><a id=__codelineno-44-16 name=__codelineno-44-16 href=#__codelineno-44-16></a>    <span class=n>eval_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>collate_fn</span><span class=o>=</span><span class=n>default_data_collator</span>
</span><span id=__span-44-17><a id=__codelineno-44-17 name=__codelineno-44-17 href=#__codelineno-44-17></a><span class=p>)</span>
</span></code></pre></div> <h3 id=_17>モデルとオプティマイザーの準備<a class=headerlink href=#_17 title="Permanent link">&para;</a></h3> <p>ここから、Hugging Face Accelerateを使った標準的なステップに従います。まず、事前学習済みモデルの新しいバージョンを読み込みます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a><span class=c1># 新しいモデルインスタンスを読み込み</span>
</span><span id=__span-45-2><a id=__codelineno-45-2 name=__codelineno-45-2 href=#__codelineno-45-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForMaskedLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_checkpoint</span><span class=p>)</span>
</span></code></pre></div> <p>次にオプティマイザーを指定します。標準的な<code>AdamW</code>を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=kn>import</span> <span class=n>AdamW</span>
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a>
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a><span class=c1># オプティマイザーを設定</span>
</span><span id=__span-46-4><a id=__codelineno-46-4 name=__codelineno-46-4 href=#__codelineno-46-4></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>)</span>
</span></code></pre></div> <p>これらのオブジェクトを使って、<code>Accelerator</code>オブジェクトで訓練のためのすべてを準備できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a><span class=kn>from</span><span class=w> </span><span class=nn>accelerate</span><span class=w> </span><span class=kn>import</span> <span class=n>Accelerator</span>
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a>
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a><span class=c1># Acceleratorを初期化し、すべてのコンポーネントを準備</span>
</span><span id=__span-47-4><a id=__codelineno-47-4 name=__codelineno-47-4 href=#__codelineno-47-4></a><span class=n>accelerator</span> <span class=o>=</span> <span class=n>Accelerator</span><span class=p>()</span>
</span><span id=__span-47-5><a id=__codelineno-47-5 name=__codelineno-47-5 href=#__codelineno-47-5></a><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>eval_dataloader</span> <span class=o>=</span> <span class=n>accelerator</span><span class=o>.</span><span class=n>prepare</span><span class=p>(</span>
</span><span id=__span-47-6><a id=__codelineno-47-6 name=__codelineno-47-6 href=#__codelineno-47-6></a>    <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>eval_dataloader</span>
</span><span id=__span-47-7><a id=__codelineno-47-7 name=__codelineno-47-7 href=#__codelineno-47-7></a><span class=p>)</span>
</span></code></pre></div> <h3 id=_18>学習率スケジューラーの設定<a class=headerlink href=#_18 title="Permanent link">&para;</a></h3> <p>モデル、オプティマイザー、データローダーが設定されたので、学習率スケジューラーを以下のように指定できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>get_scheduler</span>
</span><span id=__span-48-2><a id=__codelineno-48-2 name=__codelineno-48-2 href=#__codelineno-48-2></a>
</span><span id=__span-48-3><a id=__codelineno-48-3 name=__codelineno-48-3 href=#__codelineno-48-3></a><span class=c1># 訓練パラメータの設定</span>
</span><span id=__span-48-4><a id=__codelineno-48-4 name=__codelineno-48-4 href=#__codelineno-48-4></a><span class=n>num_train_epochs</span> <span class=o>=</span> <span class=mi>3</span>
</span><span id=__span-48-5><a id=__codelineno-48-5 name=__codelineno-48-5 href=#__codelineno-48-5></a><span class=n>num_update_steps_per_epoch</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>)</span>
</span><span id=__span-48-6><a id=__codelineno-48-6 name=__codelineno-48-6 href=#__codelineno-48-6></a><span class=n>num_training_steps</span> <span class=o>=</span> <span class=n>num_train_epochs</span> <span class=o>*</span> <span class=n>num_update_steps_per_epoch</span>
</span><span id=__span-48-7><a id=__codelineno-48-7 name=__codelineno-48-7 href=#__codelineno-48-7></a>
</span><span id=__span-48-8><a id=__codelineno-48-8 name=__codelineno-48-8 href=#__codelineno-48-8></a><span class=c1># 線形学習率スケジューラーを設定</span>
</span><span id=__span-48-9><a id=__codelineno-48-9 name=__codelineno-48-9 href=#__codelineno-48-9></a><span class=n>lr_scheduler</span> <span class=o>=</span> <span class=n>get_scheduler</span><span class=p>(</span>
</span><span id=__span-48-10><a id=__codelineno-48-10 name=__codelineno-48-10 href=#__codelineno-48-10></a>    <span class=s2>&quot;linear&quot;</span><span class=p>,</span>
</span><span id=__span-48-11><a id=__codelineno-48-11 name=__codelineno-48-11 href=#__codelineno-48-11></a>    <span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-48-12><a id=__codelineno-48-12 name=__codelineno-48-12 href=#__codelineno-48-12></a>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span><span id=__span-48-13><a id=__codelineno-48-13 name=__codelineno-48-13 href=#__codelineno-48-13></a>    <span class=n>num_training_steps</span><span class=o>=</span><span class=n>num_training_steps</span><span class=p>,</span>
</span><span id=__span-48-14><a id=__codelineno-48-14 name=__codelineno-48-14 href=#__codelineno-48-14></a><span class=p>)</span>
</span></code></pre></div> <h3 id=_19>完全な訓練・評価ループ<a class=headerlink href=#_19 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a><span class=kn>from</span><span class=w> </span><span class=nn>tqdm.auto</span><span class=w> </span><span class=kn>import</span> <span class=n>tqdm</span>
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a><span class=kn>import</span><span class=w> </span><span class=nn>math</span>
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a>
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a><span class=c1># プログレスバーと出力ディレクトリを設定</span>
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6 href=#__codelineno-49-6></a><span class=n>progress_bar</span> <span class=o>=</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_training_steps</span><span class=p>))</span>
</span><span id=__span-49-7><a id=__codelineno-49-7 name=__codelineno-49-7 href=#__codelineno-49-7></a><span class=n>output_dir</span> <span class=o>=</span> <span class=s2>&quot;distilbert-base-uncased-finetuned-imdb-accelerate&quot;</span>
</span><span id=__span-49-8><a id=__codelineno-49-8 name=__codelineno-49-8 href=#__codelineno-49-8></a>
</span><span id=__span-49-9><a id=__codelineno-49-9 name=__codelineno-49-9 href=#__codelineno-49-9></a><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_train_epochs</span><span class=p>):</span>
</span><span id=__span-49-10><a id=__codelineno-49-10 name=__codelineno-49-10 href=#__codelineno-49-10></a>    <span class=c1># 訓練フェーズ</span>
</span><span id=__span-49-11><a id=__codelineno-49-11 name=__codelineno-49-11 href=#__codelineno-49-11></a>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-49-12><a id=__codelineno-49-12 name=__codelineno-49-12 href=#__codelineno-49-12></a>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_dataloader</span><span class=p>:</span>
</span><span id=__span-49-13><a id=__codelineno-49-13 name=__codelineno-49-13 href=#__codelineno-49-13></a>        <span class=c1># 順伝播</span>
</span><span id=__span-49-14><a id=__codelineno-49-14 name=__codelineno-49-14 href=#__codelineno-49-14></a>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-49-15><a id=__codelineno-49-15 name=__codelineno-49-15 href=#__codelineno-49-15></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span><span id=__span-49-16><a id=__codelineno-49-16 name=__codelineno-49-16 href=#__codelineno-49-16></a>        <span class=c1># 逆伝播</span>
</span><span id=__span-49-17><a id=__codelineno-49-17 name=__codelineno-49-17 href=#__codelineno-49-17></a>        <span class=n>accelerator</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-49-18><a id=__codelineno-49-18 name=__codelineno-49-18 href=#__codelineno-49-18></a>
</span><span id=__span-49-19><a id=__codelineno-49-19 name=__codelineno-49-19 href=#__codelineno-49-19></a>        <span class=c1># パラメータ更新</span>
</span><span id=__span-49-20><a id=__codelineno-49-20 name=__codelineno-49-20 href=#__codelineno-49-20></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-49-21><a id=__codelineno-49-21 name=__codelineno-49-21 href=#__codelineno-49-21></a>        <span class=n>lr_scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-49-22><a id=__codelineno-49-22 name=__codelineno-49-22 href=#__codelineno-49-22></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-49-23><a id=__codelineno-49-23 name=__codelineno-49-23 href=#__codelineno-49-23></a>        <span class=n>progress_bar</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-49-24><a id=__codelineno-49-24 name=__codelineno-49-24 href=#__codelineno-49-24></a>
</span><span id=__span-49-25><a id=__codelineno-49-25 name=__codelineno-49-25 href=#__codelineno-49-25></a>    <span class=c1># 評価フェーズ</span>
</span><span id=__span-49-26><a id=__codelineno-49-26 name=__codelineno-49-26 href=#__codelineno-49-26></a>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span><span id=__span-49-27><a id=__codelineno-49-27 name=__codelineno-49-27 href=#__codelineno-49-27></a>    <span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-49-28><a id=__codelineno-49-28 name=__codelineno-49-28 href=#__codelineno-49-28></a>    <span class=k>for</span> <span class=n>step</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>eval_dataloader</span><span class=p>):</span>
</span><span id=__span-49-29><a id=__codelineno-49-29 name=__codelineno-49-29 href=#__codelineno-49-29></a>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-49-30><a id=__codelineno-49-30 name=__codelineno-49-30 href=#__codelineno-49-30></a>            <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-49-31><a id=__codelineno-49-31 name=__codelineno-49-31 href=#__codelineno-49-31></a>
</span><span id=__span-49-32><a id=__codelineno-49-32 name=__codelineno-49-32 href=#__codelineno-49-32></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span><span id=__span-49-33><a id=__codelineno-49-33 name=__codelineno-49-33 href=#__codelineno-49-33></a>        <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>accelerator</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)))</span>
</span><span id=__span-49-34><a id=__codelineno-49-34 name=__codelineno-49-34 href=#__codelineno-49-34></a>
</span><span id=__span-49-35><a id=__codelineno-49-35 name=__codelineno-49-35 href=#__codelineno-49-35></a>    <span class=c1># 困惑度の計算</span>
</span><span id=__span-49-36><a id=__codelineno-49-36 name=__codelineno-49-36 href=#__codelineno-49-36></a>    <span class=n>losses</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>
</span><span id=__span-49-37><a id=__codelineno-49-37 name=__codelineno-49-37 href=#__codelineno-49-37></a>    <span class=n>losses</span> <span class=o>=</span> <span class=n>losses</span><span class=p>[:</span> <span class=nb>len</span><span class=p>(</span><span class=n>eval_dataset</span><span class=p>)]</span>
</span><span id=__span-49-38><a id=__codelineno-49-38 name=__codelineno-49-38 href=#__codelineno-49-38></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-49-39><a id=__codelineno-49-39 name=__codelineno-49-39 href=#__codelineno-49-39></a>        <span class=n>perplexity</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>losses</span><span class=p>))</span>
</span><span id=__span-49-40><a id=__codelineno-49-40 name=__codelineno-49-40 href=#__codelineno-49-40></a>    <span class=k>except</span> <span class=ne>OverflowError</span><span class=p>:</span>
</span><span id=__span-49-41><a id=__codelineno-49-41 name=__codelineno-49-41 href=#__codelineno-49-41></a>        <span class=n>perplexity</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&quot;inf&quot;</span><span class=p>)</span>
</span><span id=__span-49-42><a id=__codelineno-49-42 name=__codelineno-49-42 href=#__codelineno-49-42></a>
</span><span id=__span-49-43><a id=__codelineno-49-43 name=__codelineno-49-43 href=#__codelineno-49-43></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&gt;&gt;&gt; エポック </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>: 困惑度: </span><span class=si>{</span><span class=n>perplexity</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-49-44><a id=__codelineno-49-44 name=__codelineno-49-44 href=#__codelineno-49-44></a>
</span><span id=__span-49-45><a id=__codelineno-49-45 name=__codelineno-49-45 href=#__codelineno-49-45></a>    <span class=c1># モデルの保存とアップロード</span>
</span><span id=__span-49-46><a id=__codelineno-49-46 name=__codelineno-49-46 href=#__codelineno-49-46></a>    <span class=n>accelerator</span><span class=o>.</span><span class=n>wait_for_everyone</span><span class=p>()</span>
</span><span id=__span-49-47><a id=__codelineno-49-47 name=__codelineno-49-47 href=#__codelineno-49-47></a>    <span class=n>unwrapped_model</span> <span class=o>=</span> <span class=n>accelerator</span><span class=o>.</span><span class=n>unwrap_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-49-48><a id=__codelineno-49-48 name=__codelineno-49-48 href=#__codelineno-49-48></a>    <span class=n>unwrapped_model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>output_dir</span><span class=p>,</span> <span class=n>save_function</span><span class=o>=</span><span class=n>accelerator</span><span class=o>.</span><span class=n>save</span><span class=p>)</span>
</span><span id=__span-49-49><a id=__codelineno-49-49 name=__codelineno-49-49 href=#__codelineno-49-49></a>    <span class=k>if</span> <span class=n>accelerator</span><span class=o>.</span><span class=n>is_main_process</span><span class=p>:</span>
</span><span id=__span-49-50><a id=__codelineno-49-50 name=__codelineno-49-50 href=#__codelineno-49-50></a>        <span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>output_dir</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a>&gt;&gt;&gt; エポック 0: 困惑度: 11.560117477655108
</span><span id=__span-50-2><a id=__codelineno-50-2 name=__codelineno-50-2 href=#__codelineno-50-2></a>&gt;&gt;&gt; エポック 1: 困惑度: 11.106818026989123
</span><span id=__span-50-3><a id=__codelineno-50-3 name=__codelineno-50-3 href=#__codelineno-50-3></a>&gt;&gt;&gt; エポック 2: 困惑度: 10.901680692261271
</span></code></pre></div></p> <h2 id=_20>ファインチューニング済みモデルの使用<a class=headerlink href=#_20 title="Permanent link">&para;</a></h2> <p>ファインチューニング済みモデルは、Hubのウィジェットを使用するか、Hugging Face Transformersの<code>pipeline</code>を使用してローカルで操作できます。後者を使用して、<code>fill-mask</code>パイプラインでモデルをダウンロードしましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a>
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a><span class=c1># ファインチューニング済みモデルでパイプラインを作成</span>
</span><span id=__span-51-4><a id=__codelineno-51-4 name=__codelineno-51-4 href=#__codelineno-51-4></a><span class=n>mask_filler</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span>
</span><span id=__span-51-5><a id=__codelineno-51-5 name=__codelineno-51-5 href=#__codelineno-51-5></a>    <span class=s2>&quot;fill-mask&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&quot;distilbert-base-uncased-finetuned-imdb-accelerate&quot;</span>
</span><span id=__span-51-6><a id=__codelineno-51-6 name=__codelineno-51-6 href=#__codelineno-51-6></a><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a>Device set to use mps:0
</span></code></pre></div></p> <p>「This is a great [MASK]」というサンプルテキストをパイプラインに供給し、上位5つの予測を確認してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a><span class=c1># マスク予測を実行</span>
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a><span class=n>preds</span> <span class=o>=</span> <span class=n>mask_filler</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a>
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4 href=#__codelineno-53-4></a><span class=k>for</span> <span class=n>pred</span> <span class=ow>in</span> <span class=n>preds</span><span class=p>:</span>
</span><span id=__span-53-5><a id=__codelineno-53-5 name=__codelineno-53-5 href=#__codelineno-53-5></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&gt;&gt;&gt; </span><span class=si>{</span><span class=n>pred</span><span class=p>[</span><span class=s1>&#39;sequence&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a>&gt;&gt;&gt; this is a great film.
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a>&gt;&gt;&gt; this is a great movie.
</span><span id=__span-54-3><a id=__codelineno-54-3 name=__codelineno-54-3 href=#__codelineno-54-3></a>&gt;&gt;&gt; this is a great idea.
</span><span id=__span-54-4><a id=__codelineno-54-4 name=__codelineno-54-4 href=#__codelineno-54-4></a>&gt;&gt;&gt; this is a great one.
</span><span id=__span-54-5><a id=__codelineno-54-5 name=__codelineno-54-5 href=#__codelineno-54-5></a>&gt;&gt;&gt; this is a great show.
</span></code></pre></div></p> <p>素晴らしいです。ファインチューニング前は「deal」、「success」、「adventure」などの一般的な用語が予測されていましたが、ファインチューニング後は「film」、「movie」、「show」といった映画レビューのドメインに特化した用語が予測されるようになりました。これは、ドメイン適応が成功したことを明確に示しています。</p> <h2 id=_21>参考資料<a class=headerlink href=#_21 title="Permanent link">&para;</a></h2> <ul> <li><a href=https://huggingface.co/docs/transformers/ >Hugging Face Transformers Documentation</a></li> <li><a href=https://arxiv.org/abs/1910.01108>DistilBERT: a distilled version of BERT</a></li> <li><a href=https://arxiv.org/abs/1801.06146>Universal Language Model Fine-tuning for Text Classification (ULMFiT)</a></li> <li><a href=https://huggingface.co/datasets/imdb>IMDb Dataset</a></li> <li><a href=https://www.oreilly.com/library/view/natural-language-processing/9781098136789/ >Natural Language Processing with Transformers</a></li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../../javascripts/mathjax.js></script> <script src=../../../javascripts/meta.js></script> <script src=../../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>