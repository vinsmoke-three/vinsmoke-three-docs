<!doctype html><html lang=ja class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Hugging Face Tokenizersライブラリを使った新しいトークナイザーの訓練方法から、BPE、WordPiece、Unigramアルゴリズムの詳細まで、実践的なコード例とともに徹底解説"><meta name=author content=vinsmoke-three><link href=https://vinsmoke-three.com/LLM/04_the_huggingface_tokenizers_library/ rel=canonical><link href=../03_fine_tuning_a_pretrained_model/ rel=prev><link href=../05_Let%27s_build_GPT_from_scratch/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.20"><title>Hugging Face Tokenizersライブラリの完全ガイド - 高速トークナイザーの仕組みと実装 - vinsmoke-three - 機械学習・深層学習ドキュメント</title><link rel=stylesheet href=../../assets/stylesheets/main.e53b48f4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-BXKYE0NT9N"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-BXKYE0NT9N",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-BXKYE0NT9N",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#huggingface-tokenizers class=md-skip> コンテンツにスキップ </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=ヘッダー> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-header__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> vinsmoke-three - 機械学習・深層学習ドキュメント </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Hugging Face Tokenizersライブラリの完全ガイド - 高速トークナイザーの仕組みと実装 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=検索 placeholder=検索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=検索> <a href=javascript:void(0) class="md-search__icon md-icon" title=共有 aria-label=共有 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=クリア aria-label=クリア tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 検索を初期化 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=タブ data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PyTorch/00_setup/ class=md-tabs__link> PyTorch </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../00_illustrated_transformer/ class=md-tabs__link> LLM </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=ナビゲーション data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="vinsmoke-three - 機械学習・深層学習ドキュメント" class="md-nav__button md-logo" aria-label="vinsmoke-three - 機械学習・深層学習ドキュメント" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 14.27 10.64 13A11.24 11.24 0 0 0 5 10.18v6.95c2.61.34 5 1.34 7 2.82 2-1.48 4.39-2.48 7-2.82v-6.95c-2.16.39-4.09 1.39-5.64 2.82M19 8.15c.65-.1 1.32-.15 2-.15v11c-3.5 0-6.64 1.35-9 3.54C9.64 20.35 6.5 19 3 19V8c.68 0 1.35.05 2 .15 2.69.41 5.1 1.63 7 3.39 1.9-1.76 4.31-2.98 7-3.39M12 6c.27 0 .5-.1.71-.29.19-.21.29-.44.29-.71s-.1-.5-.29-.71C12.5 4.11 12.27 4 12 4s-.5.11-.71.29c-.18.21-.29.45-.29.71s.11.5.29.71c.21.19.45.29.71.29m2.12 1.12a2.997 2.997 0 1 1-4.24-4.24 2.997 2.997 0 1 1 4.24 4.24"/></svg> </a> vinsmoke-three - 機械学習・深層学習ドキュメント </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../PyTorch/00_setup/ class=md-nav__link> <span class=md-ellipsis> 0. setup </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/01_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 1. PyTorch fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/02_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 2. PyTorch workflow </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/03_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 3. PyTorch classification </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/04_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 4. PyTorch computer vision </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/05_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 5. PyTorch custom datasets </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/06_pytorch_modular/ class=md-nav__link> <span class=md-ellipsis> 6. PyTorch modular </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/07_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 7. PyTorch transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/08_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 8. PyTorch experiment tracking </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/09_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 9. PyTorch paper replicating </span> </a> </li> <li class=md-nav__item> <a href=../../PyTorch/10_pytorch_model_deployment/ class=md-nav__link> <span class=md-ellipsis> 10. PyTorch model deployment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_illustrated_transformer/ class=md-nav__link> <span class=md-ellipsis> 0. The illustrated transformer </span> </a> </li> <li class=md-nav__item> <a href=../01_transformer_models/ class=md-nav__link> <span class=md-ellipsis> 1. Transformer models </span> </a> </li> <li class=md-nav__item> <a href=../02_using_transformers/ class=md-nav__link> <span class=md-ellipsis> 2. Using transformers </span> </a> </li> <li class=md-nav__item> <a href=../03_fine_tuning_a_pretrained_model/ class=md-nav__link> <span class=md-ellipsis> 3. Fine-tuning a pretrained model </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 4. Tokenizers library </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 4. Tokenizers library </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> はじめに </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 既存のトークナイザーから新しいトークナイザーを訓練する </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 高速トークナイザーの特別な機能 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 正規化と前処理 </span> </a> </li> <li class=md-nav__item> <a href=#byte-pair-encoding class=md-nav__link> <span class=md-ellipsis> Byte-Pair Encodingトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#wordpiece class=md-nav__link> <span class=md-ellipsis> WordPieceトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#unigram class=md-nav__link> <span class=md-ellipsis> Unigramトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#_24 class=md-nav__link> <span class=md-ellipsis> ブロックごとのトークナイザー構築 </span> </a> </li> <li class=md-nav__item> <a href=#_26 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../05_Let%27s_build_GPT_from_scratch/ class=md-nav__link> <span class=md-ellipsis> 5. Let't build GPT from scratch </span> </a> </li> <li class=md-nav__item> <a href=../06_the_huggingface_datasets_library/ class=md-nav__link> <span class=md-ellipsis> 6. Datasets library </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> 7. Classical NLP Tasks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> 7. Classical NLP Tasks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ClassicalNLP/71_token_classification/ class=md-nav__link> <span class=md-ellipsis> Token Classification </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/72_masked_language_modeling/ class=md-nav__link> <span class=md-ellipsis> Masked Language Modeling </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/73_translation/ class=md-nav__link> <span class=md-ellipsis> Translation </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/74_summarization/ class=md-nav__link> <span class=md-ellipsis> Summarization </span> </a> </li> <li class=md-nav__item> <a href=../ClassicalNLP/75_question_answering/ class=md-nav__link> <span class=md-ellipsis> Question Answering </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目次> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目次 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 概要 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 前提知識 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> はじめに </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 既存のトークナイザーから新しいトークナイザーを訓練する </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 高速トークナイザーの特別な機能 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 正規化と前処理 </span> </a> </li> <li class=md-nav__item> <a href=#byte-pair-encoding class=md-nav__link> <span class=md-ellipsis> Byte-Pair Encodingトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#wordpiece class=md-nav__link> <span class=md-ellipsis> WordPieceトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#unigram class=md-nav__link> <span class=md-ellipsis> Unigramトークン化 </span> </a> </li> <li class=md-nav__item> <a href=#_24 class=md-nav__link> <span class=md-ellipsis> ブロックごとのトークナイザー構築 </span> </a> </li> <li class=md-nav__item> <a href=#_26 class=md-nav__link> <span class=md-ellipsis> まとめ </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=huggingface-tokenizers>HuggingFace Tokenizersライブラリの完全ガイド<a class=headerlink href=#huggingface-tokenizers title="Permanent link">&para;</a></h1> <h2 id=_1>概要<a class=headerlink href=#_1 title="Permanent link">&para;</a></h2> <p>この記事では、自然言語処理において重要な役割を果たすトークナイザーについて、特にHugging Face Tokenizersライブラリを中心に詳しく解説します。新しいトークナイザーを一から訓練する方法、高速トークナイザーの特別な機能、そして主要なサブワード・トークン化アルゴリズムの違いについて学習します。</p> <p>実際のコード例を通じて、BPE（Byte-Pair Encoding）、WordPiece、Unigramの3つの主要アルゴリズムの仕組みを理解し、独自のトークナイザーを構築する方法を習得できます。</p> <div class="admonition info"> <p class=admonition-title>参考資料</p> <p>本ドキュメントは <a href=https://huggingface.co/learn/llm-course/chapter6/1>Hugging Face LLM Course</a> を参考に、日本語で学習内容をまとめた個人的な学習ノートです。詳細な内容や最新情報については、原文も併せてご参照ください。</p> </div> <h2 id=_2>前提知識<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <p>この記事を理解するために、以下の知識があることを推奨します：</p> <ul> <li>Pythonプログラミングの基礎知識</li> <li>Transformersモデルの基本的な理解</li> <li>機械学習とディープラーニングの概念</li> <li>pandas、numpy等のデータ処理ライブラリの使用経験</li> </ul> <h2 id=_3>はじめに<a class=headerlink href=#_3 title="Permanent link">&para;</a></h2> <p>第3章では、特定のタスクでモデルをファインチューニングする方法を学びました。その際、モデルが事前学習で使用したのと同じトークナイザーを使用しました。しかし、モデルを一から訓練したい場合はどうでしょうか？このような場合、別のドメインや言語のコーパスで事前学習されたトークナイザーを使用するのは通常最適ではありません。例えば、英語のコーパスで訓練されたトークナイザーは、日本語のテキストのコーパスでは適切に動作しません。これは、両言語でのスペースや句読点の使用方法が大きく異なるためです。</p> <p>この章では、テキストのコーパスで全く新しいトークナイザーを訓練し、それを言語モデルの事前学習に使用する方法を学習します。これはすべて<a href=https://github.com/huggingface/tokenizers>Hugging Face Tokenizers</a>ライブラリの支援を受けて行われます。このライブラリは<a href=https://github.com/huggingface/transformers>Hugging Face Transformers</a>ライブラリの「高速」トークナイザーを提供しています。このライブラリが提供する機能について詳しく見て、高速トークナイザーが「低速」バージョンとどのように異なるかを探ります。</p> <p>扱うトピックは以下の通りです：</p> <ul> <li>新しいテキストコーパスで、特定のチェックポイントで使用されているものと類似した新しいトークナイザーを訓練する方法</li> <li>高速トークナイザーの特別な機能</li> <li>現在NLPで使用されている3つの主要なサブワード・トークン化アルゴリズムの違い</li> <li>Hugging Face Tokenizersライブラリを使用して、一からトークナイザーを構築し、データで訓練する方法</li> </ul> <h2 id=_4>既存のトークナイザーから新しいトークナイザーを訓練する<a class=headerlink href=#_4 title="Permanent link">&para;</a></h2> <p>対象言語の言語モデルが利用できない場合や、自分のコーパスがモデルの訓練データと大きく異なる場合、データに適応したトークナイザーを使用してモデルを一から再訓練することが望ましいでしょう。これには、データセット上で新しいトークナイザーを訓練する必要があります。しかし、これは具体的にはどういう意味なのでしょうか？</p> <p>第2章で最初にトークナイザーを見た時、ほとんどのTransformerモデルが<strong>サブワード・トークン化アルゴリズム</strong>を使用することを確認しました。どのサブワードが関心があり、手元のコーパスで最も頻繁に出現するかを特定するために、トークナイザーは、コーパス内のすべてのテキストを詳細に調べる必要があります。このプロセスを<strong>訓練</strong>と呼びます。この訓練を支配する正確なルールは、使用されるトークナイザーの種類によって異なり、この章の後半で3つの主要アルゴリズムについて説明します。</p> <div class="admonition tip"> <p class=admonition-title>重要な違い</p> <p>トークナイザーの訓練はモデルの訓練とは異なります！モデルの訓練では確率的勾配降下法を使用して、各バッチで損失を少しずつ小さくします。これは本質的にランダムです（同じ訓練を2回行った場合に同じ結果を得るには、シードを設定する必要があります）。トークナイザーの訓練は統計的プロセスで、特定のコーパスに対してどのサブワードが最適かを特定しようとし、選択に使用される正確なルールはトークン化アルゴリズムによって異なります。これは決定論的で、同じアルゴリズムで同じコーパスで訓練すると常に同じ結果が得られます。</p> </div> <h3 id=_5>コーパスの構築<a class=headerlink href=#_5 title="Permanent link">&para;</a></h3> <p>Hugging Face Transformersには、既存のトークナイザーと同じ特性を持つ新しいトークナイザーを訓練するために使用できる非常にシンプルなAPIがあります：<code>AutoTokenizer.train_new_from_iterator()</code>。実際の動作を確認するために、GPT-2を英語以外の言語で一から訓練したいとします。最初のタスクは、その言語で訓練コーパス内に大量のデータを収集することです。わかりやすい例として、ここではロシア語や中国語のような言語ではなく、特殊な英語言語であるPythonコードを使用します。</p> <p><a href=https://github.com/huggingface/datasets>Hugging Face Datasets</a>ライブラリは、Pythonソースコードのコーパスを構築するのに役立ちます。通常の<code>load_dataset()</code>関数を使用して、<a href=https://huggingface.co/datasets/code_search_net>CodeSearchNet</a>データセットをダウンロードしてキャッシュします。このデータセットは<a href=https://wandb.ai/github/CodeSearchNet/benchmark>CodeSearchNet challenge</a>用に作成され、複数のプログラミング言語でGitHub上のオープンソースライブラリから数百万の関数を含んでいます。ここでは、このデータセットのPython部分を読み込みます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=c1># 読み込みに数分かかる可能性があるので、コーヒーやお茶を用意してお待ちください！</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>raw_datasets</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;claudios/code_search_net&quot;</span><span class=p>,</span> <span class=s2>&quot;python&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>Generating train split: 100%|██████████| 412178/412178 [00:00&lt;00:00, 954840.50 examples/s]
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>Generating test split: 100%|██████████| 22176/22176 [00:00&lt;00:00, 1246554.16 examples/s]
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>Generating validation split: 100%|██████████| 23107/23107 [00:00&lt;00:00, 1231765.62 examples/s]
</span></code></pre></div></p> <p>訓練分割を見て、アクセス可能な列を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>Dataset({
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    features: [&#39;repository_name&#39;, &#39;func_path_in_repository&#39;, &#39;func_name&#39;, &#39;whole_func_string&#39;, &#39;language&#39;, &#39;func_code_string&#39;, &#39;func_documentation_string&#39;, &#39;func_code_url&#39;],
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    num_rows: 412178
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>})
</span></code></pre></div></p> <p>データセットがdocstringsとコードを分離し、両方のトークン化を提案していることがわかります。ここでは、トークナイザーを訓練するために<code>whole_func_string</code>列のみを使用します。<code>train</code>分割にインデックスを付けることで、これらの関数の例を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=nb>print</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=mi>111111</span><span class=p>][</span><span class=s2>&quot;whole_func_string&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>def progress_task(name=None, t=INFO, max_value=100, *args, **kwargs):
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>    &quot;&quot;&quot;
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    This decorator extends the basic @task decorator by allowing users to
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>    display some form of progress on the console. The module can receive
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>    an increment in the progress through &quot;tick_progress&quot;.
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>    &quot;&quot;&quot;
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>    return task(name=name, t=t, init_progress=True, max_value=max_value,
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>                *args, **kwargs)
</span></code></pre></div></p> <p>最初に行う必要があることは、データセットを<strong>テキストリストのイテレータ</strong>に変換することです。例えば、リストのリストです。リストのテキストを使用することで、トークナイザーは（個別のテキストを一つずつ処理する代わりに）テキストのバッチで訓練することで高速化できます。また、すべてをメモリに一度に保存したくない場合は、イテレータである必要があります。コーパスが巨大な場合、Hugging Face DatasetsがすべてをRAMに読み込まず、データセットの要素をディスク上に格納するという事実を活用したいでしょう。</p> <p>以下のようにすると、それぞれ1,000テキストのリストのリストが作成されますが、すべてがメモリに読み込まれます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># データセットが小さくない限り、次の行をコメントアウトしないでください！</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=c1># training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]</span>
</span></code></pre></div> <p>Pythonジェネレータを使用すると、実際に必要になるまでPythonが何もメモリに読み込まないようにできます。このようなジェネレータを作成するには、ブラケットを括弧に置き換えるだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=n>training_corpus</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>    <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1000</span><span class=p>][</span><span class=s2>&quot;whole_func_string&quot;</span><span class=p>]</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]),</span> <span class=mi>1000</span><span class=p>)</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=p>)</span>
</span></code></pre></div> <p>このコード行はデータセットの要素を取得しません。Python <code>for</code>ループで使用できるオブジェクトを作成するだけです。テキストは必要な時（つまり、それらを必要とする<code>for</code>ループのステップにいる時）にのみ読み込まれ、一度に1,000テキストのみがメモリに読み込まれます。このように、巨大なデータセットを処理していても、すべてのメモリを使い果たすことはありません。</p> <p>ジェネレータオブジェクトの問題は、一度しか使用できないことです。以下のように最初の10桁のリストを2回取得する代わりに：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=n>gen</span> <span class=o>=</span> <span class=p>(</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=nb>print</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>gen</span><span class=p>))</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=nb>print</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>gen</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>[]
</span></code></pre></div></p> <p>一回取得し、その後空のリストが返されます。そのため、代わりにジェネレータを返す関数を定義します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=k>def</span><span class=w> </span><span class=nf>get_training_corpus</span><span class=p>():</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>    <span class=k>return</span> <span class=p>(</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>        <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>][</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1000</span><span class=p>][</span><span class=s2>&quot;whole_func_string&quot;</span><span class=p>]</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]),</span> <span class=mi>1000</span><span class=p>)</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>    <span class=p>)</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a><span class=n>training_corpus</span> <span class=o>=</span> <span class=n>get_training_corpus</span><span class=p>()</span>
</span></code></pre></div> <p><code>yield</code>文を使用して<code>for</code>ループ内でジェネレータを定義することもできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=k>def</span><span class=w> </span><span class=nf>get_training_corpus</span><span class=p>():</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>    <span class=n>dataset</span> <span class=o>=</span> <span class=n>raw_datasets</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>    <span class=k>for</span> <span class=n>start_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>),</span> <span class=mi>1000</span><span class=p>):</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>        <span class=n>samples</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[</span><span class=n>start_idx</span> <span class=p>:</span> <span class=n>start_idx</span> <span class=o>+</span> <span class=mi>1000</span><span class=p>]</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>        <span class=k>yield</span> <span class=n>samples</span><span class=p>[</span><span class=s2>&quot;whole_func_string&quot;</span><span class=p>]</span>
</span></code></pre></div> <p>これにより、前のものとまったく同じジェネレータが生成されますが、リスト内包表記では不可能な、より複雑なロジックを使用できます。</p> <h3 id=_6>新しいトークナイザーの訓練<a class=headerlink href=#_6 title="Permanent link">&para;</a></h3> <p>テキストのバッチのイテレータの形でコーパスを準備できたので、新しいトークナイザーを訓練する準備が整いました。これを行うには、まずモデルと組み合わせたいトークナイザー（ここではGPT-2）を読み込む必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=n>old_tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;gpt2&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>新しいトークナイザーを訓練するつもりでも、これを行うのは良いアイデアです。完全に一から始める必要がないからです。このように、トークン化アルゴリズムや使用したい特別なトークンについて何も指定する必要がありません。新しいトークナイザーはGPT-2とまったく同じになり、変化するのは語彙のみで、これはコーパスでの訓練によって決定されます。</p> <p>まず、このトークナイザーが例の関数をどのように処理するかを見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=kn>from</span><span class=w> </span><span class=nn>pprint</span><span class=w> </span><span class=kn>import</span> <span class=n>pprint</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>example</span> <span class=o>=</span> <span class=s1>&#39;&#39;&#39;def add_numbers(a, b):</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=s1>    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=s1>    return a + b&#39;&#39;&#39;</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>old_tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>example</span><span class=p>)</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>[&#39;def&#39;, &#39;Ä add&#39;, &#39;_&#39;, &#39;n&#39;, &#39;umbers&#39;, &#39;(&#39;, &#39;a&#39;, &#39;,&#39;, &#39;Ä b&#39;, &#39;):&#39;, &#39;ÄŠ&#39;, &#39;Ä &#39;, &#39;Ä &#39;,
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a> &#39;Ä &#39;, &#39;Ä &quot;&quot;&quot;&#39;, &#39;Add&#39;, &#39;Ä the&#39;, &#39;Ä two&#39;, &#39;Ä numbers&#39;, &#39;Ä `&#39;, &#39;a&#39;, &#39;`&#39;, &#39;Ä and&#39;, &#39;Ä `&#39;,
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a> &#39;b&#39;, &#39;`&#39;, &#39;.&quot;&#39;, &#39;&quot;&quot;&#39;, &#39;ÄŠ&#39;, &#39;Ä &#39;, &#39;Ä &#39;, &#39;Ä &#39;, &#39;Ä return&#39;, &#39;Ä a&#39;, &#39;Ä +&#39;, &#39;Ä b&#39;]
</span></code></pre></div></p> <p>このトークナイザーには、スペースと改行をそれぞれ表す<code>Ä</code>や<code>ÄŠ</code>のような特別なシンボルがあります。見てのとおり、これはあまり効率的ではありません：トークナイザーは各スペースに対して個別のトークンを返しますが、インデントレベルをグループ化できます（4つまたは8つのスペースのセットがコードでは非常に一般的であるため）。また、<code>_</code>文字を見慣れていないため、関数名を少し奇妙に分割しています。</p> <p>新しいトークナイザーを訓練して、これらの問題が解決するかどうか見てみましょう。これには、<code>train_new_from_iterator()</code>メソッドを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>old_tokenizer</span><span class=o>.</span><span class=n>train_new_from_iterator</span><span class=p>(</span><span class=n>training_corpus</span><span class=p>,</span> <span class=mi>52000</span><span class=p>)</span>
</span></code></pre></div> <p>コーパスが非常に大きい場合、このコマンドには少し時間がかかる可能性がありますが、1.6 GBのテキストのこのデータセットでは非常に高速です（12コアのAMD Ryzen 9 3900X CPUで1分16秒）。</p> <p><code>AutoTokenizer.train_new_from_iterator()</code>は、使用しているトークナイザーが「高速」トークナイザーの場合にのみ動作することに注意してください。次のセクションで見るように、Hugging Face Transformersライブラリには2つのタイプのトークナイザーが含まれています：純粋にPythonで書かれたものと、<a href=https://www.rust-lang.org>Rust</a>プログラミング言語で書かれたHugging Face Tokenizersライブラリによってサポートされている高速なものです。Pythonはデータサイエンスとディープラーニングアプリケーションで最もよく使用される言語ですが、何かを高速化するために並列化する必要がある場合、別の言語で書く必要があります。例えば、モデル計算の中心にある行列乗算は、GPU用に最適化されたCライブラリであるCUDAで書かれています。</p> <p>純粋なPythonで全く新しいトークナイザーを訓練することは非常に時間がかかるため、Hugging Face Tokenizersライブラリが開発されました。GPUでの入力バッチを用いたモデル実行のためにCUDA言語を学ぶ必要がないのと同様に、高速トークナイザーを使用するためにRustを学ぶ必要はありません。Hugging Face Tokenizersライブラリは、内部的にRustのコード片を呼び出す多くのメソッドにPythonバインディングを提供します。例えば、新しいトークナイザーの訓練を並列化したり、第3章で見たように、入力のバッチのトークン化を行います。</p> <p>ほとんどのTransformerモデルには高速トークナイザーが利用可能です（<a href=https://huggingface.co/transformers/#supported-frameworks>ここ</a>で確認できる例外もあります）。<code>AutoTokenizer</code> APIは、利用可能であれば常に高速トークナイザーを選択します。次のセクションでは、高速トークナイザーが持つ他の特別な機能について詳しく見ていきます。これらは、トークン分類や質問応答のようなタスクで本当に役立ちます。しかし、それに飛び込む前に、前の例で私たちの全く新しいトークナイザーを試してみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>example</span><span class=p>)</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a>[&#39;def&#39;, &#39;Ä add&#39;, &#39;_&#39;, &#39;numbers&#39;, &#39;(&#39;, &#39;a&#39;, &#39;,&#39;, &#39;Ä b&#39;, &#39;):&#39;, &#39;ÄŠÄ Ä Ä &#39;, &#39;Ä &quot;&quot;&quot;&#39;,
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a> &#39;Add&#39;, &#39;Ä the&#39;, &#39;Ä two&#39;, &#39;Ä numbers&#39;, &#39;Ä `&#39;, &#39;a&#39;, &#39;`&#39;, &#39;Ä and&#39;, &#39;Ä `&#39;, &#39;b&#39;, &#39;`.&quot;&quot;&quot;&#39;,
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a> &#39;ÄŠÄ Ä Ä &#39;, &#39;Ä return&#39;, &#39;Ä a&#39;, &#39;Ä +&#39;, &#39;Ä b&#39;]
</span></code></pre></div></p> <p>ここでも、スペースと改行を表す特別なシンボル<code>Ä</code>と<code>ÄŠ</code>が見られますが、トークナイザーがPython関数のコーパスに非常に特化したトークンを学習したことも確認できます。例えば、インデントを表す<code>ÄŠÄ Ä Ä</code>トークンや、docstringを開始する3つの引用符を表す<code>Ä """</code>トークンがあります。また、トークナイザーは<code>_</code>で関数名を正しく分割しました。これは非常にコンパクトな表現です。比較として、同じ例で通常の英語トークナイザーを使用すると、より長い文章が得られます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=nb>print</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>))</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=nb>print</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>old_tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>example</span><span class=p>)))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a>27
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>36
</span></code></pre></div></p> <p>別の例を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=n>example</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;class LinearLayer():</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=s2>    def __init__(self, input_size, output_size):</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a><span class=s2>        self.weight = torch.randn(input_size, output_size)</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a><span class=s2>        self.bias = torch.zeros(output_size)</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a><span class=s2>    def __call__(self, x):</span>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a><span class=s2>        return x @ self.weights + self.bias</span>
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=s2>    &quot;&quot;&quot;</span>
</span><span id=__span-20-9><a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=n>pprint</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>example</span><span class=p>),</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a>[&#39;class&#39;, &#39;Ä Linear&#39;, &#39;Layer&#39;, &#39;():&#39;, &#39;ÄŠÄ Ä Ä &#39;, &#39;Ä def&#39;, &#39;Ä __&#39;, &#39;init&#39;, &#39;__(&#39;,
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a> &#39;self&#39;, &#39;,&#39;, &#39;Ä input&#39;, &#39;_&#39;, &#39;size&#39;, &#39;,&#39;, &#39;Ä output&#39;, &#39;_&#39;, &#39;size&#39;, &#39;):&#39;,
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a> &#39;ÄŠÄ Ä Ä Ä Ä Ä Ä &#39;, &#39;Ä self&#39;, &#39;.&#39;, &#39;weight&#39;, &#39;Ä =&#39;, &#39;Ä torch&#39;, &#39;.&#39;, &#39;randn&#39;, &#39;(&#39;, &#39;input&#39;,
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a> &#39;_&#39;, &#39;size&#39;, &#39;,&#39;, &#39;Ä output&#39;, &#39;_&#39;, &#39;size&#39;, &#39;)&#39;, &#39;ÄŠÄ Ä Ä Ä Ä Ä Ä &#39;, &#39;Ä self&#39;, &#39;.&#39;,
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a> &#39;bias&#39;, &#39;Ä =&#39;, &#39;Ä torch&#39;, &#39;.&#39;, &#39;zeros&#39;, &#39;(&#39;, &#39;output&#39;, &#39;_&#39;, &#39;size&#39;, &#39;)&#39;, &#39;ÄŠÄŠÄ Ä Ä &#39;,
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a> &#39;Ä def&#39;, &#39;Ä __&#39;, &#39;call&#39;, &#39;__(&#39;, &#39;self&#39;, &#39;,&#39;, &#39;Ä x&#39;, &#39;):&#39;, &#39;ÄŠÄ Ä Ä Ä Ä Ä Ä &#39;, &#39;Ä return&#39;,
</span><span id=__span-21-7><a id=__codelineno-21-7 name=__codelineno-21-7 href=#__codelineno-21-7></a> &#39;Ä x&#39;, &#39;Ä @&#39;, &#39;Ä self&#39;, &#39;.&#39;, &#39;weights&#39;, &#39;Ä +&#39;, &#39;Ä self&#39;, &#39;.&#39;, &#39;bias&#39;, &#39;ÄŠÄ Ä Ä Ä &#39;]
</span></code></pre></div></p> <p>インデントに対応するトークンに加えて、ここでは二重インデントのトークン<code>ÄŠÄ Ä Ä Ä Ä Ä Ä</code>も確認できます。<code>class</code>、<code>init</code>、<code>call</code>、<code>self</code>、<code>return</code>のようなPythonの特別な単語は、それぞれ一つのトークンとしてトークン化されています。また、<code>_</code>と<code>.</code>での分割に加えて、トークナイザーはキャメルケースの名前も正しく分割します：<code>LinearLayer</code>は<code>["Ä Linear", "Layer"]</code>としてトークン化されます。</p> <h3 id=_7>トークナイザーの保存<a class=headerlink href=#_7 title="Permanent link">&para;</a></h3> <p>後で使用できるように、新しいトークナイザーを保存する必要があります。モデルと同様に、これは<code>save_pretrained()</code>メソッドで行われます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;coder-search-net-tokenizer&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>(&#39;coder-search-net-tokenizer/tokenizer_config.json&#39;,
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a> &#39;coder-search-net-tokenizer/special_tokens_map.json&#39;,
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a> &#39;coder-search-net-tokenizer/vocab.json&#39;,
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a> &#39;coder-search-net-tokenizer/merges.txt&#39;,
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a> &#39;coder-search-net-tokenizer/added_tokens.json&#39;,
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a> &#39;coder-search-net-tokenizer/tokenizer.json&#39;)
</span></code></pre></div></p> <p>これにより、<em>code-search-net-tokenizer</em>という名前の新しいフォルダが作成され、トークナイザーの再読み込みに必要なすべてのファイルが含まれます。</p> <p>その後、<code>from_pretrained()</code>メソッドでトークナイザーをどこからでも読み込むことができます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;coder-search-net-tokenizer&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=_8>高速トークナイザーの特別な機能<a class=headerlink href=#_8 title="Permanent link">&para;</a></h2> <p>このセクションでは、Hugging Face Transformersのトークナイザーの機能をより詳しく見ていきます。これまでは、入力をトークン化したり、IDをテキストにデコードしたりするためにのみ使用していましたが、トークナイザー（特にHugging Face Tokenizersライブラリによってサポートされているもの）はもっと多くのことができます。これらの追加機能を説明するために、第1章で最初に遭遇した<code>token-classification</code>（<code>ner</code>と呼んでいました）と<code>question-answering</code>パイプラインの結果を再現する方法を探求します。</p> <p>以下の議論では、「低速」トークナイザーと「高速」トークナイザーを頻繁に区別します。低速トークナイザーはHugging Face TransformersライブラリのPythonで書かれたものです。一方、高速版はHugging Face Tokenizersによって提供され、Rustで書かれています。</p> <table> <thead> <tr> <th style="text-align: center;"></th> <th style="text-align: center;">高速トークナイザー</th> <th style="text-align: center;">低速トークナイザー</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><code>batched=True</code></td> <td style="text-align: center;">10.8s</td> <td style="text-align: center;">4min41s</td> </tr> <tr> <td style="text-align: center;"><code>batched=False</code></td> <td style="text-align: center;">59.2s</td> <td style="text-align: center;">5min3s</td> </tr> </tbody> </table> <h3 id=_9>バッチエンコーディング<a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <p>トークナイザーの出力は単純なPython辞書ではありません。実際に取得するのは特別な<code>BatchEncoding</code>オブジェクトです。これは辞書のサブクラスです（そのため、以前はその結果にインデックスを付けることができました）。ただし、主に高速トークナイザーで使用される追加のメソッドも備えています。</p> <p>並列化機能に加えて、高速トークナイザーの主要な機能は、最終的なトークンが元のテキストの範囲を常に追跡することです。これは<strong>オフセットマッピング</strong>と呼ばれる機能です。これにより、各単語がどのトークンを生成したかをマッピングしたり、元のテキストの各文字がどのトークンの内部にあるかをマッピングしたり、その逆を行う機能が解放されます。</p> <p>例を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span><span id=__span-25-4><a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a><span class=n>example</span> <span class=o>=</span> <span class=s2>&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
</span><span id=__span-25-5><a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>)</span>
</span><span id=__span-25-6><a id=__codelineno-25-6 name=__codelineno-25-6 href=#__codelineno-25-6></a><span class=nb>print</span><span class=p>(</span><span class=nb>type</span><span class=p>(</span><span class=n>encoding</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a>&lt;class &#39;transformers.tokenization_utils_base.BatchEncoding&#39;&gt;
</span></code></pre></div></p> <p>前述のとおり、トークナイザーの出力で<code>BatchEncoding</code>オブジェクトを取得します。</p> <p><code>AutoTokenizer</code>クラスはデフォルトで高速トークナイザーを選択するので、この<code>BatchEncoding</code>オブジェクトが提供する追加のメソッドを使用できます。トークナイザーが高速か低速かをチェックする方法は2つあります。<code>tokenizer</code>の<code>is_fast</code>属性をチェックできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>is_fast</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a>True
</span></code></pre></div></p> <p>または<code>encoding</code>の同じ属性をチェックできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=n>encoding</span><span class=o>.</span><span class=n>is_fast</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a>True
</span></code></pre></div></p> <p>高速トークナイザーができることを見てみましょう。まず、IDをトークンに変換せずにトークンにアクセスできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=n>pprint</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>(),</span> <span class=n>compact</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a>[&#39;[CLS]&#39;, &#39;My&#39;, &#39;name&#39;, &#39;is&#39;, &#39;S&#39;, &#39;##yl&#39;, &#39;##va&#39;, &#39;##in&#39;, &#39;and&#39;, &#39;I&#39;, &#39;work&#39;,
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a> &#39;at&#39;, &#39;Hu&#39;, &#39;##gging&#39;, &#39;Face&#39;, &#39;in&#39;, &#39;Brooklyn&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span></code></pre></div></p> <p>この場合、インデックス5のトークンは<code>##yl</code>で、元の文の単語「Sylvain」の一部です。<code>word_ids()</code>メソッドを使用して、各トークンが由来する単語のインデックスを取得することもできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a><span class=n>encoding</span><span class=o>.</span><span class=n>word_ids</span><span class=p>()</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a>[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
</span></code></pre></div></p> <p>トークナイザーの特別なトークン<code>[CLS]</code>と<code>[SEP]</code>が<code>None</code>にマッピングされ、その後、各トークンが元の単語にマッピングされることがわかります。これは、トークンが単語の開始にあるかどうか、または2つのトークンが同じ単語内にあるかどうかを決定するのに特に有用です。<code>##</code>プレフィックスに依存することもできますが、これはBERTのようなトークナイザーでのみ動作します。この方法は、高速である限り、あらゆるタイプのトークナイザーで動作します。次の章では、この機能を使用して、固有表現認識（NER）や品詞タギング（POS）のようなタスクで、各単語に持っているラベルをトークンに適切に適用する方法を説明します。また、マスクド言語モデリング（<strong>全単語マスキング</strong>と呼ばれる技術）で、同じ単語から来るすべてのトークンをマスクするためにも使用できます。</p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>単語とは何かという概念は複雑です。例えば、「I'll」（「I will」の短縮形）は1つの単語でしょうか、それとも2つの単語でしょうか？実際には、トークナイザーと適用する前処理操作によって異なります。一部のトークナイザーはスペースでのみ分割するため、これを1つの単語とみなします。その他のトークナイザーはスペースに加えて句読点も使用するため、2つの単語とみなします。</p> </div> <div class="admonition question"> <p class=admonition-title>試してみましょう！</p> <p><code>bert-base-cased</code>と<code>roberta-base</code>チェックポイントからトークナイザーを作成し、「81s」でトークン化してください。何が観察されますか？単語IDは何ですか？</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a><span class=n>bert_tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a><span class=n>bert_encoding</span> <span class=o>=</span> <span class=n>bert_tokenizer</span><span class=p>(</span><span class=s2>&quot;81s&quot;</span><span class=p>)</span>
</span><span id=__span-35-4><a id=__codelineno-35-4 name=__codelineno-35-4 href=#__codelineno-35-4></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;bert_encoding tokens: </span><span class=si>{</span><span class=n>bert_encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-35-5><a id=__codelineno-35-5 name=__codelineno-35-5 href=#__codelineno-35-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;bert_encoding word_ids: </span><span class=si>{</span><span class=n>bert_encoding</span><span class=o>.</span><span class=n>word_ids</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-35-6><a id=__codelineno-35-6 name=__codelineno-35-6 href=#__codelineno-35-6></a>
</span><span id=__span-35-7><a id=__codelineno-35-7 name=__codelineno-35-7 href=#__codelineno-35-7></a><span class=n>roberta_tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;roberta-base&quot;</span><span class=p>)</span>
</span><span id=__span-35-8><a id=__codelineno-35-8 name=__codelineno-35-8 href=#__codelineno-35-8></a><span class=n>roberta_encoding</span> <span class=o>=</span> <span class=n>roberta_tokenizer</span><span class=p>(</span><span class=s2>&quot;81s&quot;</span><span class=p>)</span>
</span><span id=__span-35-9><a id=__codelineno-35-9 name=__codelineno-35-9 href=#__codelineno-35-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;roberta_encoding tokens: </span><span class=si>{</span><span class=n>roberta_encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-35-10><a id=__codelineno-35-10 name=__codelineno-35-10 href=#__codelineno-35-10></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;roberta_encoding word_ids: </span><span class=si>{</span><span class=n>roberta_encoding</span><span class=o>.</span><span class=n>word_ids</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a>bert_encoding tokens: [&#39;[CLS]&#39;, &#39;81&#39;, &#39;##s&#39;, &#39;[SEP]&#39;]
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a>bert_encoding word_ids: [None, 0, 0, None]
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a>roberta_encoding tokens: [&#39;&lt;s&gt;&#39;, &#39;81&#39;, &#39;s&#39;, &#39;&lt;/s&gt;&#39;]
</span><span id=__span-36-4><a id=__codelineno-36-4 name=__codelineno-36-4 href=#__codelineno-36-4></a>roberta_encoding word_ids: [None, 0, 1, None]
</span></code></pre></div></p> <p>同様に、トークンが由来する文にマッピングするために使用できる<code>sentence_ids()</code>メソッドもあります（ただし、この場合、トークナイザーによって返される<code>token_type_ids</code>が同じ情報を提供できます）。</p> <p>最後に、<code>word_to_chars()</code>または<code>token_to_chars()</code>および<code>char_to_word()</code>または<code>char_to_token()</code>メソッドを通じて、任意の単語またはトークンを元のテキストの文字にマッピングしたり、その逆を行うことができます。例えば、<code>word_ids()</code>メソッドは<code>##yl</code>が インデックス3の単語の一部であることを教えてくれましたが、文中ではどの単語でしょうか？次のようにして確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=n>start</span><span class=p>,</span> <span class=n>end</span> <span class=o>=</span> <span class=n>encoding</span><span class=o>.</span><span class=n>word_to_chars</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a><span class=n>example</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a>&#39;Sylvain&#39;
</span></code></pre></div></p> <p>前述のとおり、これは高速トークナイザーが<strong>オフセット</strong>のリストで各トークンが由来するテキストのスパンを追跡しているという事実によって支えられています。その使用法を説明するために、次に<code>token-classification</code>パイプラインの結果を手動で再現する方法を示します。</p> <h3 id=token-classification><code>token-classification</code>パイプラインの内部<a class=headerlink href=#token-classification title="Permanent link">&para;</a></h3> <p>第1章では、NERを初めて体験しました。NERは、テキストのどの部分が人、場所、組織などのエンティティに対応するかを特定するタスクです。Hugging Face Transformersの<code>pipeline()</code>関数を使用しました。その後、第2章では、パイプラインが生のテキストから予測を取得するために必要な3つのステージ（トークン化、モデルを通じた入力の渡し、後処理）をグループ化する方法を見ました。<code>token-classification</code>パイプラインの最初の2つのステップは他のパイプラインと同じですが、後処理はもう少し複雑です。見てみましょう！</p> <h4 id=_10>パイプラインでの基本結果の取得<a class=headerlink href=#_10 title="Permanent link">&para;</a></h4> <p>まず、トークン分類パイプラインを取得して、手動で比較するための結果を得ましょう。デフォルトで使用されるモデルは<a href=https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english><code>dbmdz/bert-large-cased-finetuned-conll03-english</code></a>で、文に対してNERを実行します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a>
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a><span class=n>token_classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&quot;token-classification&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span><span class=p>)</span>
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4 href=#__codelineno-39-4></a><span class=n>pprint</span><span class=p>(</span><span class=n>token_classifier</span><span class=p>(</span><span class=s2>&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span><span class=p>),</span> <span class=n>width</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a>[{&#39;end&#39;: 12, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 4, &#39;score&#39;: np.float32(0.99938285), &#39;start&#39;: 11, &#39;word&#39;: &#39;S&#39;},
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a> {&#39;end&#39;: 14, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 5, &#39;score&#39;: np.float32(0.99815494), &#39;start&#39;: 12, &#39;word&#39;: &#39;##yl&#39;},
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a> {&#39;end&#39;: 16, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 6, &#39;score&#39;: np.float32(0.9959072), &#39;start&#39;: 14, &#39;word&#39;: &#39;##va&#39;},
</span><span id=__span-40-4><a id=__codelineno-40-4 name=__codelineno-40-4 href=#__codelineno-40-4></a> {&#39;end&#39;: 18, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 7, &#39;score&#39;: np.float32(0.99923277), &#39;start&#39;: 16, &#39;word&#39;: &#39;##in&#39;},
</span><span id=__span-40-5><a id=__codelineno-40-5 name=__codelineno-40-5 href=#__codelineno-40-5></a> {&#39;end&#39;: 35, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 12, &#39;score&#39;: np.float32(0.9738931), &#39;start&#39;: 33, &#39;word&#39;: &#39;Hu&#39;},
</span><span id=__span-40-6><a id=__codelineno-40-6 name=__codelineno-40-6 href=#__codelineno-40-6></a> {&#39;end&#39;: 40, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 13, &#39;score&#39;: np.float32(0.97611505), &#39;start&#39;: 35, &#39;word&#39;: &#39;##gging&#39;},
</span><span id=__span-40-7><a id=__codelineno-40-7 name=__codelineno-40-7 href=#__codelineno-40-7></a> {&#39;end&#39;: 45, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 14, &#39;score&#39;: np.float32(0.9887976), &#39;start&#39;: 41, &#39;word&#39;: &#39;Face&#39;},
</span><span id=__span-40-8><a id=__codelineno-40-8 name=__codelineno-40-8 href=#__codelineno-40-8></a> {&#39;end&#39;: 57, &#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 16, &#39;score&#39;: np.float32(0.9932106), &#39;start&#39;: 49, &#39;word&#39;: &#39;Brooklyn&#39;}]
</span></code></pre></div></p> <p>モデルは「Sylvain」によって生成された各トークンを人として、「Hugging Face」によって生成された各トークンを組織として、「Brooklyn」トークンを場所として適切に識別しました。また、パイプラインに同じエンティティに対応するトークンをまとめるように依頼することもできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a>
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a><span class=n>token_classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&quot;token-classification&quot;</span><span class=p>,</span> <span class=n>aggregation_strategy</span><span class=o>=</span><span class=s2>&quot;simple&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span><span class=p>)</span>
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a><span class=n>pprint</span><span class=p>(</span><span class=n>token_classifier</span><span class=p>(</span><span class=s2>&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span><span class=p>),</span> <span class=n>width</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a>[{&#39;end&#39;: 18, &#39;entity_group&#39;: &#39;PER&#39;, &#39;score&#39;: np.float32(0.9981694), &#39;start&#39;: 11, &#39;word&#39;: &#39;Sylvain&#39;},
</span><span id=__span-42-2><a id=__codelineno-42-2 name=__codelineno-42-2 href=#__codelineno-42-2></a> {&#39;end&#39;: 45, &#39;entity_group&#39;: &#39;ORG&#39;, &#39;score&#39;: np.float32(0.9796019), &#39;start&#39;: 33, &#39;word&#39;: &#39;Hugging Face&#39;},
</span><span id=__span-42-3><a id=__codelineno-42-3 name=__codelineno-42-3 href=#__codelineno-42-3></a> {&#39;end&#39;: 57, &#39;entity_group&#39;: &#39;LOC&#39;, &#39;score&#39;: np.float32(0.9932106), &#39;start&#39;: 49, &#39;word&#39;: &#39;Brooklyn&#39;}]
</span></code></pre></div></p> <p>選択された<code>aggregation_strategy</code>は、グループ化された各エンティティに対して計算されるスコアを変更します。<code>"simple"</code>では、スコアは単に特定のエンティティ内の各トークンのスコアの平均です。例えば、「Sylvain」のスコアは、前の例で見たトークン<code>S</code>、<code>##yl</code>、<code>##va</code>、<code>##in</code>のスコアの平均です。利用可能なその他の戦略は次のとおりです：</p> <ul> <li><code>"first"</code>：各エンティティのスコアはそのエンティティの最初のトークンのスコアです（そのため「Sylvain」の場合、トークン<code>S</code>のスコア0.993828になります）</li> <li><code>"max"</code>：各エンティティのスコアはそのエンティティ内のトークンの最大スコアです（そのため「Hugging Face」の場合、「Face」のスコア0.98879766になります）</li> <li><code>"average"</code>：各エンティティのスコアはそのエンティティを構成する単語のスコアの平均です（そのため「Sylvain」の場合、<code>"simple"</code>戦略との違いはありませんが、「Hugging Face」のスコアは「Hugging」0.975と「Face」0.98879のスコアの平均である0.9819になります）</li> </ul> <h4 id=_11>入力から予測まで<a class=headerlink href=#_11 title="Permanent link">&para;</a></h4> <p>まず、入力をトークン化してモデルを通す必要があります。これは第2章でまったく同じように行われます。<code>AutoXxx</code>クラスを使用してトークナイザーとモデルをインスタンス化し、例に使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForTokenClassification</span>
</span><span id=__span-43-2><a id=__codelineno-43-2 name=__codelineno-43-2 href=#__codelineno-43-2></a>
</span><span id=__span-43-3><a id=__codelineno-43-3 name=__codelineno-43-3 href=#__codelineno-43-3></a><span class=n>model_checkpoint</span> <span class=o>=</span> <span class=s2>&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
</span><span id=__span-43-4><a id=__codelineno-43-4 name=__codelineno-43-4 href=#__codelineno-43-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_checkpoint</span><span class=p>)</span>
</span><span id=__span-43-5><a id=__codelineno-43-5 name=__codelineno-43-5 href=#__codelineno-43-5></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForTokenClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_checkpoint</span><span class=p>)</span>
</span><span id=__span-43-6><a id=__codelineno-43-6 name=__codelineno-43-6 href=#__codelineno-43-6></a>
</span><span id=__span-43-7><a id=__codelineno-43-7 name=__codelineno-43-7 href=#__codelineno-43-7></a><span class=n>example</span> <span class=o>=</span> <span class=s2>&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
</span><span id=__span-43-8><a id=__codelineno-43-8 name=__codelineno-43-8 href=#__codelineno-43-8></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-43-9><a id=__codelineno-43-9 name=__codelineno-43-9 href=#__codelineno-43-9></a><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <p>ここでは<code>AutoModelForTokenClassification</code>を使用しているので、入力シーケンスの各トークンに対して一組のロジットを取得します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span><span id=__span-44-2><a id=__codelineno-44-2 name=__codelineno-44-2 href=#__codelineno-44-2></a><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a>torch.Size([1, 19])
</span><span id=__span-45-2><a id=__codelineno-45-2 name=__codelineno-45-2 href=#__codelineno-45-2></a>torch.Size([1, 19, 9])
</span></code></pre></div></p> <p>1つのシーケンスを含む19トークンのバッチがあり、モデルには9つの異なるラベルがあるため、モデルの出力は1 x 19 x 9の形状になります。テキスト分類パイプラインと同様に、ソフトマックス関数を使用してそれらのロジットを確率に変換し、argmaxを取って予測を得ます（ソフトマックスは順序を変更しないため、ロジットでargmaxを取ることができます）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a>
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a><span class=n>probabilities</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span><span id=__span-46-4><a id=__codelineno-46-4 name=__codelineno-46-4 href=#__codelineno-46-4></a><span class=n>predictions</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span><span id=__span-46-5><a id=__codelineno-46-5 name=__codelineno-46-5 href=#__codelineno-46-5></a><span class=nb>print</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a>[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
</span></code></pre></div></p> <p><code>model.config.id2label</code>属性には、予測を理解するために使用できるインデックスからラベルへのマッピングが含まれています：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a>{0: &#39;O&#39;,
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a> 1: &#39;B-MISC&#39;,
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a> 2: &#39;I-MISC&#39;,
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a> 3: &#39;B-PER&#39;,
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a> 4: &#39;I-PER&#39;,
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6 href=#__codelineno-49-6></a> 5: &#39;B-ORG&#39;,
</span><span id=__span-49-7><a id=__codelineno-49-7 name=__codelineno-49-7 href=#__codelineno-49-7></a> 6: &#39;I-ORG&#39;,
</span><span id=__span-49-8><a id=__codelineno-49-8 name=__codelineno-49-8 href=#__codelineno-49-8></a> 7: &#39;B-LOC&#39;,
</span><span id=__span-49-9><a id=__codelineno-49-9 name=__codelineno-49-9 href=#__codelineno-49-9></a> 8: &#39;I-LOC&#39;}
</span></code></pre></div></p> <p>前述のとおり、9つのラベルがあります：<code>O</code>は名前付きエンティティ内にないトークンのラベル（「outside」の意味）で、各タイプのエンティティ（その他、人、組織、場所）に対して2つのラベルがあります。ラベル<code>B-XXX</code>はトークンがエンティティ<code>XXX</code>の開始にあることを示し、ラベル<code>I-XXX</code>はトークンがエンティティ<code>XXX</code>の内部にあることを示します。例えば、現在の例では、モデルがトークン<code>S</code>を<code>B-PER</code>（人エンティティの開始）に分類し、トークン<code>##yl</code>、<code>##va</code>、<code>##in</code>を<code>I-PER</code>（人エンティティの内部）に分類することが期待されます。</p> <p>この場合、モデルがこれら4つのトークンすべてに<code>I-PER</code>ラベルを付けたことが間違いだと思うかもしれませんが、それは完全に正しくありません。実際には、<code>B-</code>と<code>I-</code>ラベルには<strong>IOB1</strong>と<strong>IOB2</strong>という2つの形式があります。IOB2形式（下図のピンク）は我々が紹介したもので、IOB1形式（下図の青）では、<code>B-</code>で始まるラベルは同じタイプの2つの隣接するエンティティを分離するためにのみ使用されます。使用しているモデルはその形式を使用するデータセットでファインチューニングされているため、<code>S</code>トークンに<code>I-PER</code>ラベルを割り当てます。</p> <p><img alt=IOBバージョン src=../04_the_huggingface_tokenizers_library_files/IOB_versions.png></p> <p>このマップを使用して、最初のパイプラインの結果を（ほぼ完全に）再現する準備ができました。<code>O</code>として分類されなかった各トークンのスコアとラベルを取得するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a><span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-50-2><a id=__codelineno-50-2 name=__codelineno-50-2 href=#__codelineno-50-2></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>tokens</span><span class=p>()</span>
</span><span id=__span-50-3><a id=__codelineno-50-3 name=__codelineno-50-3 href=#__codelineno-50-3></a>
</span><span id=__span-50-4><a id=__codelineno-50-4 name=__codelineno-50-4 href=#__codelineno-50-4></a><span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>pred</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>predictions</span><span class=p>):</span>
</span><span id=__span-50-5><a id=__codelineno-50-5 name=__codelineno-50-5 href=#__codelineno-50-5></a>    <span class=n>label</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span><span class=p>[</span><span class=n>pred</span><span class=p>]</span>
</span><span id=__span-50-6><a id=__codelineno-50-6 name=__codelineno-50-6 href=#__codelineno-50-6></a>    <span class=k>if</span> <span class=n>label</span> <span class=o>!=</span> <span class=s2>&quot;O&quot;</span><span class=p>:</span>
</span><span id=__span-50-7><a id=__codelineno-50-7 name=__codelineno-50-7 href=#__codelineno-50-7></a>        <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span><span id=__span-50-8><a id=__codelineno-50-8 name=__codelineno-50-8 href=#__codelineno-50-8></a>            <span class=p>{</span><span class=s2>&quot;entity&quot;</span><span class=p>:</span> <span class=n>label</span><span class=p>,</span> <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=n>probabilities</span><span class=p>[</span><span class=n>idx</span><span class=p>][</span><span class=n>pred</span><span class=p>],</span> <span class=s2>&quot;index&quot;</span><span class=p>:</span> <span class=n>idx</span><span class=p>,</span> <span class=s2>&quot;word&quot;</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=n>idx</span><span class=p>]}</span>
</span><span id=__span-50-9><a id=__codelineno-50-9 name=__codelineno-50-9 href=#__codelineno-50-9></a>        <span class=p>)</span>
</span><span id=__span-50-10><a id=__codelineno-50-10 name=__codelineno-50-10 href=#__codelineno-50-10></a>
</span><span id=__span-50-11><a id=__codelineno-50-11 name=__codelineno-50-11 href=#__codelineno-50-11></a><span class=n>pprint</span><span class=p>(</span><span class=n>results</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a>[{&#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9993828535079956, &#39;word&#39;: &#39;S&#39;},
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a> {&#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 5, &#39;score&#39;: 0.9981549382209778, &#39;word&#39;: &#39;##yl&#39;},
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a> {&#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 6, &#39;score&#39;: 0.995907187461853, &#39;word&#39;: &#39;##va&#39;},
</span><span id=__span-51-4><a id=__codelineno-51-4 name=__codelineno-51-4 href=#__codelineno-51-4></a> {&#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 7, &#39;score&#39;: 0.9992326498031616, &#39;word&#39;: &#39;##in&#39;},
</span><span id=__span-51-5><a id=__codelineno-51-5 name=__codelineno-51-5 href=#__codelineno-51-5></a> {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 12, &#39;score&#39;: 0.9738929867744446, &#39;word&#39;: &#39;Hu&#39;},
</span><span id=__span-51-6><a id=__codelineno-51-6 name=__codelineno-51-6 href=#__codelineno-51-6></a> {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.9761149883270264, &#39;word&#39;: &#39;##gging&#39;},
</span><span id=__span-51-7><a id=__codelineno-51-7 name=__codelineno-51-7 href=#__codelineno-51-7></a> {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 14, &#39;score&#39;: 0.9887974858283997, &#39;word&#39;: &#39;Face&#39;},
</span><span id=__span-51-8><a id=__codelineno-51-8 name=__codelineno-51-8 href=#__codelineno-51-8></a> {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 16, &#39;score&#39;: 0.99321049451828, &#39;word&#39;: &#39;Brooklyn&#39;}]
</span></code></pre></div></p> <p>これは以前持っていたものと非常に似ていますが、1つの例外があります：パイプラインは、元の文の各エンティティの<code>start</code>と<code>end</code>に関する情報も提供しました。ここでオフセットマッピングが活躍します。オフセットを取得するには、入力にトークナイザーを適用する際に<code>return_offsets_mapping=True</code>を設定するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=n>inputs_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>,</span> <span class=n>return_offsets_mapping</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-52-2><a id=__codelineno-52-2 name=__codelineno-52-2 href=#__codelineno-52-2></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs_with_offsets</span><span class=p>[</span><span class=s2>&quot;offset_mapping&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a>[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
</span></code></pre></div></p> <p>各タプルは、各トークンに対応するテキストのスパンで、<code>(0, 0)</code>は特別なトークンのために予約されています。前述のとおり、インデックス5のトークンは<code>##yl</code>で、ここではオフセットとして<code>(12, 14)</code>があります。例の対応するスライスを取得すると：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=c1># `##`なしで適切なテキストスパンを取得します</span>
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a><span class=n>example</span><span class=p>[</span><span class=mi>12</span><span class=p>:</span><span class=mi>14</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a>&#39;yl&#39;
</span></code></pre></div></p> <p>これを使用して、前の結果を完成させることができます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a><span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-56-2><a id=__codelineno-56-2 name=__codelineno-56-2 href=#__codelineno-56-2></a><span class=n>inputs_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>,</span> <span class=n>return_offsets_mapping</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-56-3><a id=__codelineno-56-3 name=__codelineno-56-3 href=#__codelineno-56-3></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>inputs_with_offsets</span><span class=o>.</span><span class=n>tokens</span><span class=p>()</span>
</span><span id=__span-56-4><a id=__codelineno-56-4 name=__codelineno-56-4 href=#__codelineno-56-4></a><span class=n>offsets</span> <span class=o>=</span> <span class=n>inputs_with_offsets</span><span class=p>[</span><span class=s2>&quot;offset_mapping&quot;</span><span class=p>]</span>
</span><span id=__span-56-5><a id=__codelineno-56-5 name=__codelineno-56-5 href=#__codelineno-56-5></a>
</span><span id=__span-56-6><a id=__codelineno-56-6 name=__codelineno-56-6 href=#__codelineno-56-6></a><span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>pred</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>predictions</span><span class=p>):</span>
</span><span id=__span-56-7><a id=__codelineno-56-7 name=__codelineno-56-7 href=#__codelineno-56-7></a>    <span class=n>label</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span><span class=p>[</span><span class=n>pred</span><span class=p>]</span>
</span><span id=__span-56-8><a id=__codelineno-56-8 name=__codelineno-56-8 href=#__codelineno-56-8></a>    <span class=k>if</span> <span class=n>label</span> <span class=o>!=</span> <span class=s2>&quot;O&quot;</span><span class=p>:</span>
</span><span id=__span-56-9><a id=__codelineno-56-9 name=__codelineno-56-9 href=#__codelineno-56-9></a>        <span class=n>start</span><span class=p>,</span> <span class=n>end</span> <span class=o>=</span> <span class=n>offsets</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span><span id=__span-56-10><a id=__codelineno-56-10 name=__codelineno-56-10 href=#__codelineno-56-10></a>        <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span><span id=__span-56-11><a id=__codelineno-56-11 name=__codelineno-56-11 href=#__codelineno-56-11></a>            <span class=p>{</span>
</span><span id=__span-56-12><a id=__codelineno-56-12 name=__codelineno-56-12 href=#__codelineno-56-12></a>                <span class=s2>&quot;entity&quot;</span><span class=p>:</span> <span class=n>label</span><span class=p>,</span>
</span><span id=__span-56-13><a id=__codelineno-56-13 name=__codelineno-56-13 href=#__codelineno-56-13></a>                <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=n>probabilities</span><span class=p>[</span><span class=n>idx</span><span class=p>][</span><span class=n>pred</span><span class=p>],</span>
</span><span id=__span-56-14><a id=__codelineno-56-14 name=__codelineno-56-14 href=#__codelineno-56-14></a>                <span class=s2>&quot;index&quot;</span><span class=p>:</span> <span class=n>idx</span><span class=p>,</span>
</span><span id=__span-56-15><a id=__codelineno-56-15 name=__codelineno-56-15 href=#__codelineno-56-15></a>                <span class=s2>&quot;word&quot;</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=n>idx</span><span class=p>],</span>
</span><span id=__span-56-16><a id=__codelineno-56-16 name=__codelineno-56-16 href=#__codelineno-56-16></a>                <span class=s2>&quot;start&quot;</span><span class=p>:</span> <span class=n>start</span><span class=p>,</span>
</span><span id=__span-56-17><a id=__codelineno-56-17 name=__codelineno-56-17 href=#__codelineno-56-17></a>                <span class=s2>&quot;end&quot;</span><span class=p>:</span> <span class=n>end</span><span class=p>,</span>
</span><span id=__span-56-18><a id=__codelineno-56-18 name=__codelineno-56-18 href=#__codelineno-56-18></a>            <span class=p>}</span>
</span><span id=__span-56-19><a id=__codelineno-56-19 name=__codelineno-56-19 href=#__codelineno-56-19></a>        <span class=p>)</span>
</span><span id=__span-56-20><a id=__codelineno-56-20 name=__codelineno-56-20 href=#__codelineno-56-20></a>
</span><span id=__span-56-21><a id=__codelineno-56-21 name=__codelineno-56-21 href=#__codelineno-56-21></a><span class=n>pprint</span><span class=p>(</span><span class=n>results</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a>[{&#39;end&#39;: 12, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9993828535079956, &#39;start&#39;: 11, &#39;word&#39;: &#39;S&#39;},
</span><span id=__span-57-2><a id=__codelineno-57-2 name=__codelineno-57-2 href=#__codelineno-57-2></a> {&#39;end&#39;: 14, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 5, &#39;score&#39;: 0.9981549382209778, &#39;start&#39;: 12, &#39;word&#39;: &#39;##yl&#39;},
</span><span id=__span-57-3><a id=__codelineno-57-3 name=__codelineno-57-3 href=#__codelineno-57-3></a> {&#39;end&#39;: 16, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 6, &#39;score&#39;: 0.995907187461853, &#39;start&#39;: 14, &#39;word&#39;: &#39;##va&#39;},
</span><span id=__span-57-4><a id=__codelineno-57-4 name=__codelineno-57-4 href=#__codelineno-57-4></a> {&#39;end&#39;: 18, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 7, &#39;score&#39;: 0.9992326498031616, &#39;start&#39;: 16, &#39;word&#39;: &#39;##in&#39;},
</span><span id=__span-57-5><a id=__codelineno-57-5 name=__codelineno-57-5 href=#__codelineno-57-5></a> {&#39;end&#39;: 35, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 12, &#39;score&#39;: 0.9738929867744446, &#39;start&#39;: 33, &#39;word&#39;: &#39;Hu&#39;},
</span><span id=__span-57-6><a id=__codelineno-57-6 name=__codelineno-57-6 href=#__codelineno-57-6></a> {&#39;end&#39;: 40, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.9761149883270264, &#39;start&#39;: 35, &#39;word&#39;: &#39;##gging&#39;},
</span><span id=__span-57-7><a id=__codelineno-57-7 name=__codelineno-57-7 href=#__codelineno-57-7></a> {&#39;end&#39;: 45, &#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 14, &#39;score&#39;: 0.9887974858283997, &#39;start&#39;: 41, &#39;word&#39;: &#39;Face&#39;},
</span><span id=__span-57-8><a id=__codelineno-57-8 name=__codelineno-57-8 href=#__codelineno-57-8></a> {&#39;end&#39;: 57, &#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 16, &#39;score&#39;: 0.99321049451828, &#39;start&#39;: 49, &#39;word&#39;: &#39;Brooklyn&#39;}]
</span></code></pre></div></p> <h4 id=_12>エンティティのグループ化<a class=headerlink href=#_12 title="Permanent link">&para;</a></h4> <p>オフセットを使用して各エンティティのstartキーとendキーを決定するのは便利ですが、その情報は厳密には必要ありません。しかし、エンティティをグループ化したい場合、オフセットは多くの煩雑なコードを節約してくれます。例えば、トークン<code>Hu</code>、<code>##gging</code>、<code>Face</code>をグループ化したい場合、最初の2つは<code>##</code>を削除して添付し、<code>Face</code>は<code>##</code>で始まらないのでスペースで追加するという特別なルールを作成できます。しかし、これは特定のタイプのトークナイザーでのみ動作します。SentencePieceやByte-Pair-Encodingトークナイザー（この章で後述）には別のルールセットを書く必要があります。</p> <p>オフセットがあれば、すべてのカスタムコードが不要になります：最初のトークンで始まり最後のトークンで終わる元のテキストのスパンを取るだけです。トークン<code>Hu</code>、<code>##gging</code>、<code>Face</code>の場合、文字33（<code>Hu</code>の開始）で開始し、文字45（<code>Face</code>の終了）の前で終了する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=n>example</span><span class=p>[</span><span class=mi>33</span><span class=p>:</span><span class=mi>45</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a>&#39;Hugging Face&#39;
</span></code></pre></div></p> <p>エンティティをグループ化しながら予測を後処理するコードを書くために、<code>I-XXX</code>でラベル付けされた連続するエンティティをグループ化します（最初のものは<code>B-XXX</code>または<code>I-XXX</code>でラベル付けできます）。つまり、<code>O</code>、新しいタイプのエンティティ、または同じタイプのエンティティが開始することを伝える<code>B-XXX</code>を取得した場合、エンティティのグループ化を停止します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a><span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-60-4><a id=__codelineno-60-4 name=__codelineno-60-4 href=#__codelineno-60-4></a><span class=n>inputs_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>example</span><span class=p>,</span> <span class=n>return_offsets_mapping</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-60-5><a id=__codelineno-60-5 name=__codelineno-60-5 href=#__codelineno-60-5></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>inputs_with_offsets</span><span class=o>.</span><span class=n>tokens</span><span class=p>()</span>
</span><span id=__span-60-6><a id=__codelineno-60-6 name=__codelineno-60-6 href=#__codelineno-60-6></a><span class=n>offsets</span> <span class=o>=</span> <span class=n>inputs_with_offsets</span><span class=p>[</span><span class=s2>&quot;offset_mapping&quot;</span><span class=p>]</span>
</span><span id=__span-60-7><a id=__codelineno-60-7 name=__codelineno-60-7 href=#__codelineno-60-7></a>
</span><span id=__span-60-8><a id=__codelineno-60-8 name=__codelineno-60-8 href=#__codelineno-60-8></a><span class=n>idx</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-60-9><a id=__codelineno-60-9 name=__codelineno-60-9 href=#__codelineno-60-9></a><span class=k>while</span> <span class=n>idx</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>):</span>
</span><span id=__span-60-10><a id=__codelineno-60-10 name=__codelineno-60-10 href=#__codelineno-60-10></a>    <span class=n>pred</span> <span class=o>=</span> <span class=n>predictions</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span><span id=__span-60-11><a id=__codelineno-60-11 name=__codelineno-60-11 href=#__codelineno-60-11></a>    <span class=n>label</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span><span class=p>[</span><span class=n>pred</span><span class=p>]</span>
</span><span id=__span-60-12><a id=__codelineno-60-12 name=__codelineno-60-12 href=#__codelineno-60-12></a>    <span class=k>if</span> <span class=n>label</span> <span class=o>!=</span> <span class=s2>&quot;O&quot;</span><span class=p>:</span>
</span><span id=__span-60-13><a id=__codelineno-60-13 name=__codelineno-60-13 href=#__codelineno-60-13></a>        <span class=c1># B-またはI-を削除</span>
</span><span id=__span-60-14><a id=__codelineno-60-14 name=__codelineno-60-14 href=#__codelineno-60-14></a>        <span class=n>label</span> <span class=o>=</span> <span class=n>label</span><span class=p>[</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-60-15><a id=__codelineno-60-15 name=__codelineno-60-15 href=#__codelineno-60-15></a>        <span class=n>start</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>offsets</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span><span id=__span-60-16><a id=__codelineno-60-16 name=__codelineno-60-16 href=#__codelineno-60-16></a>
</span><span id=__span-60-17><a id=__codelineno-60-17 name=__codelineno-60-17 href=#__codelineno-60-17></a>        <span class=c1># I-labelでラベル付けされたすべてのトークンを取得</span>
</span><span id=__span-60-18><a id=__codelineno-60-18 name=__codelineno-60-18 href=#__codelineno-60-18></a>        <span class=n>all_scores</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-60-19><a id=__codelineno-60-19 name=__codelineno-60-19 href=#__codelineno-60-19></a>        <span class=k>while</span> <span class=p>(</span>
</span><span id=__span-60-20><a id=__codelineno-60-20 name=__codelineno-60-20 href=#__codelineno-60-20></a>            <span class=n>idx</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span><span id=__span-60-21><a id=__codelineno-60-21 name=__codelineno-60-21 href=#__codelineno-60-21></a>            <span class=ow>and</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>id2label</span><span class=p>[</span><span class=n>predictions</span><span class=p>[</span><span class=n>idx</span><span class=p>]]</span> <span class=o>==</span> <span class=sa>f</span><span class=s2>&quot;I-</span><span class=si>{</span><span class=n>label</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-60-22><a id=__codelineno-60-22 name=__codelineno-60-22 href=#__codelineno-60-22></a>        <span class=p>):</span>
</span><span id=__span-60-23><a id=__codelineno-60-23 name=__codelineno-60-23 href=#__codelineno-60-23></a>            <span class=n>all_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>probabilities</span><span class=p>[</span><span class=n>idx</span><span class=p>][</span><span class=n>pred</span><span class=p>])</span>
</span><span id=__span-60-24><a id=__codelineno-60-24 name=__codelineno-60-24 href=#__codelineno-60-24></a>            <span class=n>_</span><span class=p>,</span> <span class=n>end</span> <span class=o>=</span> <span class=n>offsets</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span><span id=__span-60-25><a id=__codelineno-60-25 name=__codelineno-60-25 href=#__codelineno-60-25></a>            <span class=n>idx</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-60-26><a id=__codelineno-60-26 name=__codelineno-60-26 href=#__codelineno-60-26></a>
</span><span id=__span-60-27><a id=__codelineno-60-27 name=__codelineno-60-27 href=#__codelineno-60-27></a>        <span class=c1># スコアはそのグループ化されたエンティティ内のすべてのトークンのスコアの平均</span>
</span><span id=__span-60-28><a id=__codelineno-60-28 name=__codelineno-60-28 href=#__codelineno-60-28></a>        <span class=n>score</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>all_scores</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span><span id=__span-60-29><a id=__codelineno-60-29 name=__codelineno-60-29 href=#__codelineno-60-29></a>        <span class=n>word</span> <span class=o>=</span> <span class=n>example</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>]</span>
</span><span id=__span-60-30><a id=__codelineno-60-30 name=__codelineno-60-30 href=#__codelineno-60-30></a>        <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span><span id=__span-60-31><a id=__codelineno-60-31 name=__codelineno-60-31 href=#__codelineno-60-31></a>            <span class=p>{</span>
</span><span id=__span-60-32><a id=__codelineno-60-32 name=__codelineno-60-32 href=#__codelineno-60-32></a>                <span class=s2>&quot;entity_group&quot;</span><span class=p>:</span> <span class=n>label</span><span class=p>,</span>
</span><span id=__span-60-33><a id=__codelineno-60-33 name=__codelineno-60-33 href=#__codelineno-60-33></a>                <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=n>score</span><span class=p>,</span>
</span><span id=__span-60-34><a id=__codelineno-60-34 name=__codelineno-60-34 href=#__codelineno-60-34></a>                <span class=s2>&quot;word&quot;</span><span class=p>:</span> <span class=n>word</span><span class=p>,</span>
</span><span id=__span-60-35><a id=__codelineno-60-35 name=__codelineno-60-35 href=#__codelineno-60-35></a>                <span class=s2>&quot;start&quot;</span><span class=p>:</span> <span class=n>start</span><span class=p>,</span>
</span><span id=__span-60-36><a id=__codelineno-60-36 name=__codelineno-60-36 href=#__codelineno-60-36></a>                <span class=s2>&quot;end&quot;</span><span class=p>:</span> <span class=n>end</span><span class=p>,</span>
</span><span id=__span-60-37><a id=__codelineno-60-37 name=__codelineno-60-37 href=#__codelineno-60-37></a>            <span class=p>}</span>
</span><span id=__span-60-38><a id=__codelineno-60-38 name=__codelineno-60-38 href=#__codelineno-60-38></a>        <span class=p>)</span>
</span><span id=__span-60-39><a id=__codelineno-60-39 name=__codelineno-60-39 href=#__codelineno-60-39></a>    <span class=n>idx</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-60-40><a id=__codelineno-60-40 name=__codelineno-60-40 href=#__codelineno-60-40></a>
</span><span id=__span-60-41><a id=__codelineno-60-41 name=__codelineno-60-41 href=#__codelineno-60-41></a><span class=n>pprint</span><span class=p>(</span><span class=n>results</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a>[{&#39;end&#39;: 18, &#39;entity_group&#39;: &#39;PER&#39;, &#39;score&#39;: 0.998169407248497, &#39;start&#39;: 11, &#39;word&#39;: &#39;Sylvain&#39;},
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a> {&#39;end&#39;: 45, &#39;entity_group&#39;: &#39;ORG&#39;, &#39;score&#39;: 0.9796018203099569, &#39;start&#39;: 33, &#39;word&#39;: &#39;Hugging Face&#39;},
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a> {&#39;end&#39;: 57, &#39;entity_group&#39;: &#39;LOC&#39;, &#39;score&#39;: 0.99321049451828, &#39;start&#39;: 49, &#39;word&#39;: &#39;Brooklyn&#39;}]
</span></code></pre></div></p> <p>これらのオフセットが非常に有用なタスクの別の例は質問応答です。次のセクションで飛び込むそのパイプラインにより、入力を特定の長さに切り捨てる際にオーバーフローするトークンを処理する方法について、Hugging Face Transformersライブラリのトークナイザーの最後の機能の1つも見ることができます。</p> <h2 id=_13>正規化と前処理<a class=headerlink href=#_13 title="Permanent link">&para;</a></h2> <p>Transformerモデルで使用される3つの最も一般的なサブワード・トークン化アルゴリズム（Byte-Pair Encoding [BPE]、WordPiece、Unigram）についてより深く掘り下げる前に、各トークナイザーがテキストに適用する前処理について最初に見てみましょう。トークン化パイプラインのステップの概要は次のとおりです：</p> <p><img alt=トークン化パイプライン src=../04_the_huggingface_tokenizers_library_files/tokenization_pipeline.png></p> <p>テキストをサブトークン（そのモデルに従って）に分割する前に、トークナイザーは2つのステップを実行します：<strong>正規化</strong>と<strong>前処理</strong>。</p> <h3 id=_14>正規化<a class=headerlink href=#_14 title="Permanent link">&para;</a></h3> <p>正規化ステップには、不要な空白の除去、小文字化、アクセント記号の除去などの一般的なクリーンアップが含まれます。<a href=http://www.unicode.org/reports/tr15/ >Unicode正規化</a>（NFCやNFKCなど）に精通している場合、これもトークナイザーが適用する可能性があるものです。</p> <p>Hugging Face Transformersの<code>tokenizer</code>には<code>backend_tokenizer</code>という属性があり、Hugging Face Tokenizersライブラリからの基礎となるトークナイザーにアクセスを提供します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-62-2><a id=__codelineno-62-2 name=__codelineno-62-2 href=#__codelineno-62-2></a>
</span><span id=__span-62-3><a id=__codelineno-62-3 name=__codelineno-62-3 href=#__codelineno-62-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-uncased&quot;</span><span class=p>)</span>
</span><span id=__span-62-4><a id=__codelineno-62-4 name=__codelineno-62-4 href=#__codelineno-62-4></a><span class=nb>print</span><span class=p>(</span><span class=nb>type</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>&lt;class &#39;tokenizers.Tokenizer&#39;&gt;
</span></code></pre></div></p> <p><code>tokenizer</code>オブジェクトの<code>normalizer</code>属性には、正規化がどのように実行されるかを確認するために使用できる<code>normalize_str()</code>メソッドがあります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>normalizer</span><span class=o>.</span><span class=n>normalize_str</span><span class=p>(</span><span class=s2>&quot;Héllò hôw are ü?&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>hello how are u?
</span></code></pre></div></p> <p>この例では、<code>bert-base-uncased</code>チェックポイントを選択したため、適用された正規化は小文字化とアクセント記号の除去でした。</p> <div class="admonition question"> <p class=admonition-title>試してみましょう！</p> <p><code>bert-base-cased</code>チェックポイントからトークナイザーを読み込み、同じ例を渡してください。casedバージョンとuncasedバージョンのトークナイザーの主な違いは何でしょうか？</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span><span id=__span-66-2><a id=__codelineno-66-2 name=__codelineno-66-2 href=#__codelineno-66-2></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>normalizer</span><span class=o>.</span><span class=n>normalize_str</span><span class=p>(</span><span class=s2>&quot;Héllò hôw are ü?&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a>Héllò hôw are ü?
</span></code></pre></div></p> <h3 id=_15>前処理<a class=headerlink href=#_15 title="Permanent link">&para;</a></h3> <p>次のセクションで見るように、トークナイザーは生のテキストだけでは訓練できません。代わりに、最初にテキストを単語のような小さなエンティティに分割する必要があります。そこで前処理ステップが登場します。第2章で見たように、単語ベースのトークナイザーは、空白と句読点で生のテキストを単語に分割するだけです。これらの単語は、トークナイザーが訓練中に学習できるサブトークンの境界になります。</p> <p>高速トークナイザーがどのように前処理を実行するかを確認するために、<code>tokenizer</code>オブジェクトの<code>pre_tokenizer</code>属性の<code>pre_tokenize_str()</code>メソッドを使用できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-68-2><a id=__codelineno-68-2 name=__codelineno-68-2 href=#__codelineno-68-2></a>
</span><span id=__span-68-3><a id=__codelineno-68-3 name=__codelineno-68-3 href=#__codelineno-68-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-uncased&quot;</span><span class=p>)</span>
</span><span id=__span-68-4><a id=__codelineno-68-4 name=__codelineno-68-4 href=#__codelineno-68-4></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Hello, how are you?&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a>[(&#39;Hello&#39;, (0, 5)),
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a> (&#39;,&#39;, (5, 6)),
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a> (&#39;how&#39;, (7, 10)),
</span><span id=__span-69-4><a id=__codelineno-69-4 name=__codelineno-69-4 href=#__codelineno-69-4></a> (&#39;are&#39;, (11, 14)),
</span><span id=__span-69-5><a id=__codelineno-69-5 name=__codelineno-69-5 href=#__codelineno-69-5></a> (&#39;you&#39;, (15, 18)),
</span><span id=__span-69-6><a id=__codelineno-69-6 name=__codelineno-69-6 href=#__codelineno-69-6></a> (&#39;?&#39;, (18, 19))]
</span></code></pre></div></p> <p>トークナイザーが既にオフセットを追跡していることに注目してください。これが前のセクションで使用したオフセットマッピングを提供できる理由です。ここで、トークナイザーは2つのスペースを無視して1つだけで置き換えますが、オフセットは<code>are</code>と<code>you</code>の間でそれを考慮してジャンプします。</p> <p>BERTトークナイザーを使用しているため、前処理には空白と句読点での分割が含まれます。他のトークナイザーはこのステップで異なるルールを持つ可能性があります。例えば、GPT-2トークナイザーを使用する場合：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;gpt2&quot;</span><span class=p>)</span>
</span><span id=__span-70-2><a id=__codelineno-70-2 name=__codelineno-70-2 href=#__codelineno-70-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Hello, how are  you?&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a>[(&#39;Hello&#39;, (0, 5)),
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a> (&#39;,&#39;, (5, 6)),
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a> (&#39;Ġ how&#39;, (6, 10)),
</span><span id=__span-71-4><a id=__codelineno-71-4 name=__codelineno-71-4 href=#__codelineno-71-4></a> (&#39;Ġ are&#39;, (10, 14)),
</span><span id=__span-71-5><a id=__codelineno-71-5 name=__codelineno-71-5 href=#__codelineno-71-5></a> (&#39;Ġ &#39;, (14, 15)),
</span><span id=__span-71-6><a id=__codelineno-71-6 name=__codelineno-71-6 href=#__codelineno-71-6></a> (&#39;Ġ you&#39;, (15, 19)),
</span><span id=__span-71-7><a id=__codelineno-71-7 name=__codelineno-71-7 href=#__codelineno-71-7></a> (&#39;?&#39;, (19, 20))]
</span></code></pre></div></p> <p>これも空白と句読点で分割しますが、スペースを保持して<code>Ġ</code>記号に置き換えるため、トークンをデコードする際に元のスペースを復元できます。</p> <p>また、BERTトークナイザーとは異なり、このトークナイザーは二重スペースを無視しないことにも注目してください。</p> <p>最後の例として、SentencePieceアルゴリズムに基づくT5トークナイザーを見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;t5-small&quot;</span><span class=p>)</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Hello, how are  you?&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a>[(&#39;▁Hello,&#39;, (0, 6)),
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a> (&#39;▁how&#39;, (7, 10)),
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a> (&#39;▁are&#39;, (11, 14)),
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a> (&#39;▁you?&#39;, (16, 20))]
</span></code></pre></div></p> <p>GPT-2トークナイザーと同様に、これはスペースを保持し、特定のトークン（<code>_</code>）に置き換えますが、T5トークナイザーは句読点ではなく空白でのみ分割します。また、デフォルトで文の最初（<code>Hello</code>の前）にスペースを追加し、<code>are</code>と<code>you</code>の間の二重スペースを無視したことにも注目してください。</p> <p>いくつかの異なるトークナイザーがテキストを処理する方法を少し見たので、基礎となるアルゴリズム自体の探索を開始できます。広く適用可能なSentencePieceを簡単に見てから、次の3つのセクションで、サブワード・トークン化に使用される3つの主要アルゴリズムの動作を検証します。</p> <h3 id=sentencepiece>SentencePiece<a class=headerlink href=#sentencepiece title="Permanent link">&para;</a></h3> <p><a href=https://github.com/google/sentencepiece>SentencePiece</a>は、次の3つのセクションで見るモデルのいずれかと組み合わせて使用できるテキストの前処理用のトークン化アルゴリズムです。テキストをUnicode文字のシーケンスとして考慮し、スペースを特別な文字<code>▁</code>に置き換えます。Unigramアルゴリズムと組み合わせて使用すると、前処理ステップも必要ありません。これはスペース文字が使用されていない言語（中国語や日本語など）に非常に役立ちます。</p> <p>SentencePieceのもう1つの主要な機能は<strong>可逆的トークン化</strong>です：スペースの特別な処理がないため、トークンのデコードは単純にそれらを連結し、<code>_</code>をスペースに置き換えることで行われます。これにより、正規化されたテキストが得られます。前述のとおり、BERTトークナイザーは重複するスペースを除去するため、そのトークン化は可逆的ではありません。</p> <h3 id=_16>アルゴリズムの概要<a class=headerlink href=#_16 title="Permanent link">&para;</a></h3> <p>以下のセクションでは、3つの主要なサブワード・トークン化アルゴリズムについて詳しく説明します：BPE（GPT-2などで使用）、WordPiece（例えばBERTで使用）、Unigram（T5などで使用）。始める前に、それぞれがどのように動作するかの簡単な概要を示します。次の各セクションを読んだ後、この表が理解できない場合は、遠慮なく戻って確認してください。</p> <table> <thead> <tr> <th style="text-align: center;">モデル</th> <th style="text-align: center;">BPE</th> <th style="text-align: center;">WordPiece</th> <th style="text-align: center;">Unigram</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">訓練</td> <td style="text-align: center;">小さな語彙から開始してトークンをマージするルールを学習</td> <td style="text-align: center;">小さな語彙から開始してトークンをマージするルールを学習</td> <td style="text-align: center;">大きな語彙から開始してトークンを削除するルールを学習</td> </tr> <tr> <td style="text-align: center;">訓練ステップ</td> <td style="text-align: center;">最も一般的なペアに対応するトークンをマージ</td> <td style="text-align: center;">ペアの頻度に基づく最高スコアを持つペアに対応するトークンをマージし、各個別トークンがより頻度が低いペアを優先</td> <td style="text-align: center;">全コーパスで計算された損失を最小化するすべてのトークンを語彙から除去</td> </tr> <tr> <td style="text-align: center;">学習内容</td> <td style="text-align: center;">マージルールと語彙</td> <td style="text-align: center;">語彙のみ</td> <td style="text-align: center;">各トークンのスコア付き語彙</td> </tr> <tr> <td style="text-align: center;">エンコーディング</td> <td style="text-align: center;">単語を文字に分割し、訓練中に学習されたマージを適用</td> <td style="text-align: center;">語彙内の最初から始まる最長のサブワードを見つけ、単語の残りの部分に対して同じことを行う</td> <td style="text-align: center;">訓練中に学習されたスコアを使用して、最も可能性の高いトークン分割を見つける</td> </tr> </tbody> </table> <h2 id=byte-pair-encoding>Byte-Pair Encodingトークン化<a class=headerlink href=#byte-pair-encoding title="Permanent link">&para;</a></h2> <p>Byte-Pair Encoding（BPE）は元々テキストを圧縮するアルゴリズムとして開発され、その後OpenAIがGPTモデルを事前学習する際にトークン化に使用しました。GPT、GPT-2、RoBERTa、BART、DeBERTaなど、多くのTransformerモデルで使用されています。</p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>このセクションではBPEを詳細に説明し、完全な実装を示すところまで行います。一般的なトークン化アルゴリズムの概要だけが欲しい場合は、最後まで飛ばしても構いません。</p> </div> <h3 id=_17>訓練アルゴリズム<a class=headerlink href=#_17 title="Permanent link">&para;</a></h3> <p>BPE訓練は、（正規化と前処理ステップが完了した後）コーパスで使用される単語の一意のセットを計算し、これらの単語を書くために使用されるすべてのシンボルを取ることで語彙を構築することから始まります。非常に簡単な例として、コーパスで以下の5つの単語が使用されているとします：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a>&quot;hug&quot;, &quot;pug&quot;, &quot;pun&quot;, &quot;bun&quot;, &quot;hugs&quot;
</span></code></pre></div> <p>基本語彙は<code>["b", "g", "h", "n", "p", "s", "u"]</code>になります。実世界の場合、その基本語彙には少なくともすべてのASCII文字が含まれ、おそらく一部のUnicode文字も含まれるでしょう。トークン化している例が訓練コーパスにない文字を使用している場合、その文字は未知トークンに変換されます。これが、多くのNLPモデルが絵文字を含むコンテンツの分析で非常に悪い性能を示す理由の1つです。</p> <div class="admonition tip"> <p class=admonition-title>GPT-2とRoBERTaの工夫</p> <p>GPT-2とRoBERTaトークナイザー（かなり似ている）には、これに対処する巧妙な方法があります：Unicode文字で書かれているとして単語を見るのではなく、バイトで見ます。このように、基本語彙のサイズは小さく（256）、考えられるすべての文字が含まれ、未知トークンに変換されることはありません。この技巧は<strong>バイトレベルBPE</strong>と呼ばれます。</p> </div> <p>この基本語彙を取得した後、既存の語彙の2つの要素を一緒にマージして新しいものにする<strong>マージ</strong>（ルール）を学習することで、望ましい語彙サイズに達するまで新しいトークンを追加します。そのため、最初はこれらのマージは2文字のトークンを作成し、その後、訓練が進むにつれて、より長いサブワードを作成します。</p> <p>トークナイザー訓練中の任意のステップで、BPEアルゴリズムは既存トークンの最も頻繁なペア（ここでの「ペア」は単語内の2つの連続するトークンを意味します）を検索します。その最も頻繁なペアがマージされ、次のステップのためにすすぎと繰り返しを行います。</p> <p>前の例に戻って、単語が以下の頻度を持っていたとします：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
</span></code></pre></div> <p>つまり、<code>"hug"</code>がコーパス内に10回、<code>"pug"</code>が5回、<code>"pun"</code>が12回、<code>"bun"</code>が4回、<code>"hugs"</code>が5回存在したということです。訓練を開始するため、各単語を文字に分割し（初期語彙を形成するもの）、各単語をトークンのリストとして見ることができます：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a>(&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)
</span></code></pre></div> <p>次に、ペアを見ます。ペア<code>("h", "u")</code>は単語<code>"hug"</code>と<code>"hugs"</code>に存在するので、コーパス内で合計15回存在します。しかし、最も頻繁なペアではありません：その栄誉は<code>("u", "g")</code>に属し、<code>"hug"</code>、<code>"pug"</code>、<code>"hugs"</code>に存在し、語彙内で合計20回存在します。</p> <p>したがって、トークナイザーが学習する最初のマージルールは<code>("u", "g") -&gt; "ug"</code>で、<code>"ug"</code>が語彙に追加され、ペアはコーパスのすべての単語でマージされる必要があります。このステージの終わりに、語彙とコーパスは次のようになります：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a>Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]
</span><span id=__span-77-2><a id=__codelineno-77-2 name=__codelineno-77-2 href=#__codelineno-77-2></a>Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)
</span></code></pre></div> <p>これで、2文字よりも長いトークンを生成するペアがいくつかあります：例えば、ペア<code>("h", "ug")</code>（コーパス内で15回存在）。しかし、このステージで最も頻繁なペアは<code>("u", "n")</code>で、コーパス内で16回存在するため、学習される2番目のマージルールは<code>("u", "n") -&gt; "un"</code>です。これを語彙に追加し、すべての既存の出現をマージすると：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-78-1><a id=__codelineno-78-1 name=__codelineno-78-1 href=#__codelineno-78-1></a>Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;]
</span><span id=__span-78-2><a id=__codelineno-78-2 name=__codelineno-78-2 href=#__codelineno-78-2></a>Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)
</span></code></pre></div> <p>今、最も頻繁なペアは<code>("h", "ug")</code>なので、マージルール<code>("h", "ug") -&gt; "hug"</code>を学習し、最初の3文字トークンを取得します。マージ後、コーパスは次のようになります：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-79-1><a id=__codelineno-79-1 name=__codelineno-79-1 href=#__codelineno-79-1></a>Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]
</span><span id=__span-79-2><a id=__codelineno-79-2 name=__codelineno-79-2 href=#__codelineno-79-2></a>Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)
</span></code></pre></div> <p>そして、望ましい語彙サイズに達するまでこのように続けます。</p> <div class="admonition question"> <p class=admonition-title>今度はあなたの番！</p> <p>次のマージルールは何だと思いますか？</p> <p>答え： <div class="language-text highlight"><pre><span></span><code><span id=__span-80-1><a id=__codelineno-80-1 name=__codelineno-80-1 href=#__codelineno-80-1></a>Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;, &quot;pun&quot;]
</span><span id=__span-80-2><a id=__codelineno-80-2 name=__codelineno-80-2 href=#__codelineno-80-2></a>Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;pun&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)
</span></code></pre></div></p> </div> <h3 id=_18>トークン化アルゴリズム<a class=headerlink href=#_18 title="Permanent link">&para;</a></h3> <p>トークン化は訓練プロセスに密接に従います。新しい入力は以下のステップを適用してトークン化されます：</p> <ol> <li>正規化</li> <li>前処理</li> <li>単語を個別の文字に分割</li> <li>学習されたマージルールを順番に分割に適用</li> </ol> <p>訓練中に使用した例を使用し、学習した3つのマージルールを使用しましょう：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-81-1><a id=__codelineno-81-1 name=__codelineno-81-1 href=#__codelineno-81-1></a>(&quot;u&quot;, &quot;g&quot;) -&gt; &quot;ug&quot;
</span><span id=__span-81-2><a id=__codelineno-81-2 name=__codelineno-81-2 href=#__codelineno-81-2></a>(&quot;u&quot;, &quot;n&quot;) -&gt; &quot;un&quot;
</span><span id=__span-81-3><a id=__codelineno-81-3 name=__codelineno-81-3 href=#__codelineno-81-3></a>(&quot;h&quot;, &quot;ug&quot;) -&gt; &quot;hug&quot;
</span></code></pre></div> <p>単語<code>"bug"</code>は<code>["b", "ug"]</code>としてトークン化されます。しかし、<code>"mug"</code>は<code>["[UNK]", "ug"]</code>としてトークン化されます。文字<code>"m"</code>が基本語彙になかったからです。同様に、単語<code>"thug"</code>は<code>["[UNK]", "hug"]</code>としてトークン化されます：文字<code>"t"</code>が基本語彙になく、マージルールを適用すると最初に<code>"u"</code>と<code>"g"</code>がマージされ、次に<code>"h"</code>と<code>"ug"</code>がマージされます。</p> <div class="admonition question"> <p class=admonition-title>今度はあなたの番！</p> <p>単語<code>"unhug"</code>はどのようにトークン化されると思いますか？</p> <p>答え：<code>["un", "hug"]</code></p> </div> <h3 id=bpe>BPEの実装<a class=headerlink href=#bpe title="Permanent link">&para;</a></h3> <p>BPEアルゴリズムの実装を見てみましょう。これは大きなコーパスで実際に使用できる最適化されたバージョンではありません。アルゴリズムをもう少しよく理解できるようにコードを示すだけです。</p> <p>最初にコーパスが必要なので、いくつかの文で簡単なものを作成しましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-82-1><a id=__codelineno-82-1 name=__codelineno-82-1 href=#__codelineno-82-1></a><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-82-2><a id=__codelineno-82-2 name=__codelineno-82-2 href=#__codelineno-82-2></a>    <span class=s2>&quot;This is the Hugging Face Course.&quot;</span><span class=p>,</span>
</span><span id=__span-82-3><a id=__codelineno-82-3 name=__codelineno-82-3 href=#__codelineno-82-3></a>    <span class=s2>&quot;This chapter is about tokenization.&quot;</span><span class=p>,</span>
</span><span id=__span-82-4><a id=__codelineno-82-4 name=__codelineno-82-4 href=#__codelineno-82-4></a>    <span class=s2>&quot;This section shows several tokenizer algorithms.&quot;</span><span class=p>,</span>
</span><span id=__span-82-5><a id=__codelineno-82-5 name=__codelineno-82-5 href=#__codelineno-82-5></a>    <span class=s2>&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span><span class=p>,</span>
</span><span id=__span-82-6><a id=__codelineno-82-6 name=__codelineno-82-6 href=#__codelineno-82-6></a><span class=p>]</span>
</span></code></pre></div> <p>次に、そのコーパスを単語に前処理する必要があります。BPEトークナイザー（GPT-2など）を複製しているので、前処理に<code>gpt2</code>トークナイザーを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-83-1><a id=__codelineno-83-1 name=__codelineno-83-1 href=#__codelineno-83-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-83-2><a id=__codelineno-83-2 name=__codelineno-83-2 href=#__codelineno-83-2></a>
</span><span id=__span-83-3><a id=__codelineno-83-3 name=__codelineno-83-3 href=#__codelineno-83-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;gpt2&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>次に、前処理を行う際にコーパス内の各単語の頻度を計算します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-84-1><a id=__codelineno-84-1 name=__codelineno-84-1 href=#__codelineno-84-1></a><span class=kn>from</span><span class=w> </span><span class=nn>collections</span><span class=w> </span><span class=kn>import</span> <span class=n>defaultdict</span>
</span><span id=__span-84-2><a id=__codelineno-84-2 name=__codelineno-84-2 href=#__codelineno-84-2></a>
</span><span id=__span-84-3><a id=__codelineno-84-3 name=__codelineno-84-3 href=#__codelineno-84-3></a><span class=n>word_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-84-4><a id=__codelineno-84-4 name=__codelineno-84-4 href=#__codelineno-84-4></a>
</span><span id=__span-84-5><a id=__codelineno-84-5 name=__codelineno-84-5 href=#__codelineno-84-5></a><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>:</span>
</span><span id=__span-84-6><a id=__codelineno-84-6 name=__codelineno-84-6 href=#__codelineno-84-6></a>    <span class=n>words_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-84-7><a id=__codelineno-84-7 name=__codelineno-84-7 href=#__codelineno-84-7></a>    <span class=n>new_words</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>words_with_offsets</span><span class=p>]</span>
</span><span id=__span-84-8><a id=__codelineno-84-8 name=__codelineno-84-8 href=#__codelineno-84-8></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>new_words</span><span class=p>:</span>
</span><span id=__span-84-9><a id=__codelineno-84-9 name=__codelineno-84-9 href=#__codelineno-84-9></a>        <span class=n>word_freqs</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-84-10><a id=__codelineno-84-10 name=__codelineno-84-10 href=#__codelineno-84-10></a>
</span><span id=__span-84-11><a id=__codelineno-84-11 name=__codelineno-84-11 href=#__codelineno-84-11></a><span class=nb>print</span><span class=p>(</span><span class=n>word_freqs</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-85-1><a id=__codelineno-85-1 name=__codelineno-85-1 href=#__codelineno-85-1></a>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;This&#39;: 3, &#39;Ġ is&#39;: 2, &#39;Ġ the&#39;: 1, &#39;Ġ Hugging&#39;: 1, &#39;Ġ Face&#39;: 1, &#39;Ġ Course&#39;: 1, &#39;.&#39;: 4, &#39;Ġ chapter&#39;: 1, &#39;Ġ about&#39;: 1, &#39;Ġ tokenization&#39;: 1, &#39;Ġ section&#39;: 1, &#39;Ġ shows&#39;: 1, &#39;Ġ several&#39;: 1, &#39;Ġ tokenizer&#39;: 1, &#39;Ġ algorithms&#39;: 1, &#39;Hopefully&#39;: 1, &#39;,&#39;: 1, &#39;Ġ you&#39;: 1, &#39;Ġ will&#39;: 1, &#39;Ġ be&#39;: 1, &#39;Ġ able&#39;: 1, &#39;Ġ to&#39;: 1, &#39;Ġ understand&#39;: 1, &#39;Ġ how&#39;: 1, &#39;Ġ they&#39;: 1, &#39;Ġ are&#39;: 1, &#39;Ġ trained&#39;: 1, &#39;Ġ and&#39;: 1, &#39;Ġ generate&#39;: 1, &#39;Ġ tokens&#39;: 1})
</span></code></pre></div></p> <p>次のステップは、コーパスで使用されるすべての文字で形成される基本語彙を計算することです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-86-1><a id=__codelineno-86-1 name=__codelineno-86-1 href=#__codelineno-86-1></a><span class=n>alphabet</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-86-2><a id=__codelineno-86-2 name=__codelineno-86-2 href=#__codelineno-86-2></a>
</span><span id=__span-86-3><a id=__codelineno-86-3 name=__codelineno-86-3 href=#__codelineno-86-3></a><span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span><span id=__span-86-4><a id=__codelineno-86-4 name=__codelineno-86-4 href=#__codelineno-86-4></a>    <span class=k>for</span> <span class=n>letter</span> <span class=ow>in</span> <span class=n>word</span><span class=p>:</span>
</span><span id=__span-86-5><a id=__codelineno-86-5 name=__codelineno-86-5 href=#__codelineno-86-5></a>        <span class=k>if</span> <span class=n>letter</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>alphabet</span><span class=p>:</span>
</span><span id=__span-86-6><a id=__codelineno-86-6 name=__codelineno-86-6 href=#__codelineno-86-6></a>            <span class=n>alphabet</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>letter</span><span class=p>)</span>
</span><span id=__span-86-7><a id=__codelineno-86-7 name=__codelineno-86-7 href=#__codelineno-86-7></a><span class=n>alphabet</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</span><span id=__span-86-8><a id=__codelineno-86-8 name=__codelineno-86-8 href=#__codelineno-86-8></a>
</span><span id=__span-86-9><a id=__codelineno-86-9 name=__codelineno-86-9 href=#__codelineno-86-9></a><span class=nb>print</span><span class=p>(</span><span class=n>alphabet</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-87-1><a id=__codelineno-87-1 name=__codelineno-87-1 href=#__codelineno-87-1></a>[&#39;,&#39;, &#39;.&#39;, &#39;C&#39;, &#39;F&#39;, &#39;H&#39;, &#39;T&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;y&#39;, &#39;z&#39;, &#39;Ġ &#39;]
</span></code></pre></div></p> <p>また、その語彙の最初にモデルで使用される特別なトークンも追加します。GPT-2の場合、唯一の特別なトークンは<code>"&lt;|endoftext|&gt;"</code>です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-88-1><a id=__codelineno-88-1 name=__codelineno-88-1 href=#__codelineno-88-1></a><span class=n>vocab</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;&lt;|endoftext|&gt;&quot;</span><span class=p>]</span> <span class=o>+</span> <span class=n>alphabet</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></code></pre></div> <p>次に、訓練を開始できるように、各単語を個別の文字に分割する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-89-1><a id=__codelineno-89-1 name=__codelineno-89-1 href=#__codelineno-89-1></a><span class=n>splits</span> <span class=o>=</span> <span class=p>{</span><span class=n>word</span><span class=p>:</span> <span class=p>[</span><span class=n>c</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>word</span><span class=p>]</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>keys</span><span class=p>()}</span>
</span></code></pre></div> <p>訓練の準備が整ったので、各ペアの頻度を計算する関数を書きましょう。訓練の各ステップでこれを使用する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-90-1><a id=__codelineno-90-1 name=__codelineno-90-1 href=#__codelineno-90-1></a><span class=k>def</span><span class=w> </span><span class=nf>compute_pair_freqs</span><span class=p>(</span><span class=n>splits</span><span class=p>):</span>
</span><span id=__span-90-2><a id=__codelineno-90-2 name=__codelineno-90-2 href=#__codelineno-90-2></a>    <span class=n>pair_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-90-3><a id=__codelineno-90-3 name=__codelineno-90-3 href=#__codelineno-90-3></a>    <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-90-4><a id=__codelineno-90-4 name=__codelineno-90-4 href=#__codelineno-90-4></a>        <span class=n>split</span> <span class=o>=</span> <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span>
</span><span id=__span-90-5><a id=__codelineno-90-5 name=__codelineno-90-5 href=#__codelineno-90-5></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-90-6><a id=__codelineno-90-6 name=__codelineno-90-6 href=#__codelineno-90-6></a>            <span class=k>continue</span>
</span><span id=__span-90-7><a id=__codelineno-90-7 name=__codelineno-90-7 href=#__codelineno-90-7></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>):</span>
</span><span id=__span-90-8><a id=__codelineno-90-8 name=__codelineno-90-8 href=#__codelineno-90-8></a>            <span class=n>pair</span> <span class=o>=</span> <span class=p>(</span><span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>])</span>
</span><span id=__span-90-9><a id=__codelineno-90-9 name=__codelineno-90-9 href=#__codelineno-90-9></a>            <span class=n>pair_freqs</span><span class=p>[</span><span class=n>pair</span><span class=p>]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-90-10><a id=__codelineno-90-10 name=__codelineno-90-10 href=#__codelineno-90-10></a>    <span class=k>return</span> <span class=n>pair_freqs</span>
</span></code></pre></div> <p>初期分割後、この辞書の一部を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-91-1><a id=__codelineno-91-1 name=__codelineno-91-1 href=#__codelineno-91-1></a><span class=n>pair_freqs</span> <span class=o>=</span> <span class=n>compute_pair_freqs</span><span class=p>(</span><span class=n>splits</span><span class=p>)</span>
</span><span id=__span-91-2><a id=__codelineno-91-2 name=__codelineno-91-2 href=#__codelineno-91-2></a>
</span><span id=__span-91-3><a id=__codelineno-91-3 name=__codelineno-91-3 href=#__codelineno-91-3></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>key</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>pair_freqs</span><span class=o>.</span><span class=n>keys</span><span class=p>()):</span>
</span><span id=__span-91-4><a id=__codelineno-91-4 name=__codelineno-91-4 href=#__codelineno-91-4></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>pair_freqs</span><span class=p>[</span><span class=n>key</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-91-5><a id=__codelineno-91-5 name=__codelineno-91-5 href=#__codelineno-91-5></a>    <span class=k>if</span> <span class=n>i</span> <span class=o>&gt;=</span> <span class=mi>5</span><span class=p>:</span>
</span><span id=__span-91-6><a id=__codelineno-91-6 name=__codelineno-91-6 href=#__codelineno-91-6></a>        <span class=k>break</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-92-1><a id=__codelineno-92-1 name=__codelineno-92-1 href=#__codelineno-92-1></a>(&#39;T&#39;, &#39;h&#39;): 3
</span><span id=__span-92-2><a id=__codelineno-92-2 name=__codelineno-92-2 href=#__codelineno-92-2></a>(&#39;h&#39;, &#39;i&#39;): 3
</span><span id=__span-92-3><a id=__codelineno-92-3 name=__codelineno-92-3 href=#__codelineno-92-3></a>(&#39;i&#39;, &#39;s&#39;): 5
</span><span id=__span-92-4><a id=__codelineno-92-4 name=__codelineno-92-4 href=#__codelineno-92-4></a>(&#39;Ġ &#39;, &#39;i&#39;): 2
</span><span id=__span-92-5><a id=__codelineno-92-5 name=__codelineno-92-5 href=#__codelineno-92-5></a>(&#39;Ġ &#39;, &#39;t&#39;): 7
</span><span id=__span-92-6><a id=__codelineno-92-6 name=__codelineno-92-6 href=#__codelineno-92-6></a>(&#39;t&#39;, &#39;h&#39;): 3
</span></code></pre></div></p> <p>今、最も頻繁なペアを見つけるには、簡単なループが必要です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-93-1><a id=__codelineno-93-1 name=__codelineno-93-1 href=#__codelineno-93-1></a><span class=n>best_pair</span> <span class=o>=</span> <span class=s2>&quot;&quot;</span>
</span><span id=__span-93-2><a id=__codelineno-93-2 name=__codelineno-93-2 href=#__codelineno-93-2></a><span class=n>max_freq</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-93-3><a id=__codelineno-93-3 name=__codelineno-93-3 href=#__codelineno-93-3></a>
</span><span id=__span-93-4><a id=__codelineno-93-4 name=__codelineno-93-4 href=#__codelineno-93-4></a><span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>pair_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-93-5><a id=__codelineno-93-5 name=__codelineno-93-5 href=#__codelineno-93-5></a>    <span class=k>if</span> <span class=n>max_freq</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>max_freq</span> <span class=o>&lt;</span> <span class=n>freq</span><span class=p>:</span>
</span><span id=__span-93-6><a id=__codelineno-93-6 name=__codelineno-93-6 href=#__codelineno-93-6></a>        <span class=n>best_pair</span> <span class=o>=</span> <span class=n>pair</span>
</span><span id=__span-93-7><a id=__codelineno-93-7 name=__codelineno-93-7 href=#__codelineno-93-7></a>        <span class=n>max_freq</span> <span class=o>=</span> <span class=n>freq</span>
</span><span id=__span-93-8><a id=__codelineno-93-8 name=__codelineno-93-8 href=#__codelineno-93-8></a>
</span><span id=__span-93-9><a id=__codelineno-93-9 name=__codelineno-93-9 href=#__codelineno-93-9></a><span class=nb>print</span><span class=p>(</span><span class=n>best_pair</span><span class=p>,</span> <span class=n>max_freq</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-94-1><a id=__codelineno-94-1 name=__codelineno-94-1 href=#__codelineno-94-1></a>(&#39;Ġ &#39;, &#39;t&#39;) 7
</span></code></pre></div></p> <p>最初に学習するマージは<code>('Ġ ', 't') -&gt; 'Ġ t'</code>で、語彙に<code>'Ġ t'</code>を追加します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-95-1><a id=__codelineno-95-1 name=__codelineno-95-1 href=#__codelineno-95-1></a><span class=n>merges</span> <span class=o>=</span> <span class=p>{(</span><span class=s2>&quot;Ġ &quot;</span><span class=p>,</span> <span class=s2>&quot;t&quot;</span><span class=p>):</span> <span class=s2>&quot;Ġ t&quot;</span><span class=p>}</span>
</span><span id=__span-95-2><a id=__codelineno-95-2 name=__codelineno-95-2 href=#__codelineno-95-2></a><span class=n>vocab</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&quot;Ġ t&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>続行するには、<code>splits</code>辞書にそのマージを適用する必要があります。別の関数を書きましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-96-1><a id=__codelineno-96-1 name=__codelineno-96-1 href=#__codelineno-96-1></a><span class=k>def</span><span class=w> </span><span class=nf>merge_pair</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>splits</span><span class=p>):</span>
</span><span id=__span-96-2><a id=__codelineno-96-2 name=__codelineno-96-2 href=#__codelineno-96-2></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=p>:</span>
</span><span id=__span-96-3><a id=__codelineno-96-3 name=__codelineno-96-3 href=#__codelineno-96-3></a>        <span class=n>split</span> <span class=o>=</span> <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span>
</span><span id=__span-96-4><a id=__codelineno-96-4 name=__codelineno-96-4 href=#__codelineno-96-4></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-96-5><a id=__codelineno-96-5 name=__codelineno-96-5 href=#__codelineno-96-5></a>            <span class=k>continue</span>
</span><span id=__span-96-6><a id=__codelineno-96-6 name=__codelineno-96-6 href=#__codelineno-96-6></a>
</span><span id=__span-96-7><a id=__codelineno-96-7 name=__codelineno-96-7 href=#__codelineno-96-7></a>        <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-96-8><a id=__codelineno-96-8 name=__codelineno-96-8 href=#__codelineno-96-8></a>        <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-96-9><a id=__codelineno-96-9 name=__codelineno-96-9 href=#__codelineno-96-9></a>            <span class=k>if</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>a</span> <span class=ow>and</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>b</span><span class=p>:</span>
</span><span id=__span-96-10><a id=__codelineno-96-10 name=__codelineno-96-10 href=#__codelineno-96-10></a>                <span class=n>split</span> <span class=o>=</span> <span class=n>split</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>]</span> <span class=o>+</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>2</span> <span class=p>:]</span>
</span><span id=__span-96-11><a id=__codelineno-96-11 name=__codelineno-96-11 href=#__codelineno-96-11></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-96-12><a id=__codelineno-96-12 name=__codelineno-96-12 href=#__codelineno-96-12></a>                <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-96-13><a id=__codelineno-96-13 name=__codelineno-96-13 href=#__codelineno-96-13></a>        <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>=</span> <span class=n>split</span>
</span><span id=__span-96-14><a id=__codelineno-96-14 name=__codelineno-96-14 href=#__codelineno-96-14></a>    <span class=k>return</span> <span class=n>splits</span>
</span></code></pre></div> <p>最初のマージの結果を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-97-1><a id=__codelineno-97-1 name=__codelineno-97-1 href=#__codelineno-97-1></a><span class=n>splits</span> <span class=o>=</span> <span class=n>merge_pair</span><span class=p>(</span><span class=s2>&quot;Ġ &quot;</span><span class=p>,</span> <span class=s2>&quot;t&quot;</span><span class=p>,</span> <span class=n>splits</span><span class=p>)</span>
</span><span id=__span-97-2><a id=__codelineno-97-2 name=__codelineno-97-2 href=#__codelineno-97-2></a><span class=nb>print</span><span class=p>(</span><span class=n>splits</span><span class=p>[</span><span class=s2>&quot;Ġ trained&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-98-1><a id=__codelineno-98-1 name=__codelineno-98-1 href=#__codelineno-98-1></a>[&#39;Ġ t&#39;, &#39;r&#39;, &#39;a&#39;, &#39;i&#39;, &#39;n&#39;, &#39;e&#39;, &#39;d&#39;]
</span></code></pre></div></p> <p>すべてを学習したいマージまでループするために必要なすべてがあります。語彙サイズを50にしましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-99-1><a id=__codelineno-99-1 name=__codelineno-99-1 href=#__codelineno-99-1></a><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>50</span>
</span><span id=__span-99-2><a id=__codelineno-99-2 name=__codelineno-99-2 href=#__codelineno-99-2></a>
</span><span id=__span-99-3><a id=__codelineno-99-3 name=__codelineno-99-3 href=#__codelineno-99-3></a><span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>vocab_size</span><span class=p>:</span>
</span><span id=__span-99-4><a id=__codelineno-99-4 name=__codelineno-99-4 href=#__codelineno-99-4></a>    <span class=n>pair_freqs</span> <span class=o>=</span> <span class=n>compute_pair_freqs</span><span class=p>(</span><span class=n>splits</span><span class=p>)</span>
</span><span id=__span-99-5><a id=__codelineno-99-5 name=__codelineno-99-5 href=#__codelineno-99-5></a>    <span class=n>best_pair</span> <span class=o>=</span> <span class=s2>&quot;&quot;</span>
</span><span id=__span-99-6><a id=__codelineno-99-6 name=__codelineno-99-6 href=#__codelineno-99-6></a>    <span class=n>max_freq</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-99-7><a id=__codelineno-99-7 name=__codelineno-99-7 href=#__codelineno-99-7></a>    <span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>pair_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-99-8><a id=__codelineno-99-8 name=__codelineno-99-8 href=#__codelineno-99-8></a>        <span class=k>if</span> <span class=n>max_freq</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>max_freq</span> <span class=o>&lt;</span> <span class=n>freq</span><span class=p>:</span>
</span><span id=__span-99-9><a id=__codelineno-99-9 name=__codelineno-99-9 href=#__codelineno-99-9></a>            <span class=n>best_pair</span> <span class=o>=</span> <span class=n>pair</span>
</span><span id=__span-99-10><a id=__codelineno-99-10 name=__codelineno-99-10 href=#__codelineno-99-10></a>            <span class=n>max_freq</span> <span class=o>=</span> <span class=n>freq</span>
</span><span id=__span-99-11><a id=__codelineno-99-11 name=__codelineno-99-11 href=#__codelineno-99-11></a>    <span class=n>splits</span> <span class=o>=</span> <span class=n>merge_pair</span><span class=p>(</span><span class=o>*</span><span class=n>best_pair</span><span class=p>,</span> <span class=n>splits</span><span class=p>)</span>
</span><span id=__span-99-12><a id=__codelineno-99-12 name=__codelineno-99-12 href=#__codelineno-99-12></a>    <span class=n>merges</span><span class=p>[</span><span class=n>best_pair</span><span class=p>]</span> <span class=o>=</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-99-13><a id=__codelineno-99-13 name=__codelineno-99-13 href=#__codelineno-99-13></a>    <span class=n>vocab</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>best_pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></code></pre></div> <p>結果として、19のマージルールを学習しました（初期語彙のサイズは31 - アルファベットの30文字＋特別なトークン）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-100-1><a id=__codelineno-100-1 name=__codelineno-100-1 href=#__codelineno-100-1></a><span class=nb>print</span><span class=p>(</span><span class=n>merges</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-101-1><a id=__codelineno-101-1 name=__codelineno-101-1 href=#__codelineno-101-1></a>{(&#39;Ġ &#39;, &#39;t&#39;): &#39;Ġ t&#39;, (&#39;i&#39;, &#39;s&#39;): &#39;is&#39;, (&#39;e&#39;, &#39;r&#39;): &#39;er&#39;, (&#39;Ġ &#39;, &#39;a&#39;): &#39;Ġ a&#39;, (&#39;Ġ t&#39;, &#39;o&#39;): &#39;Ġ to&#39;, (&#39;e&#39;, &#39;n&#39;): &#39;en&#39;, (&#39;T&#39;, &#39;h&#39;): &#39;Th&#39;, (&#39;Th&#39;, &#39;is&#39;): &#39;This&#39;, (&#39;o&#39;, &#39;u&#39;): &#39;ou&#39;, (&#39;s&#39;, &#39;e&#39;): &#39;se&#39;, (&#39;Ġ to&#39;, &#39;k&#39;): &#39;Ġ tok&#39;, (&#39;Ġ tok&#39;, &#39;en&#39;): &#39;Ġ token&#39;, (&#39;n&#39;, &#39;d&#39;): &#39;nd&#39;, (&#39;Ġ &#39;, &#39;is&#39;): &#39;Ġ is&#39;, (&#39;Ġ t&#39;, &#39;h&#39;): &#39;Ġ th&#39;, (&#39;Ġ th&#39;, &#39;e&#39;): &#39;Ġ the&#39;, (&#39;i&#39;, &#39;n&#39;): &#39;in&#39;, (&#39;Ġ a&#39;, &#39;b&#39;): &#39;Ġ ab&#39;, (&#39;Ġ token&#39;, &#39;i&#39;): &#39;Ġ tokeni&#39;}
</span></code></pre></div></p> <p>語彙は特別なトークン、初期アルファベット、すべてのマージの結果で構成されています：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-102-1><a id=__codelineno-102-1 name=__codelineno-102-1 href=#__codelineno-102-1></a><span class=nb>print</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-103-1><a id=__codelineno-103-1 name=__codelineno-103-1 href=#__codelineno-103-1></a>[&#39;&lt;|endoftext|&gt;&#39;, &#39;,&#39;, &#39;.&#39;, &#39;C&#39;, &#39;F&#39;, &#39;H&#39;, &#39;T&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;y&#39;, &#39;z&#39;, &#39;Ġ &#39;, &#39;Ġ t&#39;, &#39;is&#39;, &#39;er&#39;, &#39;Ġ a&#39;, &#39;Ġ to&#39;, &#39;en&#39;, &#39;Th&#39;, &#39;This&#39;, &#39;ou&#39;, &#39;se&#39;, &#39;Ġ tok&#39;, &#39;Ġ token&#39;, &#39;nd&#39;, &#39;Ġ is&#39;, &#39;Ġ th&#39;, &#39;Ġ the&#39;, &#39;in&#39;, &#39;Ġ ab&#39;, &#39;Ġ tokeni&#39;]
</span></code></pre></div></p> <div class="admonition tip"> <p class=admonition-title>同一コーパスでも異なる結果</p> <p>同じコーパスで<code>train_new_from_iterator()</code>を使用しても、まったく同じ語彙にはなりません。これは、最も頻繁なペアの選択がある場合に、最初に遭遇したものを選択したのに対し、Hugging Face Tokenizersライブラリは内部IDに基づいて最初のものを選択するためです。</p> </div> <p>新しいテキストをトークン化するために、前処理を行い、分割してから、学習したすべてのマージルールを適用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-104-1><a id=__codelineno-104-1 name=__codelineno-104-1 href=#__codelineno-104-1></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span><span id=__span-104-2><a id=__codelineno-104-2 name=__codelineno-104-2 href=#__codelineno-104-2></a>    <span class=n>pre_tokenize_result</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-104-3><a id=__codelineno-104-3 name=__codelineno-104-3 href=#__codelineno-104-3></a>    <span class=n>pre_tokenized_text</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>pre_tokenize_result</span><span class=p>]</span>
</span><span id=__span-104-4><a id=__codelineno-104-4 name=__codelineno-104-4 href=#__codelineno-104-4></a>    <span class=n>splits</span> <span class=o>=</span> <span class=p>[[</span><span class=n>l</span> <span class=k>for</span> <span class=n>l</span> <span class=ow>in</span> <span class=n>word</span><span class=p>]</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>pre_tokenized_text</span><span class=p>]</span>
</span><span id=__span-104-5><a id=__codelineno-104-5 name=__codelineno-104-5 href=#__codelineno-104-5></a>    <span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>merge</span> <span class=ow>in</span> <span class=n>merges</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-104-6><a id=__codelineno-104-6 name=__codelineno-104-6 href=#__codelineno-104-6></a>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>split</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>splits</span><span class=p>):</span>
</span><span id=__span-104-7><a id=__codelineno-104-7 name=__codelineno-104-7 href=#__codelineno-104-7></a>            <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-104-8><a id=__codelineno-104-8 name=__codelineno-104-8 href=#__codelineno-104-8></a>            <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-104-9><a id=__codelineno-104-9 name=__codelineno-104-9 href=#__codelineno-104-9></a>                <span class=k>if</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=ow>and</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>pair</span><span class=p>[</span><span class=mi>1</span><span class=p>]:</span>
</span><span id=__span-104-10><a id=__codelineno-104-10 name=__codelineno-104-10 href=#__codelineno-104-10></a>                    <span class=n>split</span> <span class=o>=</span> <span class=n>split</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=n>merge</span><span class=p>]</span> <span class=o>+</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>2</span> <span class=p>:]</span>
</span><span id=__span-104-11><a id=__codelineno-104-11 name=__codelineno-104-11 href=#__codelineno-104-11></a>                <span class=k>else</span><span class=p>:</span>
</span><span id=__span-104-12><a id=__codelineno-104-12 name=__codelineno-104-12 href=#__codelineno-104-12></a>                    <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-104-13><a id=__codelineno-104-13 name=__codelineno-104-13 href=#__codelineno-104-13></a>            <span class=n>splits</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>split</span>
</span><span id=__span-104-14><a id=__codelineno-104-14 name=__codelineno-104-14 href=#__codelineno-104-14></a>
</span><span id=__span-104-15><a id=__codelineno-104-15 name=__codelineno-104-15 href=#__codelineno-104-15></a>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>splits</span><span class=p>,</span> <span class=p>[])</span>
</span></code></pre></div> <p>アルファベット内の文字で構成される任意のテキストでこれを試すことができます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-105-1><a id=__codelineno-105-1 name=__codelineno-105-1 href=#__codelineno-105-1></a><span class=n>tokenize</span><span class=p>(</span><span class=s2>&quot;This is not a token.&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-106-1><a id=__codelineno-106-1 name=__codelineno-106-1 href=#__codelineno-106-1></a>[&#39;This&#39;, &#39;Ġ is&#39;, &#39;Ġ &#39;, &#39;n&#39;, &#39;o&#39;, &#39;t&#39;, &#39;Ġ a&#39;, &#39;Ġ token&#39;, &#39;.&#39;]
</span></code></pre></div></p> <div class="admonition warning"> <p class=admonition-title>未知文字への対処</p> <p>未知の文字がある場合、実装はエラーをスローします。何も処理していないためです。GPT-2には実際には未知トークンがありません（バイトレベルBPEを使用する場合、未知文字を取得することは不可能です）が、可能なすべてのバイトを初期語彙に含めていないため、ここで発生する可能性があります。BPEのこの側面はこのセクションの範囲を超えているため、詳細は省略しています。</p> </div> <h2 id=wordpiece>WordPieceトークン化<a class=headerlink href=#wordpiece title="Permanent link">&para;</a></h2> <p>WordPieceは、GoogleがBERTを事前学習するために開発したトークン化アルゴリズムです。その後、DistilBERT、MobileBERT、Funnel Transformers、MPNETなど、BERTベースの多くのTransformerモデルで再使用されています。訓練の観点ではBPEと非常に似ていますが、実際のトークン化は異なって行われます。</p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>このセクションではWordPieceを詳細に説明し、完全な実装を示すところまで行います。一般的なトークン化アルゴリズムの概要だけが欲しい場合は、最後まで飛ばしても構いません。</p> </div> <h3 id=_19>訓練アルゴリズム<a class=headerlink href=#_19 title="Permanent link">&para;</a></h3> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>GoogleはWordPieceの訓練アルゴリズムの実装をオープンソース化したことがないため、以下は公開された文献に基づく最善の推測です。100％正確ではない可能性があります。</p> </div> <p>BPEと同様に、WordPieceは、モデルで使用される特別なトークンと初期アルファベットを含む小さな語彙から開始します。プレフィックス（BERTの<code>##</code>など）を追加してサブワードを識別するため、各単語は最初にその単語内のすべての文字にプレフィックスを追加することで分割されます。例えば、<code>"word"</code>は次のように分割されます：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-107-1><a id=__codelineno-107-1 name=__codelineno-107-1 href=#__codelineno-107-1></a>w ##o ##r ##d
</span></code></pre></div> <p>したがって、初期アルファベットには、単語の最初に存在するすべての文字と、WordPieceプレフィックスが前に付いた単語内に存在する文字が含まれます。</p> <p>次に、BPEと同様に、WordPieceはマージルールを学習します。主な違いは、マージするペアの選択方法です。最も頻繁なペアを選択する代わりに、WordPieceは次の式を使用して各ペアのスコアを計算します：</p> <p><span class=arithmatex>\(\mathrm{score} = \frac{\mathrm{freq\_of\_pair}}{\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element}}\)</span></p> <p>ペアの頻度をその各部分の頻度の積で割ることにより、アルゴリズムは個々の部分が語彙でより頻度が低いペアのマージを優先します。例えば、そのペアがコーパス内で非常に頻繁に出現しても、<code>"un"</code>と<code>"##able"</code>の2つのペアはそれぞれ他の多くの単語に出現し、高い頻度を持つ可能性が高いため、必ずしも<code>("un", "##able")</code>をマージするわけではありません。対照的に、<code>("hu", "##gging")</code>のようなペアは（単語「hugging」がコーパス内に頻繁に出現すると仮定して）<code>"hu"</code>と<code>"##gging"</code>は個々により頻度が低い可能性が高いため、より高速にマージされるでしょう。</p> <p>BPE訓練例で使用したのと同じ語彙を見てみましょう：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-108-1><a id=__codelineno-108-1 name=__codelineno-108-1 href=#__codelineno-108-1></a>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
</span></code></pre></div> <p>ここでの分割は：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-109-1><a id=__codelineno-109-1 name=__codelineno-109-1 href=#__codelineno-109-1></a>(&quot;h&quot; &quot;##u&quot; &quot;##g&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;h&quot; &quot;##u&quot; &quot;##g&quot; &quot;##s&quot;, 5)
</span></code></pre></div> <p>つまり、初期語彙は<code>["b", "h", "p", "##g", "##n", "##s", "##u"]</code>になります（今のところ特別なトークンを忘れるとして）。最も頻繁なペアは<code>("##u", "##g")</code>（20回存在）ですが、<code>"##u"</code>の個々の頻度が非常に高いため、そのスコアは最高ではありません（1/36）。<code>"##u"</code>を含むすべてのペアは実際に同じスコア（1/36）を持つため、最高スコアは<code>"##u"</code>なしの唯一のペア<code>("##g", "##s")</code>の1/20になり、最初に学習されるマージは<code>("##g", "##s") -&gt; ("##gs")</code>です。</p> <p>マージするとき、2つのトークン間の<code>##</code>を削除するので、語彙に<code>"##gs"</code>を追加し、コーパスの単語にマージを適用します：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-110-1><a id=__codelineno-110-1 name=__codelineno-110-1 href=#__codelineno-110-1></a>Vocabulary: [&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;, &quot;##gs&quot;]
</span><span id=__span-110-2><a id=__codelineno-110-2 name=__codelineno-110-2 href=#__codelineno-110-2></a>Corpus: (&quot;h&quot; &quot;##u&quot; &quot;##g&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;h&quot; &quot;##u&quot; &quot;##gs&quot;, 5)
</span></code></pre></div> <p>この時点で、<code>"##u"</code>はすべての可能なペアに含まれるため、すべて同じスコアになります。この場合、最初のペアがマージされるとしましょう。そのため、<code>("h", "##u") -&gt; "hu"</code>です。これにより：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-111-1><a id=__codelineno-111-1 name=__codelineno-111-1 href=#__codelineno-111-1></a>Vocabulary: [&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;, &quot;##gs&quot;, &quot;hu&quot;]
</span><span id=__span-111-2><a id=__codelineno-111-2 name=__codelineno-111-2 href=#__codelineno-111-2></a>Corpus: (&quot;hu&quot; &quot;##g&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;hu&quot; &quot;##gs&quot;, 5)
</span></code></pre></div> <p>次に最高スコアは<code>("hu", "##g")</code>と<code>("hu", "##gs")</code>で共有されます（他のすべてのペアの1/21と比較して1/15）。そのため、最大スコアを持つ最初のペアがマージされます：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-112-1><a id=__codelineno-112-1 name=__codelineno-112-1 href=#__codelineno-112-1></a>Vocabulary: [&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;, &quot;##gs&quot;, &quot;hu&quot;, &quot;hug&quot;]
</span><span id=__span-112-2><a id=__codelineno-112-2 name=__codelineno-112-2 href=#__codelineno-112-2></a>Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;hu&quot; &quot;##gs&quot;, 5)
</span></code></pre></div> <p>そして、望ましい語彙サイズに達するまでこのように続けます。</p> <h3 id=_20>トークン化アルゴリズム<a class=headerlink href=#_20 title="Permanent link">&para;</a></h3> <p>WordPieceでのトークン化はBPEと異なり、WordPieceは最終的な語彙のみを保存し、学習されたマージルールは保存しません。トークン化する単語から開始して、WordPieceは語彙内にある最長のサブワードを見つけ、それで分割します。例えば、上記の例で学習した語彙を使用する場合、単語<code>"hugs"</code>について、最初から始まる語彙内の最長サブワードは<code>"hug"</code>なので、そこで分割して<code>["hug", "##s"]</code>を取得します。次に<code>"##s"</code>を続行します。これは語彙内にあるので、<code>"hugs"</code>のトークン化は<code>["hug", "##s"]</code>です。</p> <p>BPEでは、学習した順番でマージを適用して<code>["hu", "##gs"]</code>としてトークン化するので、エンコーディングが異なります。</p> <p>別の例として、単語<code>"bugs"</code>がどのようにトークン化されるかを見てみましょう。<code>"b"</code>は語彙内にある単語の最初から始まる最長サブワードなので、そこで分割して<code>["b", "##ugs"]</code>を取得します。次に<code>"##u"</code>は語彙内にある<code>"##ugs"</code>の最初から始まる最長サブワードなので、そこで分割して<code>["b", "##u", "##gs"]</code>を取得します。最後に、<code>"##gs"</code>は語彙内にあるので、この最後のリストが<code>"bugs"</code>のトークン化です。</p> <p>トークン化が語彙内のサブワードを見つけることができないステージに到達すると、単語全体が未知としてトークン化されます。そのため、例えば<code>"mug"</code>は<code>["[UNK]"]</code>としてトークン化され、<code>"bum"</code>も同様です（<code>"b"</code>と<code>"##u"</code>で開始できても、<code>"##m"</code>は語彙にないため、結果のトークン化は<code>["b", "##u", "[UNK]"]</code>ではなく、単に<code>["[UNK]"]</code>になります）。これはBPEとの別の違いで、BPEは語彙にない個々の文字のみを未知として分類します。</p> <div class="admonition question"> <p class=admonition-title>今度はあなたの番！</p> <p>単語<code>"pugs"</code>はどのようにトークン化されるでしょうか？</p> <p>答え：<code>["p", "##u", "##gs"]</code></p> </div> <h3 id=wordpiece_1>WordPieceの実装<a class=headerlink href=#wordpiece_1 title="Permanent link">&para;</a></h3> <p>WordPieceアルゴリズムの実装を見てみましょう。BPEと同様に、これは教育的なもので、大きなコーパスでは使用できません。</p> <p>BPEの例と同じコーパスを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-113-1><a id=__codelineno-113-1 name=__codelineno-113-1 href=#__codelineno-113-1></a><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-113-2><a id=__codelineno-113-2 name=__codelineno-113-2 href=#__codelineno-113-2></a>    <span class=s2>&quot;This is the Hugging Face Course.&quot;</span><span class=p>,</span>
</span><span id=__span-113-3><a id=__codelineno-113-3 name=__codelineno-113-3 href=#__codelineno-113-3></a>    <span class=s2>&quot;This chapter is about tokenization.&quot;</span><span class=p>,</span>
</span><span id=__span-113-4><a id=__codelineno-113-4 name=__codelineno-113-4 href=#__codelineno-113-4></a>    <span class=s2>&quot;This section shows several tokenizer algorithms.&quot;</span><span class=p>,</span>
</span><span id=__span-113-5><a id=__codelineno-113-5 name=__codelineno-113-5 href=#__codelineno-113-5></a>    <span class=s2>&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span><span class=p>,</span>
</span><span id=__span-113-6><a id=__codelineno-113-6 name=__codelineno-113-6 href=#__codelineno-113-6></a><span class=p>]</span>
</span></code></pre></div> <p>まず、WordPieceトークナイザー（BERTなど）を複製しているので、前処理に<code>bert-base-cased</code>トークナイザーを使用する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-114-1><a id=__codelineno-114-1 name=__codelineno-114-1 href=#__codelineno-114-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-114-2><a id=__codelineno-114-2 name=__codelineno-114-2 href=#__codelineno-114-2></a>
</span><span id=__span-114-3><a id=__codelineno-114-3 name=__codelineno-114-3 href=#__codelineno-114-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;bert-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>次に、前処理を行う際にコーパス内の各単語の頻度を計算します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-115-1><a id=__codelineno-115-1 name=__codelineno-115-1 href=#__codelineno-115-1></a><span class=kn>from</span><span class=w> </span><span class=nn>collections</span><span class=w> </span><span class=kn>import</span> <span class=n>defaultdict</span>
</span><span id=__span-115-2><a id=__codelineno-115-2 name=__codelineno-115-2 href=#__codelineno-115-2></a>
</span><span id=__span-115-3><a id=__codelineno-115-3 name=__codelineno-115-3 href=#__codelineno-115-3></a><span class=n>word_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-115-4><a id=__codelineno-115-4 name=__codelineno-115-4 href=#__codelineno-115-4></a><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>:</span>
</span><span id=__span-115-5><a id=__codelineno-115-5 name=__codelineno-115-5 href=#__codelineno-115-5></a>    <span class=n>words_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-115-6><a id=__codelineno-115-6 name=__codelineno-115-6 href=#__codelineno-115-6></a>    <span class=n>new_words</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>words_with_offsets</span><span class=p>]</span>
</span><span id=__span-115-7><a id=__codelineno-115-7 name=__codelineno-115-7 href=#__codelineno-115-7></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>new_words</span><span class=p>:</span>
</span><span id=__span-115-8><a id=__codelineno-115-8 name=__codelineno-115-8 href=#__codelineno-115-8></a>        <span class=n>word_freqs</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-115-9><a id=__codelineno-115-9 name=__codelineno-115-9 href=#__codelineno-115-9></a>
</span><span id=__span-115-10><a id=__codelineno-115-10 name=__codelineno-115-10 href=#__codelineno-115-10></a><span class=nb>print</span><span class=p>(</span><span class=n>word_freqs</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-116-1><a id=__codelineno-116-1 name=__codelineno-116-1 href=#__codelineno-116-1></a>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;This&#39;: 3, &#39;is&#39;: 2, &#39;the&#39;: 1, &#39;Hugging&#39;: 1, &#39;Face&#39;: 1, &#39;Course&#39;: 1, &#39;.&#39;: 4, &#39;chapter&#39;: 1, &#39;about&#39;: 1, &#39;tokenization&#39;: 1, &#39;section&#39;: 1, &#39;shows&#39;: 1, &#39;several&#39;: 1, &#39;tokenizer&#39;: 1, &#39;algorithms&#39;: 1, &#39;Hopefully&#39;: 1, &#39;,&#39;: 1, &#39;you&#39;: 1, &#39;will&#39;: 1, &#39;be&#39;: 1, &#39;able&#39;: 1, &#39;to&#39;: 1, &#39;understand&#39;: 1, &#39;how&#39;: 1, &#39;they&#39;: 1, &#39;are&#39;: 1, &#39;trained&#39;: 1, &#39;and&#39;: 1, &#39;generate&#39;: 1, &#39;tokens&#39;: 1})
</span></code></pre></div></p> <p>前述のとおり、アルファベットは単語のすべての最初の文字と、<code>##</code>プレフィックスが付いた単語に現れるすべての他の文字で構成される一意のセットです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-117-1><a id=__codelineno-117-1 name=__codelineno-117-1 href=#__codelineno-117-1></a><span class=n>alphabet</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-117-2><a id=__codelineno-117-2 name=__codelineno-117-2 href=#__codelineno-117-2></a><span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span><span id=__span-117-3><a id=__codelineno-117-3 name=__codelineno-117-3 href=#__codelineno-117-3></a>    <span class=k>if</span> <span class=n>word</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>alphabet</span><span class=p>:</span>
</span><span id=__span-117-4><a id=__codelineno-117-4 name=__codelineno-117-4 href=#__codelineno-117-4></a>        <span class=n>alphabet</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>word</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span><span id=__span-117-5><a id=__codelineno-117-5 name=__codelineno-117-5 href=#__codelineno-117-5></a>    <span class=k>for</span> <span class=n>letter</span> <span class=ow>in</span> <span class=n>word</span><span class=p>[</span><span class=mi>1</span><span class=p>:]:</span>
</span><span id=__span-117-6><a id=__codelineno-117-6 name=__codelineno-117-6 href=#__codelineno-117-6></a>        <span class=k>if</span> <span class=sa>f</span><span class=s2>&quot;##</span><span class=si>{</span><span class=n>letter</span><span class=si>}</span><span class=s2>&quot;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>alphabet</span><span class=p>:</span>
</span><span id=__span-117-7><a id=__codelineno-117-7 name=__codelineno-117-7 href=#__codelineno-117-7></a>            <span class=n>alphabet</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;##</span><span class=si>{</span><span class=n>letter</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-117-8><a id=__codelineno-117-8 name=__codelineno-117-8 href=#__codelineno-117-8></a>
</span><span id=__span-117-9><a id=__codelineno-117-9 name=__codelineno-117-9 href=#__codelineno-117-9></a><span class=n>alphabet</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</span><span id=__span-117-10><a id=__codelineno-117-10 name=__codelineno-117-10 href=#__codelineno-117-10></a><span class=nb>print</span><span class=p>(</span><span class=n>alphabet</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-118-1><a id=__codelineno-118-1 name=__codelineno-118-1 href=#__codelineno-118-1></a>[&#39;##a&#39;, &#39;##b&#39;, &#39;##c&#39;, &#39;##d&#39;, &#39;##e&#39;, &#39;##f&#39;, &#39;##g&#39;, &#39;##h&#39;, &#39;##i&#39;, &#39;##k&#39;, &#39;##l&#39;, &#39;##m&#39;, &#39;##n&#39;, &#39;##o&#39;, &#39;##p&#39;, &#39;##r&#39;, &#39;##s&#39;, &#39;##t&#39;, &#39;##u&#39;, &#39;##v&#39;, &#39;##w&#39;, &#39;##y&#39;, &#39;##z&#39;, &#39;,&#39;, &#39;.&#39;, &#39;C&#39;, &#39;F&#39;, &#39;H&#39;, &#39;T&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;w&#39;, &#39;y&#39;]
</span></code></pre></div></p> <p>また、その語彙の最初にモデルで使用される特別なトークンも追加します。BERTの場合、リストは<code>["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]</code>です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-119-1><a id=__codelineno-119-1 name=__codelineno-119-1 href=#__codelineno-119-1></a><span class=n>vocab</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;[PAD]&quot;</span><span class=p>,</span> <span class=s2>&quot;[UNK]&quot;</span><span class=p>,</span> <span class=s2>&quot;[CLS]&quot;</span><span class=p>,</span> <span class=s2>&quot;[SEP]&quot;</span><span class=p>,</span> <span class=s2>&quot;[MASK]&quot;</span><span class=p>]</span> <span class=o>+</span> <span class=n>alphabet</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></code></pre></div> <p>次に、各単語を分割する必要があります。最初の文字以外のすべての文字に<code>##</code>プレフィックスが付きます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-120-1><a id=__codelineno-120-1 name=__codelineno-120-1 href=#__codelineno-120-1></a><span class=n>splits</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-120-2><a id=__codelineno-120-2 name=__codelineno-120-2 href=#__codelineno-120-2></a>    <span class=n>word</span><span class=p>:</span> <span class=p>[</span><span class=n>c</span> <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span> <span class=k>else</span> <span class=sa>f</span><span class=s2>&quot;##</span><span class=si>{</span><span class=n>c</span><span class=si>}</span><span class=s2>&quot;</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>c</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>word</span><span class=p>)]</span>
</span><span id=__span-120-3><a id=__codelineno-120-3 name=__codelineno-120-3 href=#__codelineno-120-3></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span>
</span><span id=__span-120-4><a id=__codelineno-120-4 name=__codelineno-120-4 href=#__codelineno-120-4></a><span class=p>}</span>
</span></code></pre></div> <p>訓練の準備が整ったので、各ペアのスコアを計算する関数を書きましょう。訓練の各ステップでこれを使用する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-121-1><a id=__codelineno-121-1 name=__codelineno-121-1 href=#__codelineno-121-1></a><span class=k>def</span><span class=w> </span><span class=nf>compute_pair_scores</span><span class=p>(</span><span class=n>splits</span><span class=p>):</span>
</span><span id=__span-121-2><a id=__codelineno-121-2 name=__codelineno-121-2 href=#__codelineno-121-2></a>    <span class=n>letter_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-121-3><a id=__codelineno-121-3 name=__codelineno-121-3 href=#__codelineno-121-3></a>    <span class=n>pair_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-121-4><a id=__codelineno-121-4 name=__codelineno-121-4 href=#__codelineno-121-4></a>    <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-121-5><a id=__codelineno-121-5 name=__codelineno-121-5 href=#__codelineno-121-5></a>        <span class=n>split</span> <span class=o>=</span> <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span>
</span><span id=__span-121-6><a id=__codelineno-121-6 name=__codelineno-121-6 href=#__codelineno-121-6></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-121-7><a id=__codelineno-121-7 name=__codelineno-121-7 href=#__codelineno-121-7></a>            <span class=n>letter_freqs</span><span class=p>[</span><span class=n>split</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-121-8><a id=__codelineno-121-8 name=__codelineno-121-8 href=#__codelineno-121-8></a>            <span class=k>continue</span>
</span><span id=__span-121-9><a id=__codelineno-121-9 name=__codelineno-121-9 href=#__codelineno-121-9></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>):</span>
</span><span id=__span-121-10><a id=__codelineno-121-10 name=__codelineno-121-10 href=#__codelineno-121-10></a>            <span class=n>pair</span> <span class=o>=</span> <span class=p>(</span><span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>])</span>
</span><span id=__span-121-11><a id=__codelineno-121-11 name=__codelineno-121-11 href=#__codelineno-121-11></a>            <span class=n>letter_freqs</span><span class=p>[</span><span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-121-12><a id=__codelineno-121-12 name=__codelineno-121-12 href=#__codelineno-121-12></a>            <span class=n>pair_freqs</span><span class=p>[</span><span class=n>pair</span><span class=p>]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-121-13><a id=__codelineno-121-13 name=__codelineno-121-13 href=#__codelineno-121-13></a>        <span class=n>letter_freqs</span><span class=p>[</span><span class=n>split</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-121-14><a id=__codelineno-121-14 name=__codelineno-121-14 href=#__codelineno-121-14></a>
</span><span id=__span-121-15><a id=__codelineno-121-15 name=__codelineno-121-15 href=#__codelineno-121-15></a>    <span class=n>scores</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-121-16><a id=__codelineno-121-16 name=__codelineno-121-16 href=#__codelineno-121-16></a>        <span class=n>pair</span><span class=p>:</span> <span class=n>freq</span> <span class=o>/</span> <span class=p>(</span><span class=n>letter_freqs</span><span class=p>[</span><span class=n>pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=n>letter_freqs</span><span class=p>[</span><span class=n>pair</span><span class=p>[</span><span class=mi>1</span><span class=p>]])</span>
</span><span id=__span-121-17><a id=__codelineno-121-17 name=__codelineno-121-17 href=#__codelineno-121-17></a>        <span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>pair_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-121-18><a id=__codelineno-121-18 name=__codelineno-121-18 href=#__codelineno-121-18></a>    <span class=p>}</span>
</span><span id=__span-121-19><a id=__codelineno-121-19 name=__codelineno-121-19 href=#__codelineno-121-19></a>    <span class=k>return</span> <span class=n>scores</span>
</span></code></pre></div> <p>初期分割後、この辞書の一部を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-122-1><a id=__codelineno-122-1 name=__codelineno-122-1 href=#__codelineno-122-1></a><span class=n>pair_scores</span> <span class=o>=</span> <span class=n>compute_pair_scores</span><span class=p>(</span><span class=n>splits</span><span class=p>)</span>
</span><span id=__span-122-2><a id=__codelineno-122-2 name=__codelineno-122-2 href=#__codelineno-122-2></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>key</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>pair_scores</span><span class=o>.</span><span class=n>keys</span><span class=p>()):</span>
</span><span id=__span-122-3><a id=__codelineno-122-3 name=__codelineno-122-3 href=#__codelineno-122-3></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>pair_scores</span><span class=p>[</span><span class=n>key</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-122-4><a id=__codelineno-122-4 name=__codelineno-122-4 href=#__codelineno-122-4></a>    <span class=k>if</span> <span class=n>i</span> <span class=o>&gt;=</span> <span class=mi>5</span><span class=p>:</span>
</span><span id=__span-122-5><a id=__codelineno-122-5 name=__codelineno-122-5 href=#__codelineno-122-5></a>        <span class=k>break</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-123-1><a id=__codelineno-123-1 name=__codelineno-123-1 href=#__codelineno-123-1></a>(&#39;T&#39;, &#39;##h&#39;): 0.125
</span><span id=__span-123-2><a id=__codelineno-123-2 name=__codelineno-123-2 href=#__codelineno-123-2></a>(&#39;##h&#39;, &#39;##i&#39;): 0.03409090909090909
</span><span id=__span-123-3><a id=__codelineno-123-3 name=__codelineno-123-3 href=#__codelineno-123-3></a>(&#39;##i&#39;, &#39;##s&#39;): 0.02727272727272727
</span><span id=__span-123-4><a id=__codelineno-123-4 name=__codelineno-123-4 href=#__codelineno-123-4></a>(&#39;i&#39;, &#39;##s&#39;): 0.1
</span><span id=__span-123-5><a id=__codelineno-123-5 name=__codelineno-123-5 href=#__codelineno-123-5></a>(&#39;t&#39;, &#39;##h&#39;): 0.03571428571428571
</span><span id=__span-123-6><a id=__codelineno-123-6 name=__codelineno-123-6 href=#__codelineno-123-6></a>(&#39;##h&#39;, &#39;##e&#39;): 0.011904761904761904
</span></code></pre></div></p> <p>最高スコアを持つペアを見つけるには、簡単なループが必要です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-124-1><a id=__codelineno-124-1 name=__codelineno-124-1 href=#__codelineno-124-1></a><span class=n>best_pair</span> <span class=o>=</span> <span class=s2>&quot;&quot;</span>
</span><span id=__span-124-2><a id=__codelineno-124-2 name=__codelineno-124-2 href=#__codelineno-124-2></a><span class=n>max_score</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-124-3><a id=__codelineno-124-3 name=__codelineno-124-3 href=#__codelineno-124-3></a><span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>pair_scores</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-124-4><a id=__codelineno-124-4 name=__codelineno-124-4 href=#__codelineno-124-4></a>    <span class=k>if</span> <span class=n>max_score</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>max_score</span> <span class=o>&lt;</span> <span class=n>score</span><span class=p>:</span>
</span><span id=__span-124-5><a id=__codelineno-124-5 name=__codelineno-124-5 href=#__codelineno-124-5></a>        <span class=n>best_pair</span> <span class=o>=</span> <span class=n>pair</span>
</span><span id=__span-124-6><a id=__codelineno-124-6 name=__codelineno-124-6 href=#__codelineno-124-6></a>        <span class=n>max_score</span> <span class=o>=</span> <span class=n>score</span>
</span><span id=__span-124-7><a id=__codelineno-124-7 name=__codelineno-124-7 href=#__codelineno-124-7></a>
</span><span id=__span-124-8><a id=__codelineno-124-8 name=__codelineno-124-8 href=#__codelineno-124-8></a><span class=nb>print</span><span class=p>(</span><span class=n>best_pair</span><span class=p>,</span> <span class=n>max_score</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-125-1><a id=__codelineno-125-1 name=__codelineno-125-1 href=#__codelineno-125-1></a>(&#39;a&#39;, &#39;##b&#39;) 0.2
</span></code></pre></div></p> <p>最初に学習するマージは<code>('a', '##b') -&gt; 'ab'</code>で、語彙に<code>'ab'</code>を追加します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-126-1><a id=__codelineno-126-1 name=__codelineno-126-1 href=#__codelineno-126-1></a><span class=n>vocab</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&quot;ab&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>続行するには、<code>splits</code>辞書にそのマージを適用する必要があります。別の関数を書きましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-127-1><a id=__codelineno-127-1 name=__codelineno-127-1 href=#__codelineno-127-1></a><span class=k>def</span><span class=w> </span><span class=nf>merge_pair</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>splits</span><span class=p>):</span>
</span><span id=__span-127-2><a id=__codelineno-127-2 name=__codelineno-127-2 href=#__codelineno-127-2></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=p>:</span>
</span><span id=__span-127-3><a id=__codelineno-127-3 name=__codelineno-127-3 href=#__codelineno-127-3></a>        <span class=n>split</span> <span class=o>=</span> <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span>
</span><span id=__span-127-4><a id=__codelineno-127-4 name=__codelineno-127-4 href=#__codelineno-127-4></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-127-5><a id=__codelineno-127-5 name=__codelineno-127-5 href=#__codelineno-127-5></a>            <span class=k>continue</span>
</span><span id=__span-127-6><a id=__codelineno-127-6 name=__codelineno-127-6 href=#__codelineno-127-6></a>        <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-127-7><a id=__codelineno-127-7 name=__codelineno-127-7 href=#__codelineno-127-7></a>        <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>split</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-127-8><a id=__codelineno-127-8 name=__codelineno-127-8 href=#__codelineno-127-8></a>            <span class=k>if</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>a</span> <span class=ow>and</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>b</span><span class=p>:</span>
</span><span id=__span-127-9><a id=__codelineno-127-9 name=__codelineno-127-9 href=#__codelineno-127-9></a>                <span class=n>merge</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>[</span><span class=mi>2</span><span class=p>:]</span> <span class=k>if</span> <span class=n>b</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&quot;##&quot;</span><span class=p>)</span> <span class=k>else</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-127-10><a id=__codelineno-127-10 name=__codelineno-127-10 href=#__codelineno-127-10></a>                <span class=n>split</span> <span class=o>=</span> <span class=n>split</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=n>merge</span><span class=p>]</span> <span class=o>+</span> <span class=n>split</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>2</span> <span class=p>:]</span>
</span><span id=__span-127-11><a id=__codelineno-127-11 name=__codelineno-127-11 href=#__codelineno-127-11></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-127-12><a id=__codelineno-127-12 name=__codelineno-127-12 href=#__codelineno-127-12></a>                <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-127-13><a id=__codelineno-127-13 name=__codelineno-127-13 href=#__codelineno-127-13></a>        <span class=n>splits</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>=</span> <span class=n>split</span>
</span><span id=__span-127-14><a id=__codelineno-127-14 name=__codelineno-127-14 href=#__codelineno-127-14></a>    <span class=k>return</span> <span class=n>splits</span>
</span></code></pre></div> <p>最初のマージの結果を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-128-1><a id=__codelineno-128-1 name=__codelineno-128-1 href=#__codelineno-128-1></a><span class=n>splits</span> <span class=o>=</span> <span class=n>merge_pair</span><span class=p>(</span><span class=s2>&quot;a&quot;</span><span class=p>,</span> <span class=s2>&quot;##b&quot;</span><span class=p>,</span> <span class=n>splits</span><span class=p>)</span>
</span><span id=__span-128-2><a id=__codelineno-128-2 name=__codelineno-128-2 href=#__codelineno-128-2></a><span class=n>splits</span><span class=p>[</span><span class=s2>&quot;about&quot;</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-129-1><a id=__codelineno-129-1 name=__codelineno-129-1 href=#__codelineno-129-1></a>[&#39;ab&#39;, &#39;##o&#39;, &#39;##u&#39;, &#39;##t&#39;]
</span></code></pre></div></p> <p>すべてを学習したいマージまでループするために必要なすべてがあります。語彙サイズを70にしましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-130-1><a id=__codelineno-130-1 name=__codelineno-130-1 href=#__codelineno-130-1></a><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>70</span>
</span><span id=__span-130-2><a id=__codelineno-130-2 name=__codelineno-130-2 href=#__codelineno-130-2></a><span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>vocab_size</span><span class=p>:</span>
</span><span id=__span-130-3><a id=__codelineno-130-3 name=__codelineno-130-3 href=#__codelineno-130-3></a>    <span class=n>scores</span> <span class=o>=</span> <span class=n>compute_pair_scores</span><span class=p>(</span><span class=n>splits</span><span class=p>)</span>
</span><span id=__span-130-4><a id=__codelineno-130-4 name=__codelineno-130-4 href=#__codelineno-130-4></a>    <span class=n>best_pair</span><span class=p>,</span> <span class=n>max_score</span> <span class=o>=</span> <span class=s2>&quot;&quot;</span><span class=p>,</span> <span class=kc>None</span>
</span><span id=__span-130-5><a id=__codelineno-130-5 name=__codelineno-130-5 href=#__codelineno-130-5></a>    <span class=k>for</span> <span class=n>pair</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>scores</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-130-6><a id=__codelineno-130-6 name=__codelineno-130-6 href=#__codelineno-130-6></a>        <span class=k>if</span> <span class=n>max_score</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>max_score</span> <span class=o>&lt;</span> <span class=n>score</span><span class=p>:</span>
</span><span id=__span-130-7><a id=__codelineno-130-7 name=__codelineno-130-7 href=#__codelineno-130-7></a>            <span class=n>best_pair</span> <span class=o>=</span> <span class=n>pair</span>
</span><span id=__span-130-8><a id=__codelineno-130-8 name=__codelineno-130-8 href=#__codelineno-130-8></a>            <span class=n>max_score</span> <span class=o>=</span> <span class=n>score</span>
</span><span id=__span-130-9><a id=__codelineno-130-9 name=__codelineno-130-9 href=#__codelineno-130-9></a>    <span class=n>splits</span> <span class=o>=</span> <span class=n>merge_pair</span><span class=p>(</span><span class=o>*</span><span class=n>best_pair</span><span class=p>,</span> <span class=n>splits</span><span class=p>)</span>
</span><span id=__span-130-10><a id=__codelineno-130-10 name=__codelineno-130-10 href=#__codelineno-130-10></a>    <span class=n>new_token</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-130-11><a id=__codelineno-130-11 name=__codelineno-130-11 href=#__codelineno-130-11></a>        <span class=n>best_pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-130-12><a id=__codelineno-130-12 name=__codelineno-130-12 href=#__codelineno-130-12></a>        <span class=k>if</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&quot;##&quot;</span><span class=p>)</span>
</span><span id=__span-130-13><a id=__codelineno-130-13 name=__codelineno-130-13 href=#__codelineno-130-13></a>        <span class=k>else</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>best_pair</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-130-14><a id=__codelineno-130-14 name=__codelineno-130-14 href=#__codelineno-130-14></a>    <span class=p>)</span>
</span><span id=__span-130-15><a id=__codelineno-130-15 name=__codelineno-130-15 href=#__codelineno-130-15></a>    <span class=n>vocab</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_token</span><span class=p>)</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-131-1><a id=__codelineno-131-1 name=__codelineno-131-1 href=#__codelineno-131-1></a><span class=nb>print</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-132-1><a id=__codelineno-132-1 name=__codelineno-132-1 href=#__codelineno-132-1></a>[&#39;[PAD]&#39;, &#39;[UNK]&#39;, &#39;[CLS]&#39;, &#39;[SEP]&#39;, &#39;[MASK]&#39;, &#39;##a&#39;, &#39;##b&#39;, &#39;##c&#39;, &#39;##d&#39;, &#39;##e&#39;, &#39;##f&#39;, &#39;##g&#39;, &#39;##h&#39;, &#39;##i&#39;, &#39;##k&#39;, &#39;##l&#39;, &#39;##m&#39;, &#39;##n&#39;, &#39;##o&#39;, &#39;##p&#39;, &#39;##r&#39;, &#39;##s&#39;, &#39;##t&#39;, &#39;##u&#39;, &#39;##v&#39;, &#39;##w&#39;, &#39;##y&#39;, &#39;##z&#39;, &#39;,&#39;, &#39;.&#39;, &#39;C&#39;, &#39;F&#39;, &#39;H&#39;, &#39;T&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;w&#39;, &#39;y&#39;, &#39;ab&#39;, &#39;##fu&#39;, &#39;Fa&#39;, &#39;Fac&#39;, &#39;##ct&#39;, &#39;##ful&#39;, &#39;##full&#39;, &#39;##fully&#39;, &#39;Th&#39;, &#39;ch&#39;, &#39;##hm&#39;, &#39;cha&#39;, &#39;chap&#39;, &#39;chapt&#39;, &#39;##thm&#39;, &#39;Hu&#39;, &#39;Hug&#39;, &#39;Hugg&#39;, &#39;sh&#39;, &#39;th&#39;, &#39;is&#39;, &#39;##thms&#39;, &#39;##za&#39;, &#39;##zat&#39;, &#39;##ut&#39;]
</span></code></pre></div></p> <p>見てのとおり、BPEと比較して、このトークナイザーはトークンとして単語の一部をもう少し速く学習します。</p> <div class="admonition tip"> <p class=admonition-title>同一コーパスでも異なる結果</p> <p>同じコーパスで<code>train_new_from_iterator()</code>を使用しても、まったく同じ語彙にはなりません。これは、Hugging Face TokenizersライブラリがWordPieceを訓練用に実装していない（内部について完全に確信していないため）ため、代わりにBPEを使用するためです。</p> </div> <p>新しいテキストをトークン化するために、前処理を行い、分割してから、各単語にトークン化アルゴリズムを適用します。つまり、最初の単語の最初から始まる最大のサブワードを探して分割し、次に2番目の部分でプロセスを繰り返し、その単語の残りの部分とテキストの以下の単語についても同様に行います：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-133-1><a id=__codelineno-133-1 name=__codelineno-133-1 href=#__codelineno-133-1></a><span class=k>def</span><span class=w> </span><span class=nf>encode_word</span><span class=p>(</span><span class=n>word</span><span class=p>):</span>
</span><span id=__span-133-2><a id=__codelineno-133-2 name=__codelineno-133-2 href=#__codelineno-133-2></a>    <span class=n>tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-133-3><a id=__codelineno-133-3 name=__codelineno-133-3 href=#__codelineno-133-3></a>    <span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-133-4><a id=__codelineno-133-4 name=__codelineno-133-4 href=#__codelineno-133-4></a>        <span class=n>i</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span><span id=__span-133-5><a id=__codelineno-133-5 name=__codelineno-133-5 href=#__codelineno-133-5></a>        <span class=k>while</span> <span class=n>i</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>word</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>vocab</span><span class=p>:</span>
</span><span id=__span-133-6><a id=__codelineno-133-6 name=__codelineno-133-6 href=#__codelineno-133-6></a>            <span class=n>i</span> <span class=o>-=</span> <span class=mi>1</span>
</span><span id=__span-133-7><a id=__codelineno-133-7 name=__codelineno-133-7 href=#__codelineno-133-7></a>        <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-133-8><a id=__codelineno-133-8 name=__codelineno-133-8 href=#__codelineno-133-8></a>            <span class=k>return</span> <span class=p>[</span><span class=s2>&quot;[UNK]&quot;</span><span class=p>]</span>
</span><span id=__span-133-9><a id=__codelineno-133-9 name=__codelineno-133-9 href=#__codelineno-133-9></a>        <span class=n>tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>word</span><span class=p>[:</span><span class=n>i</span><span class=p>])</span>
</span><span id=__span-133-10><a id=__codelineno-133-10 name=__codelineno-133-10 href=#__codelineno-133-10></a>        <span class=n>word</span> <span class=o>=</span> <span class=n>word</span><span class=p>[</span><span class=n>i</span><span class=p>:]</span>
</span><span id=__span-133-11><a id=__codelineno-133-11 name=__codelineno-133-11 href=#__codelineno-133-11></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-133-12><a id=__codelineno-133-12 name=__codelineno-133-12 href=#__codelineno-133-12></a>            <span class=n>word</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;##</span><span class=si>{</span><span class=n>word</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-133-13><a id=__codelineno-133-13 name=__codelineno-133-13 href=#__codelineno-133-13></a>    <span class=k>return</span> <span class=n>tokens</span>
</span></code></pre></div> <p>語彙内にある単語と、そうでない単語でテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-134-1><a id=__codelineno-134-1 name=__codelineno-134-1 href=#__codelineno-134-1></a><span class=nb>print</span><span class=p>(</span><span class=n>encode_word</span><span class=p>(</span><span class=s2>&quot;Hugging&quot;</span><span class=p>))</span>
</span><span id=__span-134-2><a id=__codelineno-134-2 name=__codelineno-134-2 href=#__codelineno-134-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encode_word</span><span class=p>(</span><span class=s2>&quot;HOgging&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-135-1><a id=__codelineno-135-1 name=__codelineno-135-1 href=#__codelineno-135-1></a>[&#39;Hugg&#39;, &#39;##i&#39;, &#39;##n&#39;, &#39;##g&#39;]
</span><span id=__span-135-2><a id=__codelineno-135-2 name=__codelineno-135-2 href=#__codelineno-135-2></a>[&#39;[UNK]&#39;]
</span></code></pre></div></p> <p>次に、テキストをトークン化する関数を書きましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-136-1><a id=__codelineno-136-1 name=__codelineno-136-1 href=#__codelineno-136-1></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span><span id=__span-136-2><a id=__codelineno-136-2 name=__codelineno-136-2 href=#__codelineno-136-2></a>    <span class=n>pre_tokenize_result</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-136-3><a id=__codelineno-136-3 name=__codelineno-136-3 href=#__codelineno-136-3></a>    <span class=n>pre_tokenized_text</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>pre_tokenize_result</span><span class=p>]</span>
</span><span id=__span-136-4><a id=__codelineno-136-4 name=__codelineno-136-4 href=#__codelineno-136-4></a>    <span class=n>encoded_words</span> <span class=o>=</span> <span class=p>[</span><span class=n>encode_word</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>pre_tokenized_text</span><span class=p>]</span>
</span><span id=__span-136-5><a id=__codelineno-136-5 name=__codelineno-136-5 href=#__codelineno-136-5></a>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>encoded_words</span><span class=p>,</span> <span class=p>[])</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-137-1><a id=__codelineno-137-1 name=__codelineno-137-1 href=#__codelineno-137-1></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenize</span><span class=p>(</span><span class=s2>&quot;This is the Hugging Face course!&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-138-1><a id=__codelineno-138-1 name=__codelineno-138-1 href=#__codelineno-138-1></a>[&#39;Th&#39;, &#39;##i&#39;, &#39;##s&#39;, &#39;is&#39;, &#39;th&#39;, &#39;##e&#39;, &#39;Hugg&#39;, &#39;##i&#39;, &#39;##n&#39;, &#39;##g&#39;, &#39;Fac&#39;, &#39;##e&#39;, &#39;c&#39;, &#39;##o&#39;, &#39;##u&#39;, &#39;##r&#39;, &#39;##s&#39;, &#39;##e&#39;, &#39;[UNK]&#39;]
</span></code></pre></div></p> <h2 id=unigram>Unigramトークン化<a class=headerlink href=#unigram title="Permanent link">&para;</a></h2> <p>Unigramアルゴリズムは<a href=https://huggingface.co/papers/1808.06226>SentencePiece</a>と組み合わせて使用されます。これは、AlBERT、T5、mBART、Big Bird、XLNetなどのモデルで使用されるトークン化アルゴリズムです。</p> <p>SentencePieceは、すべての言語がスペースを使用して単語を分離するわけではないという事実に対処します。代わりに、SentencePieceは入力を生の入力ストリームとして扱い、使用する文字セットにスペースを含めます。次に、Unigramアルゴリズムを使用して適切な語彙を構築できます。</p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>このセクションではUnigramを詳細に説明し、完全な実装を示すところまで行います。一般的なトークン化アルゴリズムの概要だけが欲しい場合は、最後まで飛ばしても構いません。</p> </div> <h3 id=_21>訓練アルゴリズム<a class=headerlink href=#_21 title="Permanent link">&para;</a></h3> <p>BPEやWordPieceと比較して、Unigramは逆方向に動作します：大きな語彙から開始し、望ましい語彙サイズに達するまでトークンを削除します。基本語彙を構築するためのいくつかのオプションがあります：例えば、前処理された単語で最も一般的なサブストリングを取得する、または大きな語彙サイズで初期コーパスにBPEを適用することができます。</p> <p>訓練の各ステップで、Unigramアルゴリズムは現在の語彙が与えられたコーパス上での損失を計算します。次に、語彙内の各シンボルについて、シンボルが削除された場合に全体の損失がどの程度増加するかを計算し、最小に増加させるシンボルを探します。これらのシンボルはコーパス上の全体的な損失に対する影響が低いため、ある意味で「あまり必要でない」ため、削除の最良の候補です。</p> <p>これはすべて非常にコストの高い操作なので、最低損失増加に関連する単一シンボルを削除するのではなく、最低損失増加に関連するシンボルの\(p\)パーセント（\(p\)は制御できるハイパーパラメータで、通常10または20）を削除します。このプロセスは、語彙が望ましいサイズに達するまで繰り返されます。</p> <p>基本文字は絶対に削除しないことに注意してください。これにより、任意の単語がトークン化できることが保証されます。</p> <p>これでもまだ少し曖昧です：アルゴリズムの主要部分は、コーパス上での損失を計算し、語彙からいくつかのトークンを削除するときにそれがどのように変化するかを確認することですが、これを行う方法をまだ説明していません。このステップは、Unigramモデルのトークン化アルゴリズムに依存しているため、次にこれについて詳しく説明します。</p> <p>前の例のコーパスを再利用します：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-139-1><a id=__codelineno-139-1 name=__codelineno-139-1 href=#__codelineno-139-1></a>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
</span></code></pre></div> <p>この例では、初期語彙にすべての厳密なサブストリングを取ります：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-140-1><a id=__codelineno-140-1 name=__codelineno-140-1 href=#__codelineno-140-1></a>[&quot;h&quot;, &quot;u&quot;, &quot;g&quot;, &quot;hu&quot;, &quot;ug&quot;, &quot;p&quot;, &quot;pu&quot;, &quot;n&quot;, &quot;un&quot;, &quot;b&quot;, &quot;bu&quot;, &quot;s&quot;, &quot;hug&quot;, &quot;gs&quot;, &quot;ugs&quot;]
</span></code></pre></div> <h3 id=_22>トークン化アルゴリズム<a class=headerlink href=#_22 title="Permanent link">&para;</a></h3> <p>Unigramモデルは、各トークンが前のトークンから独立していると考える言語モデルの一種です。最も単純な言語モデルであり、前のコンテキストが与えられたトークンXの確率は、単にトークンXの確率です。そのため、Unigram言語モデルを使用してテキストを生成する場合、常に最も一般的なトークンを予測することになります。</p> <p>特定のトークンの確率は、元のコーパス内でのその頻度（見つけた回数）を、語彙内のすべてのトークンのすべての頻度の合計で割ったものです（確率の合計が1になることを確認するため）。例えば、<code>"ug"</code>は<code>"hug"</code>、<code>"pug"</code>、<code>"hugs"</code>に存在するため、コーパス内で頻度が20です。</p> <p>語彙内のすべての可能なサブワードの頻度は次のとおりです：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-141-1><a id=__codelineno-141-1 name=__codelineno-141-1 href=#__codelineno-141-1></a>(&quot;h&quot;, 15) (&quot;u&quot;, 36) (&quot;g&quot;, 20) (&quot;hu&quot;, 15) (&quot;ug&quot;, 20) (&quot;p&quot;, 17) (&quot;pu&quot;, 17) (&quot;n&quot;, 16)
</span><span id=__span-141-2><a id=__codelineno-141-2 name=__codelineno-141-2 href=#__codelineno-141-2></a>(&quot;un&quot;, 16) (&quot;b&quot;, 4) (&quot;bu&quot;, 4) (&quot;s&quot;, 5) (&quot;hug&quot;, 15) (&quot;gs&quot;, 5) (&quot;ugs&quot;, 5)
</span></code></pre></div> <p>すべての頻度の合計は210なので、サブワード<code>"ug"</code>の確率は20/210です。</p> <p>今、特定の単語をトークン化するために、すべての可能なトークンへのセグメンテーションを見て、Unigramモデルに従って各々の確率を計算します。すべてのトークンは独立していると見なされるため、この確率は単に各トークンの確率の積です。例えば、<code>"pug"</code>のトークン化<code>["p", "u", "g"]</code>の確率は：</p> <p><span class=arithmatex>\(P([<code>p",</code>u", <code>g"]) = P(</code>p") \times P(<code>u") \times P(</code>g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389\)</span></p> <p>比較として、トークン化<code>["pu", "g"]</code>の確率は：</p> <p><span class=arithmatex>\(P([<code>pu",</code>g"]) = P(<code>pu") \times P(</code>g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676\)</span></p> <p>そのため、そちらの方がはるかに可能性が高いです。一般に、可能な限り少ないトークン数でのトークン化が最高確率を持ちます（各トークンに対して210での除算が繰り返されるため）。これは、直感的に望むもの（単語をできるだけ少ないトークン数に分割する）に対応します。</p> <p>Unigramモデルでの単語のトークン化は、最高確率を持つトークン化です。<code>"pug"</code>の例では、各可能なセグメンテーションで得られる確率は次のとおりです：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-142-1><a id=__codelineno-142-1 name=__codelineno-142-1 href=#__codelineno-142-1></a>[&quot;p&quot;, &quot;u&quot;, &quot;g&quot;] : 0.000389
</span><span id=__span-142-2><a id=__codelineno-142-2 name=__codelineno-142-2 href=#__codelineno-142-2></a>[&quot;p&quot;, &quot;ug&quot;] : 0.0022676
</span><span id=__span-142-3><a id=__codelineno-142-3 name=__codelineno-142-3 href=#__codelineno-142-3></a>[&quot;pu&quot;, &quot;g&quot;] : 0.0022676
</span></code></pre></div> <p>そのため、<code>"pug"</code>は<code>["p", "ug"]</code>または<code>["pu", "g"]</code>としてトークン化されます。これらのセグメンテーションのうちどちらが最初に遭遇されるかによります（より大きなコーパスでは、このような等価ケースは稀です）。</p> <p>この場合、すべての可能なセグメンテーションを見つけて確率を計算するのは簡単でしたが、一般的にはもう少し困難です。これに使用される古典的なアルゴリズムは<strong>Viterbiアルゴリズム</strong>と呼ばれます。本質的に、特定の単語の可能なセグメンテーションを検出するためにグラフを構築します。文字_a_から文字_b_への分岐がある場合、_a_から_b_までのサブワードが語彙内にあるといって、そのブランチにサブワードの確率を属性として割り当てます。</p> <p>そのグラフで最高スコアを持つパスを見つけるために、Viterbiアルゴリズムが決定します。単語の各位置について、その位置で終わる最高スコアのセグメンテーションを決定します。最初から最後に行くので、その最高スコアは、現在位置で終わるすべてのサブワードをループして、このサブワードが始まる位置からの最高のトークン化スコアを使用することで見つけることができます。次に、最後に到達するために取ったパスを展開するだけです。</p> <p>語彙と単語<code>"unhug"</code>を使用した例を見てみましょう。各位置について、そこで終わる最高スコアのサブワードは以下の通りです：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-143-1><a id=__codelineno-143-1 name=__codelineno-143-1 href=#__codelineno-143-1></a>Character 0 (u): &quot;u&quot; (score 0.171429)
</span><span id=__span-143-2><a id=__codelineno-143-2 name=__codelineno-143-2 href=#__codelineno-143-2></a>Character 1 (n): &quot;un&quot; (score 0.076191)
</span><span id=__span-143-3><a id=__codelineno-143-3 name=__codelineno-143-3 href=#__codelineno-143-3></a>Character 2 (h): &quot;un&quot; &quot;h&quot; (score 0.005442)
</span><span id=__span-143-4><a id=__codelineno-143-4 name=__codelineno-143-4 href=#__codelineno-143-4></a>Character 3 (u): &quot;un&quot; &quot;hu&quot; (score 0.005442)
</span><span id=__span-143-5><a id=__codelineno-143-5 name=__codelineno-143-5 href=#__codelineno-143-5></a>Character 4 (g): &quot;un&quot; &quot;hug&quot; (score 0.005442)
</span></code></pre></div> <p>したがって、<code>"unhug"</code>は<code>["un", "hug"]</code>としてトークン化されます。</p> <h3 id=_23>訓練に戻る<a class=headerlink href=#_23 title="Permanent link">&para;</a></h3> <p>トークン化がどのように動作するかを見たので、訓練中に使用される損失についてもう少し深く掘り下げることができます。任意の段階で、この損失は、現在の語彙とコーパス内の各トークンの頻度によって決定されるUnigramモデルを使用して、コーパス内のすべての単語をトークン化することで計算されます（前述のとおり）。</p> <p>コーパス内の各単語にはスコアがあり、損失はそれらのスコアの負の対数尤度、つまり、コーパス内のすべての単語についてのすべての<code>-log(P(word))</code>の合計です。</p> <p>以下のコーパスを使った例に戻りましょう：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-144-1><a id=__codelineno-144-1 name=__codelineno-144-1 href=#__codelineno-144-1></a>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
</span></code></pre></div> <p>それぞれのスコアを持つ各単語のトークン化は：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-145-1><a id=__codelineno-145-1 name=__codelineno-145-1 href=#__codelineno-145-1></a>&quot;hug&quot;: [&quot;hug&quot;] (score 0.071428)
</span><span id=__span-145-2><a id=__codelineno-145-2 name=__codelineno-145-2 href=#__codelineno-145-2></a>&quot;pug&quot;: [&quot;pu&quot;, &quot;g&quot;] (score 0.007710)
</span><span id=__span-145-3><a id=__codelineno-145-3 name=__codelineno-145-3 href=#__codelineno-145-3></a>&quot;pun&quot;: [&quot;pu&quot;, &quot;n&quot;] (score 0.006168)
</span><span id=__span-145-4><a id=__codelineno-145-4 name=__codelineno-145-4 href=#__codelineno-145-4></a>&quot;bun&quot;: [&quot;bu&quot;, &quot;n&quot;] (score 0.001451)
</span><span id=__span-145-5><a id=__codelineno-145-5 name=__codelineno-145-5 href=#__codelineno-145-5></a>&quot;hugs&quot;: [&quot;hug&quot;, &quot;s&quot;] (score 0.001701)
</span></code></pre></div> <p>そのため、損失は：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-146-1><a id=__codelineno-146-1 name=__codelineno-146-1 href=#__codelineno-146-1></a>10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
</span></code></pre></div> <p>今、各トークンを削除することが損失にどのように影響するかを計算する必要があります。これはかなり面倒なので、ここでは2つのトークンについてのみ行い、コードがある時の全体的なプロセスは後で保存します。この（非常に）特殊なケースでは、すべての単語の2つの同等なトークン化がありました：前述のように、例えば<code>"pug"</code>は同じスコアで<code>["p", "ug"]</code>としてトークン化できました。したがって、語彙から<code>"pu"</code>トークンを削除すると、まったく同じ損失が得られます。</p> <p>一方、<code>"hug"</code>を削除すると損失が悪化します。<code>"hug"</code>と<code>"hugs"</code>のトークン化が以下のようになるためです：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-147-1><a id=__codelineno-147-1 name=__codelineno-147-1 href=#__codelineno-147-1></a>&quot;hug&quot;: [&quot;hu&quot;, &quot;g&quot;] (score 0.006802)
</span><span id=__span-147-2><a id=__codelineno-147-2 name=__codelineno-147-2 href=#__codelineno-147-2></a>&quot;hugs&quot;: [&quot;hu&quot;, &quot;gs&quot;] (score 0.001701)
</span></code></pre></div> <p>これらの変更により、損失は以下だけ上昇します：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-148-1><a id=__codelineno-148-1 name=__codelineno-148-1 href=#__codelineno-148-1></a>- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
</span></code></pre></div> <p>したがって、トークン<code>"pu"</code>はおそらく語彙から削除されますが、<code>"hug"</code>は削除されません。</p> <h3 id=unigram_1>Unigramの実装<a class=headerlink href=#unigram_1 title="Permanent link">&para;</a></h3> <p>これまで見てきたすべてをコードで実装してみましょう。BPEやWordPieceと同様に、これはUnigramアルゴリズムの効率的な実装ではありません（全く逆です）が、理解するのに役立つはずです。</p> <p>前と同じコーパスを例として使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-149-1><a id=__codelineno-149-1 name=__codelineno-149-1 href=#__codelineno-149-1></a><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-149-2><a id=__codelineno-149-2 name=__codelineno-149-2 href=#__codelineno-149-2></a>    <span class=s2>&quot;This is the Hugging Face Course.&quot;</span><span class=p>,</span>
</span><span id=__span-149-3><a id=__codelineno-149-3 name=__codelineno-149-3 href=#__codelineno-149-3></a>    <span class=s2>&quot;This chapter is about tokenization.&quot;</span><span class=p>,</span>
</span><span id=__span-149-4><a id=__codelineno-149-4 name=__codelineno-149-4 href=#__codelineno-149-4></a>    <span class=s2>&quot;This section shows several tokenizer algorithms.&quot;</span><span class=p>,</span>
</span><span id=__span-149-5><a id=__codelineno-149-5 name=__codelineno-149-5 href=#__codelineno-149-5></a>    <span class=s2>&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span><span class=p>,</span>
</span><span id=__span-149-6><a id=__codelineno-149-6 name=__codelineno-149-6 href=#__codelineno-149-6></a><span class=p>]</span>
</span></code></pre></div> <p>今回は、モデルとして<code>xlnet-base-cased</code>を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-150-1><a id=__codelineno-150-1 name=__codelineno-150-1 href=#__codelineno-150-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span><span id=__span-150-2><a id=__codelineno-150-2 name=__codelineno-150-2 href=#__codelineno-150-2></a>
</span><span id=__span-150-3><a id=__codelineno-150-3 name=__codelineno-150-3 href=#__codelineno-150-3></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;xlnet-base-cased&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>BPEやWordPieceと同様に、コーパス内の各単語の出現回数をカウントすることから始めます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-151-1><a id=__codelineno-151-1 name=__codelineno-151-1 href=#__codelineno-151-1></a><span class=kn>from</span><span class=w> </span><span class=nn>collections</span><span class=w> </span><span class=kn>import</span> <span class=n>defaultdict</span>
</span><span id=__span-151-2><a id=__codelineno-151-2 name=__codelineno-151-2 href=#__codelineno-151-2></a>
</span><span id=__span-151-3><a id=__codelineno-151-3 name=__codelineno-151-3 href=#__codelineno-151-3></a><span class=n>word_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-151-4><a id=__codelineno-151-4 name=__codelineno-151-4 href=#__codelineno-151-4></a><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>:</span>
</span><span id=__span-151-5><a id=__codelineno-151-5 name=__codelineno-151-5 href=#__codelineno-151-5></a>    <span class=n>words_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-151-6><a id=__codelineno-151-6 name=__codelineno-151-6 href=#__codelineno-151-6></a>    <span class=n>new_words</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>words_with_offsets</span><span class=p>]</span>
</span><span id=__span-151-7><a id=__codelineno-151-7 name=__codelineno-151-7 href=#__codelineno-151-7></a>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>new_words</span><span class=p>:</span>
</span><span id=__span-151-8><a id=__codelineno-151-8 name=__codelineno-151-8 href=#__codelineno-151-8></a>        <span class=n>word_freqs</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-151-9><a id=__codelineno-151-9 name=__codelineno-151-9 href=#__codelineno-151-9></a>
</span><span id=__span-151-10><a id=__codelineno-151-10 name=__codelineno-151-10 href=#__codelineno-151-10></a><span class=nb>print</span><span class=p>(</span><span class=n>word_freqs</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-152-1><a id=__codelineno-152-1 name=__codelineno-152-1 href=#__codelineno-152-1></a>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;▁This&#39;: 3, &#39;▁is&#39;: 2, &#39;▁the&#39;: 1, &#39;▁Hugging&#39;: 1, &#39;▁Face&#39;: 1, &#39;▁Course.&#39;: 1, &#39;▁chapter&#39;: 1, &#39;▁about&#39;: 1, &#39;▁tokenization.&#39;: 1, &#39;▁section&#39;: 1, &#39;▁shows&#39;: 1, &#39;▁several&#39;: 1, &#39;▁tokenizer&#39;: 1, &#39;▁algorithms.&#39;: 1, &#39;▁Hopefully,&#39;: 1, &#39;▁you&#39;: 1, &#39;▁will&#39;: 1, &#39;▁be&#39;: 1, &#39;▁able&#39;: 1, &#39;▁to&#39;: 1, &#39;▁understand&#39;: 1, &#39;▁how&#39;: 1, &#39;▁they&#39;: 1, &#39;▁are&#39;: 1, &#39;▁trained&#39;: 1, &#39;▁and&#39;: 1, &#39;▁generate&#39;: 1, &#39;▁tokens.&#39;: 1})
</span></code></pre></div></p> <p>次に、最終的に欲しい語彙サイズより大きな何かに語彙を初期化する必要があります。すべての基本文字を含める必要があります（そうでないと、すべての単語をトークン化できません）が、大きなサブストリングについては、最も一般的なもののみを保持するので、頻度で並べ替えます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-153-1><a id=__codelineno-153-1 name=__codelineno-153-1 href=#__codelineno-153-1></a><span class=n>char_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-153-2><a id=__codelineno-153-2 name=__codelineno-153-2 href=#__codelineno-153-2></a><span class=n>subwords_freqs</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span><span id=__span-153-3><a id=__codelineno-153-3 name=__codelineno-153-3 href=#__codelineno-153-3></a><span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-153-4><a id=__codelineno-153-4 name=__codelineno-153-4 href=#__codelineno-153-4></a>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)):</span>
</span><span id=__span-153-5><a id=__codelineno-153-5 name=__codelineno-153-5 href=#__codelineno-153-5></a>        <span class=n>char_freqs</span><span class=p>[</span><span class=n>word</span><span class=p>[</span><span class=n>i</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-153-6><a id=__codelineno-153-6 name=__codelineno-153-6 href=#__codelineno-153-6></a>        <span class=c1># 長さが少なくとも2のサブワードをループする</span>
</span><span id=__span-153-7><a id=__codelineno-153-7 name=__codelineno-153-7 href=#__codelineno-153-7></a>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span><span id=__span-153-8><a id=__codelineno-153-8 name=__codelineno-153-8 href=#__codelineno-153-8></a>            <span class=n>subwords_freqs</span><span class=p>[</span><span class=n>word</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>j</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>freq</span>
</span><span id=__span-153-9><a id=__codelineno-153-9 name=__codelineno-153-9 href=#__codelineno-153-9></a>
</span><span id=__span-153-10><a id=__codelineno-153-10 name=__codelineno-153-10 href=#__codelineno-153-10></a><span class=c1># サブワードを頻度で並べ替え</span>
</span><span id=__span-153-11><a id=__codelineno-153-11 name=__codelineno-153-11 href=#__codelineno-153-11></a><span class=n>sorted_subwords</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>subwords_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-153-12><a id=__codelineno-153-12 name=__codelineno-153-12 href=#__codelineno-153-12></a><span class=n>sorted_subwords</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-154-1><a id=__codelineno-154-1 name=__codelineno-154-1 href=#__codelineno-154-1></a>[(&#39;▁t&#39;, 7),
</span><span id=__span-154-2><a id=__codelineno-154-2 name=__codelineno-154-2 href=#__codelineno-154-2></a> (&#39;is&#39;, 5),
</span><span id=__span-154-3><a id=__codelineno-154-3 name=__codelineno-154-3 href=#__codelineno-154-3></a> (&#39;er&#39;, 5),
</span><span id=__span-154-4><a id=__codelineno-154-4 name=__codelineno-154-4 href=#__codelineno-154-4></a> (&#39;▁a&#39;, 5),
</span><span id=__span-154-5><a id=__codelineno-154-5 name=__codelineno-154-5 href=#__codelineno-154-5></a> (&#39;▁to&#39;, 4),
</span><span id=__span-154-6><a id=__codelineno-154-6 name=__codelineno-154-6 href=#__codelineno-154-6></a> (&#39;to&#39;, 4),
</span><span id=__span-154-7><a id=__codelineno-154-7 name=__codelineno-154-7 href=#__codelineno-154-7></a> (&#39;en&#39;, 4),
</span><span id=__span-154-8><a id=__codelineno-154-8 name=__codelineno-154-8 href=#__codelineno-154-8></a> (&#39;▁T&#39;, 3),
</span><span id=__span-154-9><a id=__codelineno-154-9 name=__codelineno-154-9 href=#__codelineno-154-9></a> (&#39;▁Th&#39;, 3),
</span><span id=__span-154-10><a id=__codelineno-154-10 name=__codelineno-154-10 href=#__codelineno-154-10></a> (&#39;▁Thi&#39;, 3)]
</span></code></pre></div></p> <p>文字と最良のサブワードをグループ化して、サイズ300の初期語彙に到達します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-155-1><a id=__codelineno-155-1 name=__codelineno-155-1 href=#__codelineno-155-1></a><span class=n>token_freqs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>char_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>())</span> <span class=o>+</span> <span class=n>sorted_subwords</span><span class=p>[:</span> <span class=mi>300</span> <span class=o>-</span> <span class=nb>len</span><span class=p>(</span><span class=n>char_freqs</span><span class=p>)]</span>
</span><span id=__span-155-2><a id=__codelineno-155-2 name=__codelineno-155-2 href=#__codelineno-155-2></a><span class=n>token_freqs</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span> <span class=n>freq</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=p>}</span>
</span></code></pre></div> <div class="admonition tip"> <p class=admonition-title>SentencePieceの効率的なアルゴリズム</p> <p>SentencePieceは、初期語彙を作成するためにEnhanced Suffix Array（ESA）と呼ばれるより効率的なアルゴリズムを使用します。</p> </div> <p>次に、すべての頻度の合計を計算して、頻度を確率に変換します。私たちのモデルでは、確率の対数を格納します。小さな数を掛けるよりも対数を足す方が数値的に安定しており、これによりモデルの損失の計算が簡単になります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-156-1><a id=__codelineno-156-1 name=__codelineno-156-1 href=#__codelineno-156-1></a><span class=kn>from</span><span class=w> </span><span class=nn>math</span><span class=w> </span><span class=kn>import</span> <span class=n>log</span>
</span><span id=__span-156-2><a id=__codelineno-156-2 name=__codelineno-156-2 href=#__codelineno-156-2></a>
</span><span id=__span-156-3><a id=__codelineno-156-3 name=__codelineno-156-3 href=#__codelineno-156-3></a><span class=n>total_sum</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>([</span><span class=n>freq</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()])</span>
</span><span id=__span-156-4><a id=__codelineno-156-4 name=__codelineno-156-4 href=#__codelineno-156-4></a><span class=n>model</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span> <span class=o>-</span><span class=n>log</span><span class=p>(</span><span class=n>freq</span> <span class=o>/</span> <span class=n>total_sum</span><span class=p>)</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></code></pre></div> <p>主要な関数は、Viterbiアルゴリズムを使用して単語をトークン化するものです。前述のとおり、このアルゴリズムは単語の各サブストリングの最良のセグメンテーションを計算し、<code>best_segmentations</code>という変数に格納します。単語内の各位置（0から総長まで）に対して1つの辞書を格納し、2つのキーを持ちます：最良のセグメンテーションの最後のトークンの開始のインデックスと、最良のセグメンテーションのスコアです。最後のトークンの開始のインデックスがあれば、リストが完全に入力されたら完全なセグメンテーションを取得できます。</p> <p>リストの入力は2つのループだけで行われます：メインループは各開始位置を通り、2番目のループはその開始位置で始まるすべてのサブストリングを試します。サブストリングが語彙内にある場合、その終了位置までの単語の新しいセグメンテーションがあり、これを<code>best_segmentations</code>にあるものと比較します。</p> <p>メインループが終了したら、最後から開始し、一つの開始位置から次の開始位置にホップして、進行中にトークンを記録し、単語の開始に到達するまで続けます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-157-1><a id=__codelineno-157-1 name=__codelineno-157-1 href=#__codelineno-157-1></a><span class=k>def</span><span class=w> </span><span class=nf>encode_word</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>model</span><span class=p>):</span>
</span><span id=__span-157-2><a id=__codelineno-157-2 name=__codelineno-157-2 href=#__codelineno-157-2></a>    <span class=n>best_segmentations</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&quot;start&quot;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=mi>1</span><span class=p>}]</span> <span class=o>+</span> <span class=p>[</span>
</span><span id=__span-157-3><a id=__codelineno-157-3 name=__codelineno-157-3 href=#__codelineno-157-3></a>        <span class=p>{</span><span class=s2>&quot;start&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span> <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>}</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>))</span>
</span><span id=__span-157-4><a id=__codelineno-157-4 name=__codelineno-157-4 href=#__codelineno-157-4></a>    <span class=p>]</span>
</span><span id=__span-157-5><a id=__codelineno-157-5 name=__codelineno-157-5 href=#__codelineno-157-5></a>    <span class=k>for</span> <span class=n>start_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)):</span>
</span><span id=__span-157-6><a id=__codelineno-157-6 name=__codelineno-157-6 href=#__codelineno-157-6></a>        <span class=c1># これはループの前のステップで適切に入力されているはず</span>
</span><span id=__span-157-7><a id=__codelineno-157-7 name=__codelineno-157-7 href=#__codelineno-157-7></a>        <span class=n>best_score_at_start</span> <span class=o>=</span> <span class=n>best_segmentations</span><span class=p>[</span><span class=n>start_idx</span><span class=p>][</span><span class=s2>&quot;score&quot;</span><span class=p>]</span>
</span><span id=__span-157-8><a id=__codelineno-157-8 name=__codelineno-157-8 href=#__codelineno-157-8></a>        <span class=k>for</span> <span class=n>end_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>start_idx</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span><span id=__span-157-9><a id=__codelineno-157-9 name=__codelineno-157-9 href=#__codelineno-157-9></a>            <span class=n>token</span> <span class=o>=</span> <span class=n>word</span><span class=p>[</span><span class=n>start_idx</span><span class=p>:</span><span class=n>end_idx</span><span class=p>]</span>
</span><span id=__span-157-10><a id=__codelineno-157-10 name=__codelineno-157-10 href=#__codelineno-157-10></a>            <span class=k>if</span> <span class=n>token</span> <span class=ow>in</span> <span class=n>model</span> <span class=ow>and</span> <span class=n>best_score_at_start</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-157-11><a id=__codelineno-157-11 name=__codelineno-157-11 href=#__codelineno-157-11></a>                <span class=n>score</span> <span class=o>=</span> <span class=n>model</span><span class=p>[</span><span class=n>token</span><span class=p>]</span> <span class=o>+</span> <span class=n>best_score_at_start</span>
</span><span id=__span-157-12><a id=__codelineno-157-12 name=__codelineno-157-12 href=#__codelineno-157-12></a>                <span class=c1># end_idxで終わる より良いセグメンテーションを見つけた場合、更新する</span>
</span><span id=__span-157-13><a id=__codelineno-157-13 name=__codelineno-157-13 href=#__codelineno-157-13></a>                <span class=k>if</span> <span class=p>(</span>
</span><span id=__span-157-14><a id=__codelineno-157-14 name=__codelineno-157-14 href=#__codelineno-157-14></a>                    <span class=n>best_segmentations</span><span class=p>[</span><span class=n>end_idx</span><span class=p>][</span><span class=s2>&quot;score&quot;</span><span class=p>]</span> <span class=ow>is</span> <span class=kc>None</span>
</span><span id=__span-157-15><a id=__codelineno-157-15 name=__codelineno-157-15 href=#__codelineno-157-15></a>                    <span class=ow>or</span> <span class=n>best_segmentations</span><span class=p>[</span><span class=n>end_idx</span><span class=p>][</span><span class=s2>&quot;score&quot;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>score</span>
</span><span id=__span-157-16><a id=__codelineno-157-16 name=__codelineno-157-16 href=#__codelineno-157-16></a>                <span class=p>):</span>
</span><span id=__span-157-17><a id=__codelineno-157-17 name=__codelineno-157-17 href=#__codelineno-157-17></a>                    <span class=n>best_segmentations</span><span class=p>[</span><span class=n>end_idx</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&quot;start&quot;</span><span class=p>:</span> <span class=n>start_idx</span><span class=p>,</span> <span class=s2>&quot;score&quot;</span><span class=p>:</span> <span class=n>score</span><span class=p>}</span>
</span><span id=__span-157-18><a id=__codelineno-157-18 name=__codelineno-157-18 href=#__codelineno-157-18></a>
</span><span id=__span-157-19><a id=__codelineno-157-19 name=__codelineno-157-19 href=#__codelineno-157-19></a>    <span class=n>segmentation</span> <span class=o>=</span> <span class=n>best_segmentations</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-157-20><a id=__codelineno-157-20 name=__codelineno-157-20 href=#__codelineno-157-20></a>    <span class=k>if</span> <span class=n>segmentation</span><span class=p>[</span><span class=s2>&quot;score&quot;</span><span class=p>]</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-157-21><a id=__codelineno-157-21 name=__codelineno-157-21 href=#__codelineno-157-21></a>        <span class=c1># 単語のトークン化が見つからなかった -&gt; 未知</span>
</span><span id=__span-157-22><a id=__codelineno-157-22 name=__codelineno-157-22 href=#__codelineno-157-22></a>        <span class=k>return</span> <span class=p>[</span><span class=s2>&quot;&lt;unk&gt;&quot;</span><span class=p>],</span> <span class=kc>None</span>
</span><span id=__span-157-23><a id=__codelineno-157-23 name=__codelineno-157-23 href=#__codelineno-157-23></a>
</span><span id=__span-157-24><a id=__codelineno-157-24 name=__codelineno-157-24 href=#__codelineno-157-24></a>    <span class=n>score</span> <span class=o>=</span> <span class=n>segmentation</span><span class=p>[</span><span class=s2>&quot;score&quot;</span><span class=p>]</span>
</span><span id=__span-157-25><a id=__codelineno-157-25 name=__codelineno-157-25 href=#__codelineno-157-25></a>    <span class=n>start</span> <span class=o>=</span> <span class=n>segmentation</span><span class=p>[</span><span class=s2>&quot;start&quot;</span><span class=p>]</span>
</span><span id=__span-157-26><a id=__codelineno-157-26 name=__codelineno-157-26 href=#__codelineno-157-26></a>    <span class=n>end</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span><span id=__span-157-27><a id=__codelineno-157-27 name=__codelineno-157-27 href=#__codelineno-157-27></a>    <span class=n>tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-157-28><a id=__codelineno-157-28 name=__codelineno-157-28 href=#__codelineno-157-28></a>    <span class=k>while</span> <span class=n>start</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-157-29><a id=__codelineno-157-29 name=__codelineno-157-29 href=#__codelineno-157-29></a>        <span class=n>tokens</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>word</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>])</span>
</span><span id=__span-157-30><a id=__codelineno-157-30 name=__codelineno-157-30 href=#__codelineno-157-30></a>        <span class=n>next_start</span> <span class=o>=</span> <span class=n>best_segmentations</span><span class=p>[</span><span class=n>start</span><span class=p>][</span><span class=s2>&quot;start&quot;</span><span class=p>]</span>
</span><span id=__span-157-31><a id=__codelineno-157-31 name=__codelineno-157-31 href=#__codelineno-157-31></a>        <span class=n>end</span> <span class=o>=</span> <span class=n>start</span>
</span><span id=__span-157-32><a id=__codelineno-157-32 name=__codelineno-157-32 href=#__codelineno-157-32></a>        <span class=n>start</span> <span class=o>=</span> <span class=n>next_start</span>
</span><span id=__span-157-33><a id=__codelineno-157-33 name=__codelineno-157-33 href=#__codelineno-157-33></a>    <span class=n>tokens</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>word</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>])</span>
</span><span id=__span-157-34><a id=__codelineno-157-34 name=__codelineno-157-34 href=#__codelineno-157-34></a>    <span class=k>return</span> <span class=n>tokens</span><span class=p>,</span> <span class=n>score</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-158-1><a id=__codelineno-158-1 name=__codelineno-158-1 href=#__codelineno-158-1></a><span class=nb>print</span><span class=p>(</span><span class=n>encode_word</span><span class=p>(</span><span class=s2>&quot;Hopefully&quot;</span><span class=p>,</span> <span class=n>model</span><span class=p>))</span>
</span><span id=__span-158-2><a id=__codelineno-158-2 name=__codelineno-158-2 href=#__codelineno-158-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encode_word</span><span class=p>(</span><span class=s2>&quot;This&quot;</span><span class=p>,</span> <span class=n>model</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-159-1><a id=__codelineno-159-1 name=__codelineno-159-1 href=#__codelineno-159-1></a>([&#39;H&#39;, &#39;o&#39;, &#39;p&#39;, &#39;e&#39;, &#39;f&#39;, &#39;u&#39;, &#39;ll&#39;, &#39;y&#39;], 41.5157494601402)
</span><span id=__span-159-2><a id=__codelineno-159-2 name=__codelineno-159-2 href=#__codelineno-159-2></a>([&#39;This&#39;], 6.288267030694535)
</span></code></pre></div></p> <p>コーパス上でのモデルの損失を計算するのは簡単です！</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-160-1><a id=__codelineno-160-1 name=__codelineno-160-1 href=#__codelineno-160-1></a><span class=k>def</span><span class=w> </span><span class=nf>compute_loss</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-160-2><a id=__codelineno-160-2 name=__codelineno-160-2 href=#__codelineno-160-2></a>    <span class=n>loss</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-160-3><a id=__codelineno-160-3 name=__codelineno-160-3 href=#__codelineno-160-3></a>    <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>word_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-160-4><a id=__codelineno-160-4 name=__codelineno-160-4 href=#__codelineno-160-4></a>        <span class=n>_</span><span class=p>,</span> <span class=n>word_loss</span> <span class=o>=</span> <span class=n>encode_word</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>model</span><span class=p>)</span>
</span><span id=__span-160-5><a id=__codelineno-160-5 name=__codelineno-160-5 href=#__codelineno-160-5></a>        <span class=n>loss</span> <span class=o>+=</span> <span class=n>freq</span> <span class=o>*</span> <span class=n>word_loss</span>
</span><span id=__span-160-6><a id=__codelineno-160-6 name=__codelineno-160-6 href=#__codelineno-160-6></a>    <span class=k>return</span> <span class=n>loss</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-161-1><a id=__codelineno-161-1 name=__codelineno-161-1 href=#__codelineno-161-1></a><span class=n>compute_loss</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-162-1><a id=__codelineno-162-1 name=__codelineno-162-1 href=#__codelineno-162-1></a>413.10377642940875
</span></code></pre></div></p> <p>各トークンのスコアを計算するのも非常に困難ではありません。各トークンを削除して得られるモデルの損失を計算するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-163-1><a id=__codelineno-163-1 name=__codelineno-163-1 href=#__codelineno-163-1></a><span class=kn>import</span><span class=w> </span><span class=nn>copy</span>
</span><span id=__span-163-2><a id=__codelineno-163-2 name=__codelineno-163-2 href=#__codelineno-163-2></a>
</span><span id=__span-163-3><a id=__codelineno-163-3 name=__codelineno-163-3 href=#__codelineno-163-3></a><span class=k>def</span><span class=w> </span><span class=nf>compute_scores</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-163-4><a id=__codelineno-163-4 name=__codelineno-163-4 href=#__codelineno-163-4></a>    <span class=n>scores</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-163-5><a id=__codelineno-163-5 name=__codelineno-163-5 href=#__codelineno-163-5></a>    <span class=n>model_loss</span> <span class=o>=</span> <span class=n>compute_loss</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-163-6><a id=__codelineno-163-6 name=__codelineno-163-6 href=#__codelineno-163-6></a>    <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-163-7><a id=__codelineno-163-7 name=__codelineno-163-7 href=#__codelineno-163-7></a>        <span class=c1># 長さ1のトークンは常に保持</span>
</span><span id=__span-163-8><a id=__codelineno-163-8 name=__codelineno-163-8 href=#__codelineno-163-8></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>token</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-163-9><a id=__codelineno-163-9 name=__codelineno-163-9 href=#__codelineno-163-9></a>            <span class=k>continue</span>
</span><span id=__span-163-10><a id=__codelineno-163-10 name=__codelineno-163-10 href=#__codelineno-163-10></a>        <span class=n>model_without_token</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-163-11><a id=__codelineno-163-11 name=__codelineno-163-11 href=#__codelineno-163-11></a>        <span class=n>_</span> <span class=o>=</span> <span class=n>model_without_token</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>token</span><span class=p>)</span>
</span><span id=__span-163-12><a id=__codelineno-163-12 name=__codelineno-163-12 href=#__codelineno-163-12></a>        <span class=n>scores</span><span class=p>[</span><span class=n>token</span><span class=p>]</span> <span class=o>=</span> <span class=n>compute_loss</span><span class=p>(</span><span class=n>model_without_token</span><span class=p>)</span> <span class=o>-</span> <span class=n>model_loss</span>
</span><span id=__span-163-13><a id=__codelineno-163-13 name=__codelineno-163-13 href=#__codelineno-163-13></a>    <span class=k>return</span> <span class=n>scores</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-164-1><a id=__codelineno-164-1 name=__codelineno-164-1 href=#__codelineno-164-1></a><span class=n>scores</span> <span class=o>=</span> <span class=n>compute_scores</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-164-2><a id=__codelineno-164-2 name=__codelineno-164-2 href=#__codelineno-164-2></a><span class=nb>print</span><span class=p>(</span><span class=n>scores</span><span class=p>[</span><span class=s2>&quot;ll&quot;</span><span class=p>])</span>
</span><span id=__span-164-3><a id=__codelineno-164-3 name=__codelineno-164-3 href=#__codelineno-164-3></a><span class=nb>print</span><span class=p>(</span><span class=n>scores</span><span class=p>[</span><span class=s2>&quot;his&quot;</span><span class=p>])</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-165-1><a id=__codelineno-165-1 name=__codelineno-165-1 href=#__codelineno-165-1></a>6.376412403623874
</span><span id=__span-165-2><a id=__codelineno-165-2 name=__codelineno-165-2 href=#__codelineno-165-2></a>0.0
</span></code></pre></div></p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>このアプローチは非常に非効率的なため、SentencePieceはトークンXなしのモデルの損失の近似を使用します：一から始める代わりに、残っている語彙でのトークンXをそのセグメンテーションに置き換えるだけです。このように、すべてのスコアをモデル損失と同時に一度に計算できます。</p> </div> <p>すべてが整ったので、最後に行う必要があることは、モデルで使用される特別なトークンを語彙に追加し、望ましいサイズに達するまで語彙から十分なトークンを剪定するまでループすることです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-166-1><a id=__codelineno-166-1 name=__codelineno-166-1 href=#__codelineno-166-1></a><span class=n>percent_to_remove</span> <span class=o>=</span> <span class=mf>0.1</span>
</span><span id=__span-166-2><a id=__codelineno-166-2 name=__codelineno-166-2 href=#__codelineno-166-2></a><span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>100</span><span class=p>:</span>
</span><span id=__span-166-3><a id=__codelineno-166-3 name=__codelineno-166-3 href=#__codelineno-166-3></a>    <span class=n>scores</span> <span class=o>=</span> <span class=n>compute_scores</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-166-4><a id=__codelineno-166-4 name=__codelineno-166-4 href=#__codelineno-166-4></a>    <span class=n>sorted_scores</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span><span id=__span-166-5><a id=__codelineno-166-5 name=__codelineno-166-5 href=#__codelineno-166-5></a>    <span class=c1># 最低スコアのpercent_to_removeトークンを削除</span>
</span><span id=__span-166-6><a id=__codelineno-166-6 name=__codelineno-166-6 href=#__codelineno-166-6></a>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=o>*</span> <span class=n>percent_to_remove</span><span class=p>)):</span>
</span><span id=__span-166-7><a id=__codelineno-166-7 name=__codelineno-166-7 href=#__codelineno-166-7></a>        <span class=n>_</span> <span class=o>=</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>sorted_scores</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>])</span>
</span><span id=__span-166-8><a id=__codelineno-166-8 name=__codelineno-166-8 href=#__codelineno-166-8></a>
</span><span id=__span-166-9><a id=__codelineno-166-9 name=__codelineno-166-9 href=#__codelineno-166-9></a>    <span class=n>total_sum</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>([</span><span class=n>freq</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()])</span>
</span><span id=__span-166-10><a id=__codelineno-166-10 name=__codelineno-166-10 href=#__codelineno-166-10></a>    <span class=n>model</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span> <span class=o>-</span><span class=n>log</span><span class=p>(</span><span class=n>freq</span> <span class=o>/</span> <span class=n>total_sum</span><span class=p>)</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></code></pre></div> <p>次に、テキストをトークン化するには、前処理を適用してから<code>encode_word()</code>関数を使用するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-167-1><a id=__codelineno-167-1 name=__codelineno-167-1 href=#__codelineno-167-1></a><span class=k>def</span><span class=w> </span><span class=nf>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>model</span><span class=p>):</span>
</span><span id=__span-167-2><a id=__codelineno-167-2 name=__codelineno-167-2 href=#__codelineno-167-2></a>    <span class=n>words_with_offsets</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>backend_tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-167-3><a id=__codelineno-167-3 name=__codelineno-167-3 href=#__codelineno-167-3></a>    <span class=n>pre_tokenized_text</span> <span class=o>=</span> <span class=p>[</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>offset</span> <span class=ow>in</span> <span class=n>words_with_offsets</span><span class=p>]</span>
</span><span id=__span-167-4><a id=__codelineno-167-4 name=__codelineno-167-4 href=#__codelineno-167-4></a>    <span class=n>encoded_words</span> <span class=o>=</span> <span class=p>[</span><span class=n>encode_word</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>model</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>pre_tokenized_text</span><span class=p>]</span>
</span><span id=__span-167-5><a id=__codelineno-167-5 name=__codelineno-167-5 href=#__codelineno-167-5></a>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>encoded_words</span><span class=p>,</span> <span class=p>[])</span>
</span><span id=__span-167-6><a id=__codelineno-167-6 name=__codelineno-167-6 href=#__codelineno-167-6></a>
</span><span id=__span-167-7><a id=__codelineno-167-7 name=__codelineno-167-7 href=#__codelineno-167-7></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenize</span><span class=p>(</span><span class=s2>&quot;This is the Hugging Face course.&quot;</span><span class=p>,</span> <span class=n>model</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-168-1><a id=__codelineno-168-1 name=__codelineno-168-1 href=#__codelineno-168-1></a>[&#39;▁This&#39;, &#39;▁is&#39;, &#39;▁the&#39;, &#39;▁Hugging&#39;, &#39;▁Face&#39;, &#39;▁&#39;, &#39;c&#39;, &#39;ou&#39;, &#39;r&#39;, &#39;s&#39;, &#39;e&#39;, &#39;.&#39;]
</span></code></pre></div></p> <div class="admonition tip"> <p class=admonition-title>Tip</p> <p>XLNetTokenizerはSentencePieceを使用するため、<code>"_"</code>文字が含まれています。SentencePieceでデコードするには、すべてのトークンを連結して<code>"_"</code>をスペースに置き換えます。</p> </div> <h2 id=_24>ブロックごとのトークナイザー構築<a class=headerlink href=#_24 title="Permanent link">&para;</a></h2> <p>前のセクションで見たように、トークン化にはいくつかのステップが含まれます：</p> <ul> <li>正規化（必要と思われるテキストのクリーンアップ、スペースやアクセント記号の除去、Unicode正規化など）</li> <li>前処理（入力を単語に分割）</li> <li>モデルを通じて入力を実行（前処理された単語を使用してトークンのシーケンスを生成）</li> <li>後処理（トークナイザーの特別なトークンの追加、アテンションマスクとトークンタイプIDの生成）</li> </ul> <p>思い出すために、全体的なプロセスをもう一度見てみましょう：</p> <p><img alt=トークン化パイプライン src=../04_the_huggingface_tokenizers_library_files/tokenization_pipeline.png></p> <p>Hugging Face Tokenizersライブラリは、これらの各ステップに対して複数のオプションを提供するように構築されており、それらを組み合わせることができます。このセクションでは、古いものから新しいトークナイザーを訓練するのではなく、一からトークナイザーを構築する方法を見ていきます。思い付くあらゆる種類のトークナイザーを構築できるようになります。</p> <p>より正確には、ライブラリは中心となる<code>Tokenizer</code>クラスを中心に構築されており、構成要素がサブモジュールにまとめられています：</p> <ul> <li><code>normalizers</code>は使用できる<code>Normalizer</code>のすべての可能なタイプを含みます（完全なリストは<a href=https://huggingface.co/docs/tokenizers/api/normalizers>ここ</a>）。</li> <li><code>pre_tokenizers</code>は使用できる<code>PreTokenizer</code>のすべての可能なタイプを含みます（完全なリストは<a href=https://huggingface.co/docs/tokenizers/api/pre-tokenizers>ここ</a>）。</li> <li><code>models</code>は<code>BPE</code>、<code>WordPiece</code>、<code>Unigram</code>などの使用できる様々なタイプの<code>Model</code>を含みます（完全なリストは<a href=https://huggingface.co/docs/tokenizers/api/models>ここ</a>）。</li> <li><code>trainers</code>はコーパス上でモデルを訓練するために使用できるすべての異なるタイプの<code>Trainer</code>を含みます（モデルのタイプごとに1つ；完全なリストは<a href=https://huggingface.co/docs/tokenizers/api/trainers>ここ</a>）。</li> <li><code>post_processors</code>は使用できる様々なタイプの<code>PostProcessor</code>を含みます（完全なリストは<a href=https://huggingface.co/docs/tokenizers/api/post-processors>ここ</a>）。</li> <li><code>decoders</code>はトークン化の出力をデコードするために使用できる様々なタイプの<code>Decoder</code>を含みます（完全なリストは<a href=https://huggingface.co/docs/tokenizers/components#decoders>ここ</a>）。</li> </ul> <p>構成要素の完全なリストは<a href=https://huggingface.co/docs/tokenizers/components>ここ</a>で見つけることができます。</p> <h3 id=_25>コーパスの取得<a class=headerlink href=#_25 title="Permanent link">&para;</a></h3> <p>新しいトークナイザーを訓練するために、テキストの小さなコーパスを使用します（例が高速に実行されるように）。コーパスを取得するステップは、この章の最初に取ったものと似ていますが、今回は<a href=https://huggingface.co/datasets/wikitext>WikiText-2</a>データセットを使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-169-1><a id=__codelineno-169-1 name=__codelineno-169-1 href=#__codelineno-169-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-169-2><a id=__codelineno-169-2 name=__codelineno-169-2 href=#__codelineno-169-2></a>
</span><span id=__span-169-3><a id=__codelineno-169-3 name=__codelineno-169-3 href=#__codelineno-169-3></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&quot;wikitext&quot;</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;wikitext-2-raw-v1&quot;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&quot;train&quot;</span><span class=p>)</span>
</span><span id=__span-169-4><a id=__codelineno-169-4 name=__codelineno-169-4 href=#__codelineno-169-4></a>
</span><span id=__span-169-5><a id=__codelineno-169-5 name=__codelineno-169-5 href=#__codelineno-169-5></a><span class=k>def</span><span class=w> </span><span class=nf>get_training_corpus</span><span class=p>():</span>
</span><span id=__span-169-6><a id=__codelineno-169-6 name=__codelineno-169-6 href=#__codelineno-169-6></a>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>),</span> <span class=mi>1000</span><span class=p>):</span>
</span><span id=__span-169-7><a id=__codelineno-169-7 name=__codelineno-169-7 href=#__codelineno-169-7></a>        <span class=k>yield</span> <span class=n>dataset</span><span class=p>[</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1000</span><span class=p>][</span><span class=s2>&quot;text&quot;</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-170-1><a id=__codelineno-170-1 name=__codelineno-170-1 href=#__codelineno-170-1></a>Generating test split: 100%|██████████| 4358/4358 [00:00&lt;00:00, 780910.70 examples/s]
</span><span id=__span-170-2><a id=__codelineno-170-2 name=__codelineno-170-2 href=#__codelineno-170-2></a>Generating train split: 100%|██████████| 36718/36718 [00:00&lt;00:00, 2049920.86 examples/s]
</span><span id=__span-170-3><a id=__codelineno-170-3 name=__codelineno-170-3 href=#__codelineno-170-3></a>Generating validation split: 100%|██████████| 3760/3760 [00:00&lt;00:00, 1191401.60 examples/s]
</span></code></pre></div></p> <p>関数<code>get_training_corpus()</code>は、トークナイザーを訓練するために使用する1,000テキストのバッチを生成するジェネレータです。</p> <p>Hugging Face Tokenizersは、テキストファイルでも直接訓練できます。WikiText-2からのすべてのテキスト/入力を含むテキストファイルをローカルで使用できるように生成する方法は次のとおりです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-171-1><a id=__codelineno-171-1 name=__codelineno-171-1 href=#__codelineno-171-1></a><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&quot;wikitext-2.txt&quot;</span><span class=p>,</span> <span class=s2>&quot;w&quot;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&quot;utf-8&quot;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-171-2><a id=__codelineno-171-2 name=__codelineno-171-2 href=#__codelineno-171-2></a>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)):</span>
</span><span id=__span-171-3><a id=__codelineno-171-3 name=__codelineno-171-3 href=#__codelineno-171-3></a>        <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>dataset</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=s2>&quot;text&quot;</span><span class=p>]</span> <span class=o>+</span> <span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>次に、独自のBERT、GPT-2、XLNetトークナイザーをブロックごとに構築する方法を示します。これにより、3つの主要なトークン化アルゴリズムのそれぞれの例が得られます：WordPiece、BPE、Unigram。BERTから始めましょう！</p> <h3 id=wordpiece_2>WordPieceトークナイザーを一から構築<a class=headerlink href=#wordpiece_2 title="Permanent link">&para;</a></h3> <p>Hugging Face Tokenizersライブラリでトークナイザーを構築するには、<code>model</code>で<code>Tokenizer</code>オブジェクトをインスタンス化してから、その<code>normalizer</code>、<code>pre_tokenizer</code>、<code>post_processor</code>、<code>decoder</code>属性を欲しい値に設定します。</p> <p>この例では、WordPieceモデルで<code>Tokenizer</code>を作成します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-172-1><a id=__codelineno-172-1 name=__codelineno-172-1 href=#__codelineno-172-1></a><span class=kn>from</span><span class=w> </span><span class=nn>tokenizers</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
</span><span id=__span-172-2><a id=__codelineno-172-2 name=__codelineno-172-2 href=#__codelineno-172-2></a>    <span class=n>decoders</span><span class=p>,</span>
</span><span id=__span-172-3><a id=__codelineno-172-3 name=__codelineno-172-3 href=#__codelineno-172-3></a>    <span class=n>models</span><span class=p>,</span>
</span><span id=__span-172-4><a id=__codelineno-172-4 name=__codelineno-172-4 href=#__codelineno-172-4></a>    <span class=n>normalizers</span><span class=p>,</span>
</span><span id=__span-172-5><a id=__codelineno-172-5 name=__codelineno-172-5 href=#__codelineno-172-5></a>    <span class=n>pre_tokenizers</span><span class=p>,</span>
</span><span id=__span-172-6><a id=__codelineno-172-6 name=__codelineno-172-6 href=#__codelineno-172-6></a>    <span class=n>processors</span><span class=p>,</span>
</span><span id=__span-172-7><a id=__codelineno-172-7 name=__codelineno-172-7 href=#__codelineno-172-7></a>    <span class=n>trainers</span><span class=p>,</span>
</span><span id=__span-172-8><a id=__codelineno-172-8 name=__codelineno-172-8 href=#__codelineno-172-8></a>    <span class=n>Tokenizer</span>
</span><span id=__span-172-9><a id=__codelineno-172-9 name=__codelineno-172-9 href=#__codelineno-172-9></a><span class=p>)</span>
</span><span id=__span-172-10><a id=__codelineno-172-10 name=__codelineno-172-10 href=#__codelineno-172-10></a>
</span><span id=__span-172-11><a id=__codelineno-172-11 name=__codelineno-172-11 href=#__codelineno-172-11></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>models</span><span class=o>.</span><span class=n>WordPiece</span><span class=p>(</span><span class=n>unk_token</span><span class=o>=</span><span class=s2>&quot;[UNK]&quot;</span><span class=p>))</span>
</span></code></pre></div> <p>モデルが以前に見たことのない文字に遭遇したときに何を返すかがわかるように、<code>unk_token</code>を指定する必要があります。ここで設定できる他の引数には、モデルの<code>vocab</code>（モデルを訓練するつもりなので、これを設定する必要はありません）や、各単語の最大長を指定する<code>max_input_chars_per_word</code>があります（渡された値より長い単語は分割されます）。</p> <p>トークン化の最初のステップは正規化なので、それから始めましょう。BERTは広く使用されているため、BERTに設定できる古典的なオプションを持つ<code>BertNormalizer</code>があります：<code>lowercase</code>と<code>strip_accents</code>は自明で、<code>clean_text</code>はすべての制御文字を削除し、繰り返しスペースを1つに置き換え、<code>handle_chinese_chars</code>は中国語文字の周りにスペースを配置します。<code>bert-base-uncased</code>トークナイザーを複製するために、この正規化器を設定できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-173-1><a id=__codelineno-173-1 name=__codelineno-173-1 href=#__codelineno-173-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>normalizer</span> <span class=o>=</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>BertNormalizer</span><span class=p>(</span><span class=n>lowercase</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <p>しかし、一般的に言えば、新しいトークナイザーを構築する際、Hugging Face Tokenizersライブラリで既に実装されているそのような便利な正規化器にアクセスできない場合があります。手でBERT正規化器を作成する方法を見てみましょう。ライブラリは<code>Lowercase</code>正規化器と<code>StripAccents</code>正規化器を提供し、<code>Sequence</code>を使用して複数の正規化器を構成できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-174-1><a id=__codelineno-174-1 name=__codelineno-174-1 href=#__codelineno-174-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>normalizer</span> <span class=o>=</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>Sequence</span><span class=p>(</span>
</span><span id=__span-174-2><a id=__codelineno-174-2 name=__codelineno-174-2 href=#__codelineno-174-2></a>    <span class=p>[</span><span class=n>normalizers</span><span class=o>.</span><span class=n>NFD</span><span class=p>(),</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>Lowercase</span><span class=p>(),</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>StripAccents</span><span class=p>()]</span>
</span><span id=__span-174-3><a id=__codelineno-174-3 name=__codelineno-174-3 href=#__codelineno-174-3></a><span class=p>)</span>
</span></code></pre></div> <p>また、<code>StripAccents</code>正規化器がアクセント付き文字を適切に認識してそれらを削除しないため、<code>NFD</code> Unicode正規化器も使用しています。</p> <p>前述のとおり、<code>normalizer</code>の<code>normalize_str()</code>メソッドを使用して、特定のテキストに対する効果を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-175-1><a id=__codelineno-175-1 name=__codelineno-175-1 href=#__codelineno-175-1></a><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>normalizer</span><span class=o>.</span><span class=n>normalize_str</span><span class=p>(</span><span class=s2>&quot;Héllò hôw are ü?&quot;</span><span class=p>))</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-176-1><a id=__codelineno-176-1 name=__codelineno-176-1 href=#__codelineno-176-1></a>hello how are u?
</span></code></pre></div></p> <div class="admonition tip"> <p class=admonition-title>正規化器の同等性について</p> <p>文字列<code>u"\u0085"</code>を含む文字列で前の正規化器の2つのバージョンをテストする場合、これら2つの正規化器は完全に同等ではないことに気付くでしょう。<code>normalizers.Sequence</code>のバージョンをあまり複雑にしないために、<code>clean_text</code>引数が<code>True</code>に設定されている場合（デフォルトの動作）に<code>BertNormalizer</code>が必要とするRegex置換を含めていません。しかし心配しないでください：便利な<code>BertNormalizer</code>を使用せずにまったく同じ正規化を得ることは可能で、正規化器シーケンスに2つの<code>normalizers.Replace</code>を追加するだけです。</p> </div> <p>次は前処理ステップです。ここでも、使用できる事前構築された<code>BertPreTokenizer</code>があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-177-1><a id=__codelineno-177-1 name=__codelineno-177-1 href=#__codelineno-177-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>BertPreTokenizer</span><span class=p>()</span>
</span></code></pre></div> <p>または、一から構築できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-178-1><a id=__codelineno-178-1 name=__codelineno-178-1 href=#__codelineno-178-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>Whitespace</span><span class=p>()</span>
</span></code></pre></div> <p><code>Whitespace</code>前処理器は空白およびすべての文字、数字、アンダースコア文字以外の文字で分割するため、技術的には空白と句読点で分割します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-179-1><a id=__codelineno-179-1 name=__codelineno-179-1 href=#__codelineno-179-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test my pre-tokenizer.&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-180-1><a id=__codelineno-180-1 name=__codelineno-180-1 href=#__codelineno-180-1></a>[(&#39;Let&#39;, (0, 3)),
</span><span id=__span-180-2><a id=__codelineno-180-2 name=__codelineno-180-2 href=#__codelineno-180-2></a> (&quot;&#39;&quot;, (3, 4)),
</span><span id=__span-180-3><a id=__codelineno-180-3 name=__codelineno-180-3 href=#__codelineno-180-3></a> (&#39;s&#39;, (4, 5)),
</span><span id=__span-180-4><a id=__codelineno-180-4 name=__codelineno-180-4 href=#__codelineno-180-4></a> (&#39;test&#39;, (6, 10)),
</span><span id=__span-180-5><a id=__codelineno-180-5 name=__codelineno-180-5 href=#__codelineno-180-5></a> (&#39;my&#39;, (11, 13)),
</span><span id=__span-180-6><a id=__codelineno-180-6 name=__codelineno-180-6 href=#__codelineno-180-6></a> (&#39;pre&#39;, (14, 17)),
</span><span id=__span-180-7><a id=__codelineno-180-7 name=__codelineno-180-7 href=#__codelineno-180-7></a> (&#39;-&#39;, (17, 18)),
</span><span id=__span-180-8><a id=__codelineno-180-8 name=__codelineno-180-8 href=#__codelineno-180-8></a> (&#39;tokenizer&#39;, (18, 27)),
</span><span id=__span-180-9><a id=__codelineno-180-9 name=__codelineno-180-9 href=#__codelineno-180-9></a> (&#39;.&#39;, (27, 28))]
</span></code></pre></div></p> <p>空白でのみ分割したい場合は、代わりに<code>WhitespaceSplit</code>前処理器を使用する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-181-1><a id=__codelineno-181-1 name=__codelineno-181-1 href=#__codelineno-181-1></a><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>WhitespaceSplit</span><span class=p>()</span>
</span><span id=__span-181-2><a id=__codelineno-181-2 name=__codelineno-181-2 href=#__codelineno-181-2></a><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test my pre-tokenizer.&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-182-1><a id=__codelineno-182-1 name=__codelineno-182-1 href=#__codelineno-182-1></a>[(&quot;Let&#39;s&quot;, (0, 5)),
</span><span id=__span-182-2><a id=__codelineno-182-2 name=__codelineno-182-2 href=#__codelineno-182-2></a> (&#39;test&#39;, (6, 10)),
</span><span id=__span-182-3><a id=__codelineno-182-3 name=__codelineno-182-3 href=#__codelineno-182-3></a> (&#39;my&#39;, (11, 13)),
</span><span id=__span-182-4><a id=__codelineno-182-4 name=__codelineno-182-4 href=#__codelineno-182-4></a> (&#39;pre-tokenizer.&#39;, (14, 28))]
</span></code></pre></div></p> <p>正規化器と同様に、<code>Sequence</code>を使用して複数の前処理器を構成できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-183-1><a id=__codelineno-183-1 name=__codelineno-183-1 href=#__codelineno-183-1></a><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>Sequence</span><span class=p>(</span>
</span><span id=__span-183-2><a id=__codelineno-183-2 name=__codelineno-183-2 href=#__codelineno-183-2></a>    <span class=p>[</span><span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>WhitespaceSplit</span><span class=p>(),</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>Punctuation</span><span class=p>()]</span>
</span><span id=__span-183-3><a id=__codelineno-183-3 name=__codelineno-183-3 href=#__codelineno-183-3></a><span class=p>)</span>
</span><span id=__span-183-4><a id=__codelineno-183-4 name=__codelineno-183-4 href=#__codelineno-183-4></a><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test my pre-tokenizer.&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-184-1><a id=__codelineno-184-1 name=__codelineno-184-1 href=#__codelineno-184-1></a>[(&#39;Let&#39;, (0, 3)),
</span><span id=__span-184-2><a id=__codelineno-184-2 name=__codelineno-184-2 href=#__codelineno-184-2></a> (&quot;&#39;&quot;, (3, 4)),
</span><span id=__span-184-3><a id=__codelineno-184-3 name=__codelineno-184-3 href=#__codelineno-184-3></a> (&#39;s&#39;, (4, 5)),
</span><span id=__span-184-4><a id=__codelineno-184-4 name=__codelineno-184-4 href=#__codelineno-184-4></a> (&#39;test&#39;, (6, 10)),
</span><span id=__span-184-5><a id=__codelineno-184-5 name=__codelineno-184-5 href=#__codelineno-184-5></a> (&#39;my&#39;, (11, 13)),
</span><span id=__span-184-6><a id=__codelineno-184-6 name=__codelineno-184-6 href=#__codelineno-184-6></a> (&#39;pre&#39;, (14, 17)),
</span><span id=__span-184-7><a id=__codelineno-184-7 name=__codelineno-184-7 href=#__codelineno-184-7></a> (&#39;-&#39;, (17, 18)),
</span><span id=__span-184-8><a id=__codelineno-184-8 name=__codelineno-184-8 href=#__codelineno-184-8></a> (&#39;tokenizer&#39;, (18, 27)),
</span><span id=__span-184-9><a id=__codelineno-184-9 name=__codelineno-184-9 href=#__codelineno-184-9></a> (&#39;.&#39;, (27, 28))]
</span></code></pre></div></p> <p>トークン化パイプラインの次のステップは、モデルを通じて入力を実行することです。初期化でモデルを既に指定しましたが、まだ訓練する必要があります。これには<code>WordPieceTrainer</code>が必要です。Hugging Face Tokenizersでトレーナーをインスタンス化する際に覚えておく必要がある主なことは、使用するつもりのすべての特別なトークンを渡す必要があることです。そうしないと、訓練コーパスにないため、語彙に追加されません：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-185-1><a id=__codelineno-185-1 name=__codelineno-185-1 href=#__codelineno-185-1></a><span class=n>special_tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;[UNK]&quot;</span><span class=p>,</span> <span class=s2>&quot;[PAD]&quot;</span><span class=p>,</span> <span class=s2>&quot;[CLS]&quot;</span><span class=p>,</span> <span class=s2>&quot;[SEP]&quot;</span><span class=p>,</span> <span class=s2>&quot;[MASK]&quot;</span><span class=p>]</span>
</span><span id=__span-185-2><a id=__codelineno-185-2 name=__codelineno-185-2 href=#__codelineno-185-2></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>trainers</span><span class=o>.</span><span class=n>WordPieceTrainer</span><span class=p>(</span><span class=n>vocab_size</span><span class=o>=</span><span class=mi>25000</span><span class=p>,</span> <span class=n>special_tokens</span><span class=o>=</span><span class=n>special_tokens</span><span class=p>)</span>
</span></code></pre></div> <p><code>vocab_size</code>と<code>special_tokens</code>の指定に加えて、<code>min_frequency</code>（語彙に含まれるためにトークンが出現する必要がある回数）を設定したり、<code>continuing_subword_prefix</code>を変更したりできます（<code>##</code>以外の何かを使用したい場合）。</p> <p>前に定義したイテレータを使用してモデルを訓練するには、このコマンドを実行するだけです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-186-1><a id=__codelineno-186-1 name=__codelineno-186-1 href=#__codelineno-186-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>train_from_iterator</span><span class=p>(</span><span class=n>get_training_corpus</span><span class=p>(),</span> <span class=n>trainer</span><span class=o>=</span><span class=n>trainer</span><span class=p>)</span>
</span></code></pre></div> <p>テキストファイルを使用してトークナイザーを訓練することもできます。これは次のようになります（事前に空の<code>WordPiece</code>でモデルを再初期化します）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-187-1><a id=__codelineno-187-1 name=__codelineno-187-1 href=#__codelineno-187-1></a><span class=c1># tokenizer.model = models.WordPiece(unk_token=&quot;[UNK]&quot;)</span>
</span><span id=__span-187-2><a id=__codelineno-187-2 name=__codelineno-187-2 href=#__codelineno-187-2></a><span class=c1># tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)</span>
</span></code></pre></div> <p>どちらの場合でも、<code>encode()</code>メソッドを呼び出してテキストでトークナイザーをテストできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-188-1><a id=__codelineno-188-1 name=__codelineno-188-1 href=#__codelineno-188-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer.&quot;</span><span class=p>)</span>
</span><span id=__span-188-2><a id=__codelineno-188-2 name=__codelineno-188-2 href=#__codelineno-188-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-189-1><a id=__codelineno-189-1 name=__codelineno-189-1 href=#__codelineno-189-1></a>[&#39;[CLS]&#39;, &#39;let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;test&#39;, &#39;this&#39;, &#39;tok&#39;, &#39;##eni&#39;, &#39;##zer&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span></code></pre></div></p> <p>取得される<code>encoding</code>は<code>Encoding</code>で、様々な属性でトークナイザーの必要なすべての出力を含んでいます：<code>ids</code>、<code>type_ids</code>、<code>tokens</code>、<code>offsets</code>、<code>attention_mask</code>、<code>special_tokens_mask</code>、<code>overflowing</code>。</p> <p>トークン化パイプラインの最後のステップは後処理です。最初に<code>[CLS]</code>トークンを追加し、最後に<code>[SEP]</code>トークンを追加する必要があります（または文のペアがある場合は各文の後）。これには<code>TemplateProcessor</code>を使用しますが、まず語彙内の<code>[CLS]</code>と<code>[SEP]</code>トークンのIDを知る必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-190-1><a id=__codelineno-190-1 name=__codelineno-190-1 href=#__codelineno-190-1></a><span class=n>cls_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>token_to_id</span><span class=p>(</span><span class=s2>&quot;[CLS]&quot;</span><span class=p>)</span>
</span><span id=__span-190-2><a id=__codelineno-190-2 name=__codelineno-190-2 href=#__codelineno-190-2></a><span class=n>sep_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>token_to_id</span><span class=p>(</span><span class=s2>&quot;[SEP]&quot;</span><span class=p>)</span>
</span><span id=__span-190-3><a id=__codelineno-190-3 name=__codelineno-190-3 href=#__codelineno-190-3></a><span class=nb>print</span><span class=p>(</span><span class=n>cls_token_id</span><span class=p>,</span> <span class=n>sep_token_id</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-191-1><a id=__codelineno-191-1 name=__codelineno-191-1 href=#__codelineno-191-1></a>2 3
</span></code></pre></div></p> <p><code>TemplateProcessor</code>のテンプレートを書くには、単一文と文のペアをどのように扱うかを指定する必要があります。両方について、使用したい特別なトークンを書きます；最初の（または単一の）文は<code>$A</code>で表され、2番目の文（ペアをエンコードする場合）は<code>$B</code>で表されます。これらのそれぞれ（特別なトークンと文）について、コロンの後に対応するトークンタイプIDも指定します。</p> <p>典型的なBERTテンプレートは次のように定義されます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-192-1><a id=__codelineno-192-1 name=__codelineno-192-1 href=#__codelineno-192-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>post_processor</span> <span class=o>=</span> <span class=n>processors</span><span class=o>.</span><span class=n>TemplateProcessing</span><span class=p>(</span>
</span><span id=__span-192-2><a id=__codelineno-192-2 name=__codelineno-192-2 href=#__codelineno-192-2></a>    <span class=n>single</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span><span class=p>,</span>
</span><span id=__span-192-3><a id=__codelineno-192-3 name=__codelineno-192-3 href=#__codelineno-192-3></a>    <span class=n>pair</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span><span class=p>,</span>
</span><span id=__span-192-4><a id=__codelineno-192-4 name=__codelineno-192-4 href=#__codelineno-192-4></a>    <span class=n>special_tokens</span><span class=o>=</span><span class=p>[(</span><span class=s2>&quot;[CLS]&quot;</span><span class=p>,</span> <span class=n>cls_token_id</span><span class=p>),</span> <span class=p>(</span><span class=s2>&quot;[SEP]&quot;</span><span class=p>,</span> <span class=n>sep_token_id</span><span class=p>)],</span>
</span><span id=__span-192-5><a id=__codelineno-192-5 name=__codelineno-192-5 href=#__codelineno-192-5></a><span class=p>)</span>
</span></code></pre></div> <p>トークナイザーが特別なトークンを適切にそのIDに変換できるように、特別なトークンのIDを渡す必要があることに注意してください。</p> <p>これが追加されると、前の例に戻ると次が得られます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-193-1><a id=__codelineno-193-1 name=__codelineno-193-1 href=#__codelineno-193-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer.&quot;</span><span class=p>)</span>
</span><span id=__span-193-2><a id=__codelineno-193-2 name=__codelineno-193-2 href=#__codelineno-193-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-194-1><a id=__codelineno-194-1 name=__codelineno-194-1 href=#__codelineno-194-1></a>[&#39;[CLS]&#39;, &#39;let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;test&#39;, &#39;this&#39;, &#39;tok&#39;, &#39;##eni&#39;, &#39;##zer&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span></code></pre></div></p> <p>文のペアでは、適切な結果が得られます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-195-1><a id=__codelineno-195-1 name=__codelineno-195-1 href=#__codelineno-195-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer...&quot;</span><span class=p>,</span> <span class=s2>&quot;on a pair of sentences.&quot;</span><span class=p>)</span>
</span><span id=__span-195-2><a id=__codelineno-195-2 name=__codelineno-195-2 href=#__codelineno-195-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=p>)</span>
</span><span id=__span-195-3><a id=__codelineno-195-3 name=__codelineno-195-3 href=#__codelineno-195-3></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-195-4><a id=__codelineno-195-4 name=__codelineno-195-4 href=#__codelineno-195-4></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>type_ids</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-196-1><a id=__codelineno-196-1 name=__codelineno-196-1 href=#__codelineno-196-1></a>Encoding(num_tokens=18, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
</span><span id=__span-196-2><a id=__codelineno-196-2 name=__codelineno-196-2 href=#__codelineno-196-2></a>[&#39;[CLS]&#39;, &#39;let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;test&#39;, &#39;this&#39;, &#39;tok&#39;, &#39;##eni&#39;, &#39;##zer&#39;, &#39;...&#39;, &#39;[SEP]&#39;, &#39;on&#39;, &#39;a&#39;, &#39;pair&#39;, &#39;of&#39;, &#39;sentences&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span><span id=__span-196-3><a id=__codelineno-196-3 name=__codelineno-196-3 href=#__codelineno-196-3></a>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
</span></code></pre></div></p> <p>このトークナイザーを一から構築するのはほぼ終了しました。最後のステップはデコーダーを含めることです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-197-1><a id=__codelineno-197-1 name=__codelineno-197-1 href=#__codelineno-197-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoders</span><span class=o>.</span><span class=n>WordPiece</span><span class=p>(</span><span class=n>prefix</span><span class=o>=</span><span class=s2>&quot;##&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>前の<code>encoding</code>でテストしてみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-198-1><a id=__codelineno-198-1 name=__codelineno-198-1 href=#__codelineno-198-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>ids</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-199-1><a id=__codelineno-199-1 name=__codelineno-199-1 href=#__codelineno-199-1></a>&quot;let &#39; s test this tokenizer... on a pair of sentences.&quot;
</span></code></pre></div></p> <p>素晴らしい！このように単一のJSONファイルでトークナイザーを保存できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-200-1><a id=__codelineno-200-1 name=__codelineno-200-1 href=#__codelineno-200-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&quot;tokenizer.json&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>その後、<code>from_file()</code>メソッドで<code>Tokenizer</code>オブジェクトでそのファイルを再読み込みできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-201-1><a id=__codelineno-201-1 name=__codelineno-201-1 href=#__codelineno-201-1></a><span class=n>new_tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=o>.</span><span class=n>from_file</span><span class=p>(</span><span class=s2>&quot;tokenizer.json&quot;</span><span class=p>)</span>
</span><span id=__span-201-2><a id=__codelineno-201-2 name=__codelineno-201-2 href=#__codelineno-201-2></a>
</span><span id=__span-201-3><a id=__codelineno-201-3 name=__codelineno-201-3 href=#__codelineno-201-3></a><span class=n>new_encoding</span> <span class=o>=</span> <span class=n>new_tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer...&quot;</span><span class=p>,</span> <span class=s2>&quot;on a pair of sentences.&quot;</span><span class=p>)</span>
</span><span id=__span-201-4><a id=__codelineno-201-4 name=__codelineno-201-4 href=#__codelineno-201-4></a>
</span><span id=__span-201-5><a id=__codelineno-201-5 name=__codelineno-201-5 href=#__codelineno-201-5></a><span class=nb>print</span><span class=p>(</span><span class=n>new_encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-202-1><a id=__codelineno-202-1 name=__codelineno-202-1 href=#__codelineno-202-1></a>[&#39;[CLS]&#39;, &#39;let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;test&#39;, &#39;this&#39;, &#39;tok&#39;, &#39;##eni&#39;, &#39;##zer&#39;, &#39;...&#39;, &#39;[SEP]&#39;, &#39;on&#39;, &#39;a&#39;, &#39;pair&#39;, &#39;of&#39;, &#39;sentences&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</span></code></pre></div></p> <p>Hugging Face Transformersでこのトークナイザーを使用するには、<code>PreTrainedTokenizerFast</code>でラップする必要があります。汎用クラスを使用するか、トークナイザーが既存のモデルに対応する場合はそのクラスを使用できます（ここでは<code>BertTokenizerFast</code>）。まったく新しいトークナイザーを構築するためにこのレッスンを適用する場合は、最初のオプションを使用する必要があります。</p> <p><code>PreTrainedTokenizerFast</code>でトークナイザーをラップするには、構築したトークナイザーを<code>tokenizer_object</code>として渡すか、保存したトークナイザーファイルを<code>tokenizer_file</code>として渡すことができます。覚えておく必要がある重要なことは、そのクラスが<code>tokenizer</code>オブジェクトからどのトークンがマスクトークンで、<code>[CLS]</code>トークンなどかを推測できないため、すべての特別なトークンを手動で設定する必要があることです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-203-1><a id=__codelineno-203-1 name=__codelineno-203-1 href=#__codelineno-203-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>PreTrainedTokenizerFast</span>
</span><span id=__span-203-2><a id=__codelineno-203-2 name=__codelineno-203-2 href=#__codelineno-203-2></a>
</span><span id=__span-203-3><a id=__codelineno-203-3 name=__codelineno-203-3 href=#__codelineno-203-3></a><span class=n>wrapped_tokenizer</span> <span class=o>=</span> <span class=n>PreTrainedTokenizerFast</span><span class=p>(</span>
</span><span id=__span-203-4><a id=__codelineno-203-4 name=__codelineno-203-4 href=#__codelineno-203-4></a>    <span class=n>tokenizer_object</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-203-5><a id=__codelineno-203-5 name=__codelineno-203-5 href=#__codelineno-203-5></a>    <span class=c1># tokenizer_file=&quot;tokenizer.json&quot;, # または、トークナイザーファイルから読み込み</span>
</span><span id=__span-203-6><a id=__codelineno-203-6 name=__codelineno-203-6 href=#__codelineno-203-6></a>    <span class=n>unk_token</span><span class=o>=</span><span class=s2>&quot;[UNK]&quot;</span><span class=p>,</span>
</span><span id=__span-203-7><a id=__codelineno-203-7 name=__codelineno-203-7 href=#__codelineno-203-7></a>    <span class=n>pad_token</span><span class=o>=</span><span class=s2>&quot;[PAD]&quot;</span><span class=p>,</span>
</span><span id=__span-203-8><a id=__codelineno-203-8 name=__codelineno-203-8 href=#__codelineno-203-8></a>    <span class=n>cls_token</span><span class=o>=</span><span class=s2>&quot;[CLS]&quot;</span><span class=p>,</span>
</span><span id=__span-203-9><a id=__codelineno-203-9 name=__codelineno-203-9 href=#__codelineno-203-9></a>    <span class=n>sep_token</span><span class=o>=</span><span class=s2>&quot;[SEP]&quot;</span><span class=p>,</span>
</span><span id=__span-203-10><a id=__codelineno-203-10 name=__codelineno-203-10 href=#__codelineno-203-10></a>    <span class=n>mask_token</span><span class=o>=</span><span class=s2>&quot;[MASK]&quot;</span><span class=p>,</span>
</span><span id=__span-203-11><a id=__codelineno-203-11 name=__codelineno-203-11 href=#__codelineno-203-11></a><span class=p>)</span>
</span></code></pre></div> <p>特定のトークナイザークラス（<code>BertTokenizerFast</code>など）を使用している場合、デフォルトと異なる特別なトークンのみを指定する必要があります（ここでは何もありません）：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-204-1><a id=__codelineno-204-1 name=__codelineno-204-1 href=#__codelineno-204-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>BertTokenizerFast</span>
</span><span id=__span-204-2><a id=__codelineno-204-2 name=__codelineno-204-2 href=#__codelineno-204-2></a>
</span><span id=__span-204-3><a id=__codelineno-204-3 name=__codelineno-204-3 href=#__codelineno-204-3></a><span class=n>wrapped_tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizerFast</span><span class=p>(</span><span class=n>tokenizer_object</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></code></pre></div> <p>このトークナイザーを他のHugging Face Transformersトークナイザーと同じように使用できます。<code>save_pretrained()</code>メソッドで保存できます。</p> <h3 id=bpe_1>BPEトークナイザーを一から構築<a class=headerlink href=#bpe_1 title="Permanent link">&para;</a></h3> <p>GPT-2トークナイザーを構築してみましょう。BERTトークナイザーと同様に、BPEモデルで<code>Tokenizer</code>を初期化することから始めます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-205-1><a id=__codelineno-205-1 name=__codelineno-205-1 href=#__codelineno-205-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>models</span><span class=o>.</span><span class=n>BPE</span><span class=p>())</span>
</span></code></pre></div> <p>BERTと同様に、語彙がある場合はこのモデルを初期化できます（この場合は<code>vocab</code>と<code>merges</code>を渡す必要があります）が、一から訓練するため、これを行う必要はありません。GPT-2はバイトレベルBPEを使用するため、<code>unk_token</code>を指定する必要もありません。</p> <p>GPT-2は正規化器を使用しないため、そのステップをスキップして前処理に直接進みます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-206-1><a id=__codelineno-206-1 name=__codelineno-206-1 href=#__codelineno-206-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>ByteLevel</span><span class=p>(</span><span class=n>add_prefix_space</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></code></pre></div> <p>ここで<code>ByteLevel</code>に追加したオプションは、文の最初にスペースを追加しないことです（そうしないとデフォルトになります）。前と同様に、例のテキストの前処理を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-207-1><a id=__codelineno-207-1 name=__codelineno-207-1 href=#__codelineno-207-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test pre-tokenization!&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-208-1><a id=__codelineno-208-1 name=__codelineno-208-1 href=#__codelineno-208-1></a>[(&#39;Let&#39;, (0, 3)),
</span><span id=__span-208-2><a id=__codelineno-208-2 name=__codelineno-208-2 href=#__codelineno-208-2></a> (&quot;&#39;s&quot;, (3, 5)),
</span><span id=__span-208-3><a id=__codelineno-208-3 name=__codelineno-208-3 href=#__codelineno-208-3></a> (&#39;Ġ test&#39;, (5, 10)),
</span><span id=__span-208-4><a id=__codelineno-208-4 name=__codelineno-208-4 href=#__codelineno-208-4></a> (&#39;Ġ pre&#39;, (10, 14)),
</span><span id=__span-208-5><a id=__codelineno-208-5 name=__codelineno-208-5 href=#__codelineno-208-5></a> (&#39;-&#39;, (14, 15)),
</span><span id=__span-208-6><a id=__codelineno-208-6 name=__codelineno-208-6 href=#__codelineno-208-6></a> (&#39;tokenization&#39;, (15, 27)),
</span><span id=__span-208-7><a id=__codelineno-208-7 name=__codelineno-208-7 href=#__codelineno-208-7></a> (&#39;!&#39;, (27, 28))]
</span></code></pre></div></p> <p>次はモデルで、これには訓練が必要です。GPT-2の場合、唯一の特別なトークンはend-of-textトークンです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-209-1><a id=__codelineno-209-1 name=__codelineno-209-1 href=#__codelineno-209-1></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>trainers</span><span class=o>.</span><span class=n>BpeTrainer</span><span class=p>(</span><span class=n>vocab_size</span><span class=o>=</span><span class=mi>25000</span><span class=p>,</span> <span class=n>special_tokens</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;&lt;|endoftext|&gt;&quot;</span><span class=p>])</span>
</span><span id=__span-209-2><a id=__codelineno-209-2 name=__codelineno-209-2 href=#__codelineno-209-2></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>train_from_iterator</span><span class=p>(</span><span class=n>get_training_corpus</span><span class=p>(),</span> <span class=n>trainer</span><span class=o>=</span><span class=n>trainer</span><span class=p>)</span>
</span></code></pre></div> <p><code>WordPieceTrainer</code>と同様に、<code>vocab_size</code>と<code>special_tokens</code>に加えて、望む場合は<code>min_frequency</code>を指定できます。または、単語終了サフィックス（<code>&lt;/w&gt;</code>など）がある場合、<code>end_of_word_suffix</code>で設定できます。</p> <p>このトークナイザーはテキストファイルでも訓練できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-210-1><a id=__codelineno-210-1 name=__codelineno-210-1 href=#__codelineno-210-1></a><span class=c1># tokenizer.model = models.BPE()</span>
</span><span id=__span-210-2><a id=__codelineno-210-2 name=__codelineno-210-2 href=#__codelineno-210-2></a><span class=c1># tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)</span>
</span></code></pre></div> <p>サンプルテキストのトークン化を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-211-1><a id=__codelineno-211-1 name=__codelineno-211-1 href=#__codelineno-211-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer.&quot;</span><span class=p>)</span>
</span><span id=__span-211-2><a id=__codelineno-211-2 name=__codelineno-211-2 href=#__codelineno-211-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-212-1><a id=__codelineno-212-1 name=__codelineno-212-1 href=#__codelineno-212-1></a>[&#39;L&#39;, &#39;et&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;Ġ test&#39;, &#39;Ġ this&#39;, &#39;Ġ token&#39;, &#39;izer&#39;, &#39;.&#39;]
</span></code></pre></div></p> <p>GPT-2トークナイザーにバイトレベル後処理を適用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-213-1><a id=__codelineno-213-1 name=__codelineno-213-1 href=#__codelineno-213-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>post_processor</span> <span class=o>=</span> <span class=n>processors</span><span class=o>.</span><span class=n>ByteLevel</span><span class=p>(</span><span class=n>trim_offsets</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></code></pre></div> <p><code>trim_offsets = False</code>オプションは、後処理器に「Ġ」で始まるトークンのオフセットをそのままにしておくように指示します：このように、オフセットの開始は単語の最初の文字ではなく、単語の前のスペースを指すようになります（スペースは技術的にはトークンの一部であるため）。エンコードしたばかりのテキストでの結果を見てみましょう。ここで、<code>'Ġ test'</code>はインデックス4のトークンです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-214-1><a id=__codelineno-214-1 name=__codelineno-214-1 href=#__codelineno-214-1></a><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&quot;Let&#39;s test this tokenizer.&quot;</span>
</span><span id=__span-214-2><a id=__codelineno-214-2 name=__codelineno-214-2 href=#__codelineno-214-2></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span><span id=__span-214-3><a id=__codelineno-214-3 name=__codelineno-214-3 href=#__codelineno-214-3></a><span class=n>start</span><span class=p>,</span> <span class=n>end</span> <span class=o>=</span> <span class=n>encoding</span><span class=o>.</span><span class=n>offsets</span><span class=p>[</span><span class=mi>4</span><span class=p>]</span>
</span><span id=__span-214-4><a id=__codelineno-214-4 name=__codelineno-214-4 href=#__codelineno-214-4></a><span class=n>sentence</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>]</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-215-1><a id=__codelineno-215-1 name=__codelineno-215-1 href=#__codelineno-215-1></a>&#39; test&#39;
</span></code></pre></div></p> <p>最後に、バイトレベルデコーダーを追加します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-216-1><a id=__codelineno-216-1 name=__codelineno-216-1 href=#__codelineno-216-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoders</span><span class=o>.</span><span class=n>ByteLevel</span><span class=p>()</span>
</span><span id=__span-216-2><a id=__codelineno-216-2 name=__codelineno-216-2 href=#__codelineno-216-2></a><span class=n>decoded_text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>ids</span><span class=p>)</span>
</span><span id=__span-216-3><a id=__codelineno-216-3 name=__codelineno-216-3 href=#__codelineno-216-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Decoded: </span><span class=si>{</span><span class=n>decoded_text</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-217-1><a id=__codelineno-217-1 name=__codelineno-217-1 href=#__codelineno-217-1></a>Decoded: Let&#39;s test this tokenizer.
</span></code></pre></div></p> <p>素晴らしい！完了したので、前と同じようにトークナイザーを保存し、Hugging Face Transformersで使用したい場合は<code>PreTrainedTokenizerFast</code>または<code>GPT2TokenizerFast</code>でラップできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-218-1><a id=__codelineno-218-1 name=__codelineno-218-1 href=#__codelineno-218-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>PreTrainedTokenizerFast</span>
</span><span id=__span-218-2><a id=__codelineno-218-2 name=__codelineno-218-2 href=#__codelineno-218-2></a>
</span><span id=__span-218-3><a id=__codelineno-218-3 name=__codelineno-218-3 href=#__codelineno-218-3></a><span class=n>wrapped_tokenizer</span> <span class=o>=</span> <span class=n>PreTrainedTokenizerFast</span><span class=p>(</span>
</span><span id=__span-218-4><a id=__codelineno-218-4 name=__codelineno-218-4 href=#__codelineno-218-4></a>    <span class=n>tokenizer_object</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-218-5><a id=__codelineno-218-5 name=__codelineno-218-5 href=#__codelineno-218-5></a>    <span class=n>bos_token</span><span class=o>=</span><span class=s2>&quot;&lt;|endoftext|&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-218-6><a id=__codelineno-218-6 name=__codelineno-218-6 href=#__codelineno-218-6></a>    <span class=n>eos_token</span><span class=o>=</span><span class=s2>&quot;&lt;|endoftext|&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-218-7><a id=__codelineno-218-7 name=__codelineno-218-7 href=#__codelineno-218-7></a><span class=p>)</span>
</span><span id=__span-218-8><a id=__codelineno-218-8 name=__codelineno-218-8 href=#__codelineno-218-8></a>
</span><span id=__span-218-9><a id=__codelineno-218-9 name=__codelineno-218-9 href=#__codelineno-218-9></a><span class=c1># または</span>
</span><span id=__span-218-10><a id=__codelineno-218-10 name=__codelineno-218-10 href=#__codelineno-218-10></a><span class=c1># from transformers import GPT2TokenizerFast</span>
</span><span id=__span-218-11><a id=__codelineno-218-11 name=__codelineno-218-11 href=#__codelineno-218-11></a><span class=c1># wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span>
</span></code></pre></div> <h3 id=unigram_2>Unigramトークナイザーを一から構築<a class=headerlink href=#unigram_2 title="Permanent link">&para;</a></h3> <p>XLNetトークナイザーを構築してみましょう。前のトークナイザーと同様に、Unigramモデルで<code>Tokenizer</code>を初期化することから始めます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-219-1><a id=__codelineno-219-1 name=__codelineno-219-1 href=#__codelineno-219-1></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>models</span><span class=o>.</span><span class=n>Unigram</span><span class=p>())</span>
</span></code></pre></div> <p>ここでも、語彙がある場合はこのモデルを初期化できます。</p> <p>正規化については、XLNet（SentencePieceから来る）いくつかの置換を使用します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-220-1><a id=__codelineno-220-1 name=__codelineno-220-1 href=#__codelineno-220-1></a><span class=kn>from</span><span class=w> </span><span class=nn>tokenizers</span><span class=w> </span><span class=kn>import</span> <span class=n>Regex</span>
</span><span id=__span-220-2><a id=__codelineno-220-2 name=__codelineno-220-2 href=#__codelineno-220-2></a>
</span><span id=__span-220-3><a id=__codelineno-220-3 name=__codelineno-220-3 href=#__codelineno-220-3></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>normalizer</span> <span class=o>=</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>Sequence</span><span class=p>(</span>
</span><span id=__span-220-4><a id=__codelineno-220-4 name=__codelineno-220-4 href=#__codelineno-220-4></a>    <span class=p>[</span>
</span><span id=__span-220-5><a id=__codelineno-220-5 name=__codelineno-220-5 href=#__codelineno-220-5></a>        <span class=n>normalizers</span><span class=o>.</span><span class=n>Replace</span><span class=p>(</span><span class=s2>&quot;``&quot;</span><span class=p>,</span> <span class=s1>&#39;&quot;&#39;</span><span class=p>),</span>
</span><span id=__span-220-6><a id=__codelineno-220-6 name=__codelineno-220-6 href=#__codelineno-220-6></a>        <span class=n>normalizers</span><span class=o>.</span><span class=n>Replace</span><span class=p>(</span><span class=s2>&quot;&#39;&#39;&quot;</span><span class=p>,</span> <span class=s1>&#39;&quot;&#39;</span><span class=p>),</span>
</span><span id=__span-220-7><a id=__codelineno-220-7 name=__codelineno-220-7 href=#__codelineno-220-7></a>        <span class=n>normalizers</span><span class=o>.</span><span class=n>NFKD</span><span class=p>(),</span>
</span><span id=__span-220-8><a id=__codelineno-220-8 name=__codelineno-220-8 href=#__codelineno-220-8></a>        <span class=n>normalizers</span><span class=o>.</span><span class=n>StripAccents</span><span class=p>(),</span>
</span><span id=__span-220-9><a id=__codelineno-220-9 name=__codelineno-220-9 href=#__codelineno-220-9></a>        <span class=n>normalizers</span><span class=o>.</span><span class=n>Replace</span><span class=p>(</span><span class=n>Regex</span><span class=p>(</span><span class=s2>&quot; {2,}&quot;</span><span class=p>),</span> <span class=s2>&quot; &quot;</span><span class=p>),</span>
</span><span id=__span-220-10><a id=__codelineno-220-10 name=__codelineno-220-10 href=#__codelineno-220-10></a>    <span class=p>]</span>
</span><span id=__span-220-11><a id=__codelineno-220-11 name=__codelineno-220-11 href=#__codelineno-220-11></a><span class=p>)</span>
</span></code></pre></div> <p>これは<code>``</code>と<code>''</code>を<code>"</code>に置き換え、2つ以上のスペースの任意のシーケンスを単一のスペースに置き換え、トークン化するテキスト内のアクセント記号を除去します。</p> <p>任意のSentencePieceトークナイザーに使用する前処理器は<code>Metaspace</code>です：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-221-1><a id=__codelineno-221-1 name=__codelineno-221-1 href=#__codelineno-221-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>pre_tokenizers</span><span class=o>.</span><span class=n>Metaspace</span><span class=p>()</span>
</span></code></pre></div> <p>前と同様に、例のテキストの前処理を確認できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-222-1><a id=__codelineno-222-1 name=__codelineno-222-1 href=#__codelineno-222-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span><span class=o>.</span><span class=n>pre_tokenize_str</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test the pre-tokenizer!&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-223-1><a id=__codelineno-223-1 name=__codelineno-223-1 href=#__codelineno-223-1></a>[(&quot;▁Let&#39;s&quot;, (0, 5)),
</span><span id=__span-223-2><a id=__codelineno-223-2 name=__codelineno-223-2 href=#__codelineno-223-2></a> (&#39;▁test&#39;, (5, 10)),
</span><span id=__span-223-3><a id=__codelineno-223-3 name=__codelineno-223-3 href=#__codelineno-223-3></a> (&#39;▁the&#39;, (10, 14)),
</span><span id=__span-223-4><a id=__codelineno-223-4 name=__codelineno-223-4 href=#__codelineno-223-4></a> (&#39;▁pre-tokenizer!&#39;, (14, 29))]
</span></code></pre></div></p> <p>次はモデルで、これには訓練が必要です。XLNetにはかなり多くの特別なトークンがあります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-224-1><a id=__codelineno-224-1 name=__codelineno-224-1 href=#__codelineno-224-1></a><span class=n>special_tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;&lt;cls&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;sep&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;unk&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;pad&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;mask&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;s&gt;&quot;</span><span class=p>,</span> <span class=s2>&quot;&lt;/s&gt;&quot;</span><span class=p>]</span>
</span><span id=__span-224-2><a id=__codelineno-224-2 name=__codelineno-224-2 href=#__codelineno-224-2></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>trainers</span><span class=o>.</span><span class=n>UnigramTrainer</span><span class=p>(</span>
</span><span id=__span-224-3><a id=__codelineno-224-3 name=__codelineno-224-3 href=#__codelineno-224-3></a>    <span class=n>vocab_size</span><span class=o>=</span><span class=mi>25000</span><span class=p>,</span> <span class=n>special_tokens</span><span class=o>=</span><span class=n>special_tokens</span><span class=p>,</span> <span class=n>unk_token</span><span class=o>=</span><span class=s2>&quot;&lt;unk&gt;&quot;</span>
</span><span id=__span-224-4><a id=__codelineno-224-4 name=__codelineno-224-4 href=#__codelineno-224-4></a><span class=p>)</span>
</span><span id=__span-224-5><a id=__codelineno-224-5 name=__codelineno-224-5 href=#__codelineno-224-5></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>train_from_iterator</span><span class=p>(</span><span class=n>get_training_corpus</span><span class=p>(),</span> <span class=n>trainer</span><span class=o>=</span><span class=n>trainer</span><span class=p>)</span>
</span></code></pre></div> <p><code>UnigramTrainer</code>で忘れてはならない非常に重要な引数は<code>unk_token</code>です。また、Unigramアルゴリズム固有の他の引数も渡すことができます。例えば、トークンを削除する各ステップでの<code>shrinking_factor</code>（デフォルトは0.75）や、特定のトークンの最大長を指定する<code>max_piece_length</code>（デフォルトは16）です。</p> <p>このトークナイザーはテキストファイルでも訓練できます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-225-1><a id=__codelineno-225-1 name=__codelineno-225-1 href=#__codelineno-225-1></a><span class=c1># tokenizer.model = models.Unigram()</span>
</span><span id=__span-225-2><a id=__codelineno-225-2 name=__codelineno-225-2 href=#__codelineno-225-2></a><span class=c1># tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)</span>
</span></code></pre></div> <p>サンプルテキストのトークン化を見てみましょう：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-226-1><a id=__codelineno-226-1 name=__codelineno-226-1 href=#__codelineno-226-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer.&quot;</span><span class=p>)</span>
</span><span id=__span-226-2><a id=__codelineno-226-2 name=__codelineno-226-2 href=#__codelineno-226-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-227-1><a id=__codelineno-227-1 name=__codelineno-227-1 href=#__codelineno-227-1></a>[&#39;▁Let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;▁test&#39;, &#39;▁this&#39;, &#39;▁to&#39;, &#39;ken&#39;, &#39;izer&#39;, &#39;.&#39;]
</span></code></pre></div></p> <p>XLNetの特殊性は、文の最後にタイプID 2で<code>&lt;cls&gt;</code>トークンを置くことです（他のトークンと区別するため）。結果として、左側にパディングが行われます。すべての特別なトークンとトークンタイプIDをテンプレートで処理できますが、BERTと同様に、まず<code>&lt;cls&gt;</code>と<code>&lt;sep&gt;</code>トークンのIDを取得する必要があります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-228-1><a id=__codelineno-228-1 name=__codelineno-228-1 href=#__codelineno-228-1></a><span class=n>cls_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>token_to_id</span><span class=p>(</span><span class=s2>&quot;&lt;cls&gt;&quot;</span><span class=p>)</span>
</span><span id=__span-228-2><a id=__codelineno-228-2 name=__codelineno-228-2 href=#__codelineno-228-2></a><span class=n>sep_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>token_to_id</span><span class=p>(</span><span class=s2>&quot;&lt;sep&gt;&quot;</span><span class=p>)</span>
</span><span id=__span-228-3><a id=__codelineno-228-3 name=__codelineno-228-3 href=#__codelineno-228-3></a><span class=nb>print</span><span class=p>(</span><span class=n>cls_token_id</span><span class=p>,</span> <span class=n>sep_token_id</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-229-1><a id=__codelineno-229-1 name=__codelineno-229-1 href=#__codelineno-229-1></a>0 1
</span></code></pre></div></p> <p>テンプレートは次のようになります：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-230-1><a id=__codelineno-230-1 name=__codelineno-230-1 href=#__codelineno-230-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>post_processor</span> <span class=o>=</span> <span class=n>processors</span><span class=o>.</span><span class=n>TemplateProcessing</span><span class=p>(</span>
</span><span id=__span-230-2><a id=__codelineno-230-2 name=__codelineno-230-2 href=#__codelineno-230-2></a>    <span class=n>single</span><span class=o>=</span><span class=s2>&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span><span class=p>,</span>
</span><span id=__span-230-3><a id=__codelineno-230-3 name=__codelineno-230-3 href=#__codelineno-230-3></a>    <span class=n>pair</span><span class=o>=</span><span class=s2>&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span><span class=p>,</span>
</span><span id=__span-230-4><a id=__codelineno-230-4 name=__codelineno-230-4 href=#__codelineno-230-4></a>    <span class=n>special_tokens</span><span class=o>=</span><span class=p>[(</span><span class=s2>&quot;&lt;sep&gt;&quot;</span><span class=p>,</span> <span class=n>sep_token_id</span><span class=p>),</span> <span class=p>(</span><span class=s2>&quot;&lt;cls&gt;&quot;</span><span class=p>,</span> <span class=n>cls_token_id</span><span class=p>)],</span>
</span><span id=__span-230-5><a id=__codelineno-230-5 name=__codelineno-230-5 href=#__codelineno-230-5></a><span class=p>)</span>
</span></code></pre></div> <p>文のペアをエンコードして動作することをテストできます：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-231-1><a id=__codelineno-231-1 name=__codelineno-231-1 href=#__codelineno-231-1></a><span class=n>encoding</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&quot;Let&#39;s test this tokenizer...&quot;</span><span class=p>,</span> <span class=s2>&quot;on a pair of sentences!&quot;</span><span class=p>)</span>
</span><span id=__span-231-2><a id=__codelineno-231-2 name=__codelineno-231-2 href=#__codelineno-231-2></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-231-3><a id=__codelineno-231-3 name=__codelineno-231-3 href=#__codelineno-231-3></a><span class=nb>print</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>type_ids</span><span class=p>)</span>
</span></code></pre></div> <p><strong>実行結果:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-232-1><a id=__codelineno-232-1 name=__codelineno-232-1 href=#__codelineno-232-1></a>[&#39;▁Let&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;▁test&#39;, &#39;▁this&#39;, &#39;▁to&#39;, &#39;ken&#39;, &#39;izer&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;&lt;sep&gt;&#39;, &#39;▁&#39;, &#39;on&#39;, &#39;▁&#39;, &#39;a&#39;, &#39;▁pair&#39;, &#39;▁of&#39;, &#39;▁sentence&#39;, &#39;s&#39;, &#39;!&#39;, &#39;&lt;sep&gt;&#39;, &#39;&lt;cls&gt;&#39;]
</span><span id=__span-232-2><a id=__codelineno-232-2 name=__codelineno-232-2 href=#__codelineno-232-2></a>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
</span></code></pre></div></p> <p>最後に、<code>Metaspace</code>デコーダーを追加します：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-233-1><a id=__codelineno-233-1 name=__codelineno-233-1 href=#__codelineno-233-1></a><span class=n>tokenizer</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoders</span><span class=o>.</span><span class=n>Metaspace</span><span class=p>()</span>
</span></code></pre></div> <p>このトークナイザーは完了です！前と同じようにトークナイザーを保存し、Hugging Face Transformersで使用したい場合は<code>PreTrainedTokenizerFast</code>または<code>XLNetTokenizerFast</code>でラップできます。<code>PreTrainedTokenizerFast</code>を使用する際に注意すべき点は、特別なトークンに加えて、Hugging Face Transformersライブラリに左パディングするように伝える必要があることです：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-234-1><a id=__codelineno-234-1 name=__codelineno-234-1 href=#__codelineno-234-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>PreTrainedTokenizerFast</span>
</span><span id=__span-234-2><a id=__codelineno-234-2 name=__codelineno-234-2 href=#__codelineno-234-2></a>
</span><span id=__span-234-3><a id=__codelineno-234-3 name=__codelineno-234-3 href=#__codelineno-234-3></a><span class=n>wrapped_tokenizer</span> <span class=o>=</span> <span class=n>PreTrainedTokenizerFast</span><span class=p>(</span>
</span><span id=__span-234-4><a id=__codelineno-234-4 name=__codelineno-234-4 href=#__codelineno-234-4></a>    <span class=n>tokenizer_object</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-234-5><a id=__codelineno-234-5 name=__codelineno-234-5 href=#__codelineno-234-5></a>    <span class=n>bos_token</span><span class=o>=</span><span class=s2>&quot;&lt;s&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-6><a id=__codelineno-234-6 name=__codelineno-234-6 href=#__codelineno-234-6></a>    <span class=n>eos_token</span><span class=o>=</span><span class=s2>&quot;&lt;/s&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-7><a id=__codelineno-234-7 name=__codelineno-234-7 href=#__codelineno-234-7></a>    <span class=n>unk_token</span><span class=o>=</span><span class=s2>&quot;&lt;unk&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-8><a id=__codelineno-234-8 name=__codelineno-234-8 href=#__codelineno-234-8></a>    <span class=n>pad_token</span><span class=o>=</span><span class=s2>&quot;&lt;pad&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-9><a id=__codelineno-234-9 name=__codelineno-234-9 href=#__codelineno-234-9></a>    <span class=n>cls_token</span><span class=o>=</span><span class=s2>&quot;&lt;cls&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-10><a id=__codelineno-234-10 name=__codelineno-234-10 href=#__codelineno-234-10></a>    <span class=n>sep_token</span><span class=o>=</span><span class=s2>&quot;&lt;sep&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-11><a id=__codelineno-234-11 name=__codelineno-234-11 href=#__codelineno-234-11></a>    <span class=n>mask_token</span><span class=o>=</span><span class=s2>&quot;&lt;mask&gt;&quot;</span><span class=p>,</span>
</span><span id=__span-234-12><a id=__codelineno-234-12 name=__codelineno-234-12 href=#__codelineno-234-12></a>    <span class=n>padding_side</span><span class=o>=</span><span class=s2>&quot;left&quot;</span><span class=p>,</span>
</span><span id=__span-234-13><a id=__codelineno-234-13 name=__codelineno-234-13 href=#__codelineno-234-13></a><span class=p>)</span>
</span><span id=__span-234-14><a id=__codelineno-234-14 name=__codelineno-234-14 href=#__codelineno-234-14></a>
</span><span id=__span-234-15><a id=__codelineno-234-15 name=__codelineno-234-15 href=#__codelineno-234-15></a><span class=c1># または</span>
</span><span id=__span-234-16><a id=__codelineno-234-16 name=__codelineno-234-16 href=#__codelineno-234-16></a><span class=c1># from transformers import XLNetTokenizerFast</span>
</span><span id=__span-234-17><a id=__codelineno-234-17 name=__codelineno-234-17 href=#__codelineno-234-17></a><span class=c1># wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)</span>
</span></code></pre></div> <p>様々な構成要素がどのように使用されて既存のトークナイザーを構築するかを見たので、Hugging Face Tokenizersライブラリで望むトークナイザーを書いて、Hugging Face Transformersで使用できるはずです。</p> <h2 id=_26>まとめ<a class=headerlink href=#_26 title="Permanent link">&para;</a></h2> <p>この記事を通じて、Hugging Face Tokenizersライブラリを使用した高度なトークン化技術について詳しく学習しました。新しいトークナイザーの訓練から、高速トークナイザーの特別な機能、そして3つの主要なサブワード・トークン化アルゴリズム（BPE、WordPiece、Unigram）の実装まで、実践的なコード例とともに幅広いトピックをカバーしました。</p> <h3 id=_27>学習ポイントの振り返り<a class=headerlink href=#_27 title="Permanent link">&para;</a></h3> <ol> <li><strong>新しいトークナイザーの訓練</strong>: 既存のトークナイザーをベースとして、新しいコーパスに適応させる方法を習得</li> <li><strong>高速トークナイザーの機能</strong>: オフセットマッピングを活用したトークン分類や質問応答タスクの実装</li> <li><strong>アルゴリズムの理解</strong>: BPE、WordPiece、Unigramの訓練プロセスとトークン化方法の違いを詳細に学習</li> <li><strong>一からの構築</strong>: Hugging Face Tokenizersライブラリを使用して、各コンポーネントを組み合わせたカスタムトークナイザーの構築</li> </ol> <h3 id=_28>次のステップ<a class=headerlink href=#_28 title="Permanent link">&para;</a></h3> <ul> <li>より大規模なコーパスでのトークナイザー訓練に挑戦</li> <li>特定のドメイン（医療、法律、技術文書など）に特化したトークナイザーの開発</li> <li>多言語対応トークナイザーの構築</li> <li>実際のTransformerモデルと組み合わせた事前学習の実践</li> </ul> <h3 id=_29>参考資料<a class=headerlink href=#_29 title="Permanent link">&para;</a></h3> <ul> <li><a href=https://huggingface.co/docs/tokenizers>Hugging Face Tokenizers ドキュメント</a></li> <li><a href=https://github.com/huggingface/transformers>Hugging Face Transformers ライブラリ</a></li> <li><a href=https://arxiv.org/abs/1508.07909>BPE: Neural Machine Translation of Rare Words with Subword Units</a></li> <li><a href=https://research.google/pubs/pub37842/ >WordPiece: Japanese and Korean Voice Search</a></li> <li><a href=https://arxiv.org/abs/1808.06226>SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></li> </ul> <p>この知識を基に、自然言語処理プロジェクトでより効果的なトークン化戦略を実装し、モデルの性能向上を目指してください。トークン化は機械学習モデルの基盤となる重要な技術であり、その理解と実践的な応用能力は、NLP分野での成功に不可欠です。</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最終更新日> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年9月28日 19:08:34 JST">2025年9月28日 19:08:34</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> ページトップへ戻る </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 - 2025 vinsmoke-three </div> </div> <div class=md-social> <a href=https://github.com/vinsmoke-three target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"BERT": "bert", "CNN": "convolutional-neural-network", "FashionMNIST": "fashion-mnist", "GPT": "gpt", "LLM": "large-language-model", "ML\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3": "ml-pipeline", "NLP": "nlp", "PyTorch": "pytorch", "Python": "python", "TensorBoard": "tensorboard", "TinyVGG": "tinyvgg", "Transformer": "transformer", "\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8": "custom-datasets", "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3": "computer-vision", "\u30b9\u30af\u30ea\u30d7\u30c8\u30e2\u30fc\u30c9": "script-mode", "\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb": "tutorial", "\u30c6\u30f3\u30bd\u30eb": "tensor", "\u30c7\u30fc\u30bf\u62e1\u5f35": "data-augmentation", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af": "neural-network", "\u30e2\u30b8\u30e5\u30fc\u30eb\u5316": "modularization", "\u30ef\u30fc\u30af\u30d5\u30ed\u30fc": "workflow", "\u4e0a\u7d1a\u8005\u5411\u3051": "advanced", "\u4e2d\u7d1a\u8005\u5411\u3051": "intermediate", "\u518d\u5229\u7528": "reusability", "\u5206\u985e": "classification", "\u521d\u5fc3\u8005\u5411\u3051": "beginner", "\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb": "large-language-model", "\u5b9f\u8df5": "practical", "\u5b9f\u9a13\u8ffd\u8de1": "experiment-tracking", "\u6a5f\u68b0\u5b66\u7fd2": "machine-learning", "\u6df1\u5c64\u5b66\u7fd2": "deep-learning", "\u753b\u50cf\u5206\u985e": "image-classification", "\u7dda\u5f62\u56de\u5e30": "linear-regression", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406": "natural-language-processing", "\u8ee2\u79fb\u5b66\u7fd2": "transfer-learning"}, "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/meta.js></script> <script src=../../javascripts/structured-data.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>